---
title: "Spatial Data Workshop"
subtitle: "2023 EPA R User Group Workshop"
date: October 18, 2023
format:
  revealjs:
    author: 
      - "Marc Weber"
      - "Michael Dumelle"
    institute: 
      - "EPA (USA)"
      - "EPA (USA)"
    footer: "Spatial Data Workshop"
    slide-number: true
    preview-links: true
    transition: fade
    theme: [default, slides.scss]
    smaller: false
    auto-stretch: true
    code-link: true
    incremental: false
execute: 
  echo: true
embed-resources: true
bibliography: references.bib
---

```{r}
#| label: setup
#| include: false

# set width of code output
options(width = 80)

# load background packages
library(countdown)
library(ggplot2)
```

## Welcome!

1. Please visit [this link](https://usepa.github.io/spatialdata.epaR2023/) to view the workshop's accompanying workbook

2.  Install and load R packages by visiting [this link](https://usepa.github.io/spatialdata.epaR2023/#set-up)

3. (Optional) Download the workshop slides (instructions in the workbook's "Welcome" page)

4. Follow along and have fun!

## Who Are We?

Marc Weber is a geographer at the Pacific Ecological Systems Division (PESD) at the United States Environmental Protection Agency (USEPA). His work supports various aspects of the USEPA's National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States, as he helped develop and maintains the StreamCat and LakeCat datasets.  His work focuses on spatial analysis in R and Python, Geographic Information Science (GIS), aquatic ecology, remote sensing, open source science and environmental modeling.

## Who Are We?

Michael Dumelle is a statistician for the United States Environmental Protection Agency (USEPA). He works primarily on facilitating the survey design and analysis of USEPA's National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States. His primary research interests are in spatial statistics, survey design, environmental and ecological applications, and software development.

## Disclaimer

The views expressed in this workshop are those of the authors and do not necessarily represent the views or policies of the U.S. Environmental Protection Agency or the U.S. National Oceanic and Atmospheric Administration. Any mention of trade names, products, or services does not imply an endorsement by the U.S. government, the U.S. Environmental Protection Agency, or the U.S. National Oceanic and Atmospheric Administration. The U.S. Environmental Protection Agency and the U.S. National Oceanic and Atmospheric Administration do not endorse any commercial products, services, or enterprises.

## What Will We Cover?

* Foundations of spatial data
* Visualizing spatial data
* Geoprocessing spatial data
* Advanced applications
* Focus on the **R** computing language

## Today's Agenda

- 1:00pm - 1:35pm EDT: Introduction and Spatial Data Structures in R
- 1:35pm - 2:05pm EDT: Vector Data Model
- 2:05pm - 2:10pm EDT: Break
- 2:10pm - 2:30pm EDT: Raster Data Model
- 2:30pm - 3:15pm EDT: Coordinate Reference Systems and I / O
- 3:15pm - 3:25pm EDT: Break

## Today's Agenda

- 3:25pm - 3:45pm EDT: Spatial Data Visualization
- 3:45pm - 4:15pm EDT: Geoprocecessing
- 4:15pm - 4:20pm EDT: Break
- 4:20pm - 5:00pm EDT: Advanced Applications

# Foundations

## Goals

::: goals
1. Understand fundamental spatial data structures and libraries in **R**.
2. Become familiar with coordinate reference systems.
3. Geographic I/O (input/output).
:::

## Data Structures

* Review data structures section in workbook
* Cover vectors, matrices, arrays, lists, data frames, etc.
* Cover data frame manipulation using [tidyverse](https://www.tidyverse.org/) functions and the pipe operator (`%>%` or `|>`)

## Why R for Spatial Data Analysis?

* Lightweight, open-source, and cross-platform
* Provides tools for automation and reproducibility
* Seamlessly integrates spatial data analysis and statistical analysis in one environment
* Handles vector and raster data

## R Drawbacks for GIS Work

* R is less interactive than desktop applications like ArcGIS or QGIS
* Handling coordinate reference systems is more challenging
* In-memory analysis can be prohibitive for large data
* Steep learning curve

## A Motivating Example

* Simple, expressive code

```{r}
#| warning: false
library(tmap)
library(tmaptools)
library(dplyr)

# find my town!
my_town <- tmaptools::geocode_OSM("Corvallis OR USA", as.sf = TRUE)
glimpse(my_town)
```

## Your Turn

::: task
1. What does `::` do?
2. What does `geocode_OSM()` do?
3. Explain how the code runs together using the  `|>` chaining operator
4. What is `glimpse()`?  How is it useful compared to `head()`?
:::

```{r}
#| echo: false

countdown(minutes = 5)
```

## Our Turn

::: task2
1. `::` lets you clarify from which package a function is called (e.g., `tmaptools::geocodeOSM()`)
2. `geocode_OSM()` looks up a named feature in OpenStreetMap and returns the coordinates
3. Do this `|>` then that
4. `glimpse()` glimpses at a data frame
:::

## A Choropleth Map

* A choropleth map is a map that connects statistical summaries to geographic characteristics
* The `tigris` package can retrieve census, county, and state boundaries as vector `sf` data

```{r}
#| label: fig-tmapexample
#| fig-cap: "Distribution of Oregon counties."
#| output-location: slide
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

library(tigris)
library(sf)
counties <- counties("Oregon", cb = TRUE)
counties$area <- as.numeric(st_area(counties))
glimpse(counties)

tm_shape(counties) +
  tm_polygons("area", style="quantile", title="Area of Oregon Counties")
```


## Your Turn

::: task
Create and explore an interactive map of `my_town` by running
```{r}
#| eval: false
library(mapview)
mapviewOptions(fgb=FALSE)
mapview(my_town, col="red", col.regions = "red") +
  mapview(counties, alpha.regions = .1)
```

:::

```{r}
#| echo: false
#| message: false
#| warning: false

library(mapview)
countdown(minutes = 4)
```

## Our Turn

```{r}
#| label: fig-mytowncounties
#| fig-cap: "Corvallis among distribution of Oregon counties."
#| fig-align: center
#| out-width: "100%"
#| echo: false

knitr::include_graphics("img/mytown_counties.jpg")
```

## Spatial Data Structures

* Spatial data structures are primarily organized through:

1. GDAL [link here](https://gdal.org/): For raster and feature abstraction and processing
2. PROJ [link here](https://proj.org/): For coordinate transformations and projections
3. GEOS [link here](https://libgeos.org/): For spatial operations (e.g., calculating buffers or centroids) on data with a projected coordinate reference system (CRS)

* We explore these core libraries throughout the workshop

## Types of Spatial Data

* Vector data are comprised of points, lines, and polygons that represent discrete spatial entities (e.g., river, wtaershed, stream gauge)
* Raster data divide space into small rectangles (pixels) that represent spatially continuous phenomena like elevation or precipitation

## Types of Spatial Data

```{r}
#| label: fig-vectorraster
#| fig-cap: "Vector and raster data."
#| fig-align: center
#| out-width: "100%"
#| echo: false

knitr::include_graphics("img/09-vec-raster.jpg")
```


## Vector Data Model

* Vector data are described using simple features
* Simple features is a standard way to specify how two-dimensional geometries are represented, stored in and retrieved from databases, and which geometric operations can be performed
* Provides framework for extending spatial `POINTS` to `LINES` and `POLYGONS`

## Types of Spatial Data

```{r}
#| label: fig-sfmodel
#| fig-cap: "The simple features data model."
#| fig-align: center
#| echo: false
#| out-width: "100%"

knitr::include_graphics("img/09-sf-model.png")
```

## The `sf` Package

* We use the `sf` package to work with spatial vector data in **R**
* Learn more about `sf` at their website [r-spatial.github.io/sf](r-spatial.github.io/sf) and/or by running
```{r}
#| eval: false

vignette(package = "sf") # see which vignettes are available
vignette("sf1")          # open the introduction vignette
```

## `sf` Primitives

* There are three basic `geometric` primitives with corresponding `sf` functions to create them:

1. `POINT`: created with `st_points()`
2. `LINESTRING`: created with `st_linestring()`
3. `POLYGON`: created with `st_polygon()`

* More complex simple feature geometries are built up using `st_multipoint()`, `st_multilinestring()`, `st_multipolygon()`, and `st_geometrycollection()`
* Building blocks for more complicated/large data structures

## `POINT`s

* Points in `sf` are composed of coordinate pairs and a coordinate reference system
* Points have no length or area
```{r}
# create sf object as point
pt1 <- st_point(c(1,3))
pt2 <- st_point(c(2,1))
pt3 <- st_point(c(3,2))
sf_point <- st_sfc(pt1, pt2, pt3)
```

```{r}
#| label: fig-sfpoint
#| fig-cap: "sf `POINT`"
#| output-location: slide

ggplot(sf_point) + 
  geom_sf(color = "red") +
  labs(x = "X", y = "Y")
```

## `LINESTRING`s

* An ordered sequence of two or more `POINT`s
* Points in a line are called verticies (or nodes)
* Linestrings have length but no area

```{r}
# create sf object as linestring
line1 <- sf::st_linestring(rbind(pt1, pt2))
line2 <- sf::st_linestring(rbind(pt2, pt3))
sf_linestring <- st_sfc(line1, line2)
```

```{r}
#| label: fig-sflinestring
#| fig-cap: "sf `LINESTRING`"
#| output-location: slide

ggplot(sf_linestring) + 
  geom_sf() +
  labs(x = "X", y = "Y")
```

## `POLYGON`s

* Four or more points with the same starting and ending point
* Has length and area


```{r}
pt4 <- pt1
poly1 <- st_polygon(list(rbind(pt1, pt2, pt3, pt4)))
sf_polygon <- st_sfc(poly1)

# dimension
sf::st_dimension(sf_polygon)
```

```{r}
#| label: fig-sfpolygon
#| fig-cap: "sf `POLYGON`"
#| output-location: slide

ggplot(sf_polygon) + 
  geom_sf(fill = "grey")
```

## Putting Them All Together

```{r}
#| label: fig-sfallshape
#| fig-cap: "sf `POINT`, `LINESTRING`, and `POLYGON` overlain"
#| output-location: slide

ggplot() +
  geom_sf(data = sf_point, color = "red") +
  geom_sf(data = sf_linestring) +
  geom_sf(data = sf_polygon, fill = "grey")
```

## Spatial Data Frames

* `sf` objects combine `data.frame` attributes with `POINT`, `LINESTRING`, and `POLYGON` building blocks (these are the `geometry`)
* Also characterize dimensionality, bounding box, coordinate reference system, attribute headers
* Read from shapefiles using `st_read()` or `read_sf()`

```{r}
#| warning: false
#| message: false
#| output-location: slide

library(spData)
data("us_states")
print(us_states)
```

## Spatial Data Frames

```{r}
#| label: fig-sfstructure
#| fig-cap: "The simple features data structure."
#| fig-align: center
#| echo: false
#| out-width: "100%"

knitr::include_graphics("img/sf_structure.png")
```

## Your Turn

::: task
So far we have explored plotting using `ggplot()`, but you can also use `sf`'s base plotting via `plot()`. Read more about plotting in `sf` by running `?plot.sf` and then make an appropriate plot of the `us_states` data.
:::

```{r}
#| echo: false

countdown(minutes = 5)
```

## Our Turn

::: task2
Running `plot()` in `sf` plots all variables; common to subset
:::

```{r}
#| label: fig-usarea
#| fig-cap: "Area by state in the US."
#| output-location: slide

plot(us_states["AREA"])
```


## Raster Data Model

* Raster data can be continuous or categorical
* Can be image based; have a temporal component

## Raster Data Model

```{r}
#| label: fig-rastermodel
#| fig-cap: "The raster data model."
#| fig-align: center
#| echo: false
#| out-width: "100%"

knitr::include_graphics("img/raster_data_model.jpg")
```

## The `terra` Package

* We use the `terra` package to work with spatial raster data in **R**
* `terra` is the modern replacement to `raster`
* Learn more about `terra` at their website [https://rspatial.org/](https://rspatial.org/)
* `stars` for spatio-temporal rasters

## Creating a Raster Object

* Create an empty `SpatRaster` object; define rows, columns, bounding box
```{r}
library(terra)
myrast <- rast(ncol = 10, nrow = 10, xmax = -116, xmin = -126, ymin = 42, ymax = 46)
str(myrast)
print(myrast)
```

## Your Turn

::: task
We just printed `myrast` and saw several components: `class`, `dimensions`, `resolution`, `extent`, and `coord. ref`. Explicitly return and inspect each of these pieces using `class()`, `ncell()`, `res()`, `ext()`, and `crs()`. Bonus: Return the number of raster cells using `ncell()`.
:::

```{r}
#| echo: false

countdown(minutes = 5)
```

## Our Turn

```{r}
class(myrast)
ncell(myrast)
res(myrast)
ext(myrast)
crs(myrast)
ncell(myrast)
```

## Manipulating Raster Objects

* Let's add values to the raster object
```{r}
values(myrast) <- 1:ncell(myrast)
```

```{r}
#| label: fig-rastplot
#| fig-cap: "Raster plot."
#| output-location: slide
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

plot(myrast)
```

## Reading a Raster Object

* Lets read in a raster object from the `spDataLarge` package that contains elevation data from Zion National Park

```{r}
raster_filepath <- system.file("raster/srtm.tif", package = "spDataLarge")
my_rast <- rast(raster_filepath)
```

```{r}
#| label: fig-rastplotzion
#| fig-cap: "Raster plot of elevation from Zion National Park."
#| output-location: slide
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

plot(my_rast)
```

## Your Turn

::: task
The `spatRaster` object in `terra` can hold multiple layers. Read in the four bands of Landsat 8 data from Zion National Park by running
```{r}
landsat <- system.file("raster/landsat.tif", package = "spDataLarge")
landsat <- rast(landsat)
```

Then plot this raster using `plot()`.

:::

```{r}
#| echo: false

countdown(minutes = 3)
```

## Our Turn

```{r}
#| label: fig-rastplotzion8
#| fig-cap: "Raster plot of Landsat 8 data from Zion National Park."
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

plot(landsat)
```


## Spatio-Temporal Raster Objects

* The `stars` package has tools for spatio-temporal raster data
* `stars` integrates with `sf` (recall that `terra` does not)
* Learn more at their website [https://r-spatial.github.io/stars/](https://r-spatial.github.io/stars/)

## Coordinate Reference Systems 

A coordinate reference system (CRS) is made up of several components

1. Coordinate system: The x,y grid that defines where your data lies in space
2. Horizontal and vertical units: The units that describe the coordinate system
3. Datum: The modeled version of the earth's shape
4. Projection details (if applicable): The mathematical equation used to flatten objects from round surface (earth) to flat surface (paper or screen)

Properly specifying coordinate reference systems is a crucial step in handling or analyzing spatial data

## The Ellipsoid and Geoid

* The earth can be thought of as an ellipsoid defined by a semi-major (equatorial) axis and a semi-minor (polar) axis
* More precisely, it is a geoid that is not perfectly smooth
* A datum is built on top of the ellipsoid and incorporates local features
* The most appropriate datum depends on the location of interest (in the US we typically use NAD83)

## The Ellipsoid and Geoid

```{r}
#| label: fig-ellipse
#| fig-cap: "The ellipsoid and geoid."
#| fig-align: center
#| echo: false
#| out-width: "75%"

knitr::include_graphics("img/Ellipsoid.png")
```

## Your Turn

::: task
There are several websites that are helpful for learning more about specific coordinate reference systems

1. Spatial Reference: [https://spatialreference.org/](https://spatialreference.org/)
2. Projection Wizard: [https://projectionwizard.org/](https://projectionwizard.org/)
3. EPSG: [https://epsg.io/](https://epsg.io/)

Spend a few minutes learning about one or more of these references by visiting their websites.

:::

```{r}
#| echo: false

countdown(minutes = 4)
```

## Specifying a CRS

There are many ways to specify a CRS:

1. Proj4
    * `+proj = longlat + ellps=WGS84 ...`
2. OGC WKT / ESRI WKT
    * `GEOGCS["WGS 84", DATUM["WGS_1984, SPHERIOD["WGS 84 ...]]]`
3. authority:code identifier (modern/preferred)
    * `EPSG: 4326`
    
## Projected Coordinate Systems

* Projected coordinates have been projected to two-dimensional space according to a CRS
* Projected coordinates have an origin, x-axis, y-axis, and unit of measure
* Conformal projections preserve shape
* Equal-area projections preserve area
* Equidistant projections preserve distance
* Azimuthal projection preserves direction


## Projected Coordinate Systems

* Here an example using vector data; see the workbook for an example using raster data

```{r}
#| label: fig-transformcrs
#| fig-cap: "PNW State Boundaries."
#| output-location: slide
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

library(Rspatialworkshop)
data("pnw")
# transform one to the other
utmz11 <- 2153
pnw <- st_transform(pnw, crs = utmz11)
ggplot() + 
  geom_sf(data = pnw,  color="black", fill=NA) +
  labs(title="State Boundaries in the Pacific Northwest") +
  theme_bw() 
```

## Your Turn

::: task
Re-project the `pnw` data to a different projected CRS. Then plot using base **R** or `ggplot2`.
:::

```{r}
#| echo: false

countdown(minutes = 4)
```

## Our Turn

::: task2
 Project to NAD83
:::

```{r}
pnw2 <- st_transform(pnw, crs = 5070)
plot(st_geometry(pnw2))
```

## A Note on `S2`

* `sf` version `1.0.0` supports spherical geometry operations via its interface to Google's `S2` spherical geometry engine
* `S2` is an example of a Discrete Global Grid System (DGGS)
* `sf` can run with `s2` on or off and by default the S2 geometry engine is turned on:
```{r}
#| eval: false

sf::sf_use_s2()
```

## Geographic Data I/O (Input/Ouptut)

There are several ways to read spatial data into R

1. Load spatial data from our machine or a remote source
2. Load spatial data as part of an **R** package
3. Load data using an API (which often makes use of an **R** package)
4. Convert flat files with x, y coordinates to spatial data
5. Geocoding data "by-hand" (we saw this earlier)

## Vector Data I/O

`sf` can read numerous file types:

1. shapefiles
2. geodatabases
3. geopackages
4. geojson
5. spatial database files

## Read in a Shapefile

```{r}
#| label: fig-citylims
#| fig-cap: "Oregon City Limits"
#| output-location: slide
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

filepath <- system.file("extdata/city_limits.shp", package = "Rspatialworkshop")
citylims <- read_sf(filepath)
plot(st_geometry(citylims), axes = T, main = "Oregon City Limits")
```

## Your Turn

::: task
Run `?read_sf()` and compare `read_sf()` to `st_read()`. Our preference is to use `read_sf()` -- why do you think that is?
:::

```{r}
#| echo: false

countdown(minutes = 3)
```

## Our Turn

::: task2
 
`read_sf()` calls `st_read()` with defaults chosen:
 
- `stringsAsFactors = FALSE`
- `quiet = TRUE`
- `as_tibble = TRUE`
 
:::

## Read in a Geodatabase

```{r}
#| label: fig-stateparks
#| fig-cap: "State Park Boundaries"
#| output-location: slide
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

filepath <- system.file("extdata/StateParkBoundaries.gdb", package = "Rspatialworkshop")
# List all feature classes in a file geodatabase
st_layers(filepath)
# Read the feature class
parks <- st_read(dsn = filepath, layer="StateParkBoundaries")
ggplot(parks) + 
  geom_sf()
```

## Read in a Geopackage

```{r}
#| label: fig-nc
#| fig-cap: "North Carolina Counties"
#| output-location: slide
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

filepath <- system.file("gpkg/nc.gpkg", package="sf")
nc <- read_sf(filepath)
ggplot(nc) + 
  geom_sf()
```

## Geopackage Advantages

Why use geopackages over shapefiles?

1. Geopackages avoid mult-file format of shapefiles
2. Geopackages avoid the 2gb limit of shapefiles
3. Geopackages are open-source and follow OGC standards
4. Geopackages are lighter in file size than shapefiles
5. Geopackages avoid the 10-character limit to column headers in shapefile attribute tables (stored in archaic `.dbf` files)

## Other Approaches

The workbook shows how to read in data using 

1. Open spatial data sources
2. **R** packages
3. OpenStreetMap

## Raster Data I/O

* Read in data using `rast()`

```{r}
#| label: fig-barelev
#| fig-cap: "Elevation Data Barplot"
#| output-location: slide
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

filepath <- system.file("ex/elev.tif", package="terra")
elev <- rast(filepath)
barplot(elev, digits = -1, las = 2, ylab = "Frequency")
```

## Raster Data I/O

```{r}
#| label: fig-elev
#| fig-cap: "Elevation Data"
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

plot(elev)
```

## Raster Data I/O

```{r}
#| label: fig-L7
#| fig-cap: "Landsat 7"
#| output-location: slide
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

library(stars)
filepath <- system.file("tif/L7_ETMs.tif", package = "stars")
L7 <- read_stars(filepath) |>
  slice(index = 1, along = "band")
plot(L7)
```

## Convert Flat Files to Spatial

It is common for a flat file (e.g., a `.csv`) to have columns that indicate coordinates -- how do we turn this into a spatial vector object?

```{r}
#| label: fig-gagesflat
#| fig-cap: "Stream Gages From a Flat File"
#| output-location: slide
#| fig-width: 8
#| results: hide
#| message: false
#| warning: false

filepath <- system.file("extdata/Gages_flowdata.csv", package = "Rspatialworkshop")
gages <- read.csv(filepath)
gages_sf <- gages |>
  st_as_sf(coords = c("LON_SITE", "LAT_SITE"), crs = 4269, remove = FALSE)
ggplot(data = gages_sf) + 
  geom_sf()
```

