[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Data Workshop",
    "section": "",
    "text": "Hello and welcome! The purpose of this site is to provide workshop materials for the Spatial Data workshop at the 2023 EPA R User Group Workshop. Slides that accompany this workshop are available for download linked here by clicking the “Download raw file” button via the ellipsis or downward arrow symbol on the right side of the screen.\n\n\n\n1:00pm - 1:45pm EDT: Introduction and Spatial Data Structures in R\n1:45pm - 2:15pm EDT: Vector Data Model\n2:15pm - 2:20pm Break\n2:20pm - 2:35pm EDT: Raster Data Model\n2:35pm - 2:45pm Break\n2:45pm - 3:15pm EDT:\n3:15pm - 3:20pm Break\n3:20pm - 3:45pm EDT:\n3:45pm - 4:15pm EDT:\n4:15pm - 4:45pm EDT:\n4:45pm - 5:00pm EDT:\n\n\n\n\nMarc Weber is a geographer at the Pacific Ecological Systems Division (PESD) at the United States Environmental Protection Agency (USEPA). His work supports various aspects of the USEPA’s National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States, as he helped develop and maintains the StreamCat and LakeCat datasets. His work focuses on spatial analysis in R and Python, Geographic Information Science (GIS), aquatic ecology, remote sensing, open source science and environmental modeling\nMichael Dumelle is a statistician for the United States Environmental Protection Agency (USEPA). He works primarily on facilitating the survey design and analysis of USEPA’s National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States. His primary research interests are in spatial statistics, survey design, environmental and ecological applications, and software development.\n\n\n\nThe packages that we use throughout this workshop are listed below. To install and load them, run:\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"cranlogs\")\ninstall.packages(\"sf\")\ninstall.packages(\"lubridate\")\n\n\n\n\nThis workshop was built using Quarto and bookdown and rendered to html. If you are familiar with using git and GitHub, you can fork and clone this repository, or simply clone directly and open the corresponding .qmd files to follow along with material in RStudio. You can also copy code snippets from the rendered book site and paste into your code files in RStudio.\nQuarto is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and it can render most existing Rmd files without modification. Like RMarkdown the benefits include:\n\nAllows for reproducible reporting from R\nYou write your document in markdown and embed executable code chunks using the knitr syntax\nYou can update your document at any time by re-knitting the code chunks and convert your document to a number of formats (i.e. html, pdf, word documents)\nWe assume everyone in the workshop is familiar with using RStudio\n\n\n\n\nThe views expressed in this manuscript are those of the authors and do not necessarily represent the views or policies of the U.S. Environmental Protection Agency. Any mention of trade names, products, or services does not imply an endorsement by the U.S. government or the U.S. Environmental Protection Agency. The U.S. Environmental Protection Agency does not endorse any commercial products, services, or enterprises."
  },
  {
    "objectID": "index.html#workshop-agenda",
    "href": "index.html#workshop-agenda",
    "title": "Spatial Data Workshop",
    "section": "Workshop Agenda",
    "text": "Workshop Agenda\n\n1:00pm - 1:45pm EDT: Introduction and Spatial Data Structures in R\n1:45pm - 2:15pm EDT: Vector Data Model\n2:15pm - 2:20pm Break\n2:20pm - 2:35pm EDT: Raster Data Model\n2:35pm - 2:45pm Break\n2:45pm - 3:15pm EDT:\n3:15pm - 3:20pm Break\n3:20pm - 3:45pm EDT:\n3:45pm - 4:15pm EDT:\n4:15pm - 4:45pm EDT:\n4:45pm - 5:00pm EDT:"
  },
  {
    "objectID": "index.html#author-introduction",
    "href": "index.html#author-introduction",
    "title": "Spatial Data Workshop",
    "section": "Author Introduction",
    "text": "Author Introduction\nMarc Weber is a geographer at the Pacific Ecological Systems Division (PESD) at the United States Environmental Protection Agency (USEPA). His work supports various aspects of the USEPA’s National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States, as he helped develop and maintains the StreamCat and LakeCat datasets. His work focuses on spatial analysis in R and Python, Geographic Information Science (GIS), aquatic ecology, remote sensing, open source science and environmental modeling\nMichael Dumelle is a statistician for the United States Environmental Protection Agency (USEPA). He works primarily on facilitating the survey design and analysis of USEPA’s National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States. His primary research interests are in spatial statistics, survey design, environmental and ecological applications, and software development."
  },
  {
    "objectID": "index.html#set-up",
    "href": "index.html#set-up",
    "title": "Spatial Data Workshop",
    "section": "Set Up",
    "text": "Set Up\nThe packages that we use throughout this workshop are listed below. To install and load them, run:\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"cranlogs\")\ninstall.packages(\"sf\")\ninstall.packages(\"lubridate\")"
  },
  {
    "objectID": "index.html#how-to-follow-along-with-material",
    "href": "index.html#how-to-follow-along-with-material",
    "title": "Spatial Data Workshop",
    "section": "How to follow along with material",
    "text": "How to follow along with material\nThis workshop was built using Quarto and bookdown and rendered to html. If you are familiar with using git and GitHub, you can fork and clone this repository, or simply clone directly and open the corresponding .qmd files to follow along with material in RStudio. You can also copy code snippets from the rendered book site and paste into your code files in RStudio.\nQuarto is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and it can render most existing Rmd files without modification. Like RMarkdown the benefits include:\n\nAllows for reproducible reporting from R\nYou write your document in markdown and embed executable code chunks using the knitr syntax\nYou can update your document at any time by re-knitting the code chunks and convert your document to a number of formats (i.e. html, pdf, word documents)\nWe assume everyone in the workshop is familiar with using RStudio"
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Spatial Data Workshop",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe views expressed in this manuscript are those of the authors and do not necessarily represent the views or policies of the U.S. Environmental Protection Agency. Any mention of trade names, products, or services does not imply an endorsement by the U.S. government or the U.S. Environmental Protection Agency. The U.S. Environmental Protection Agency does not endorse any commercial products, services, or enterprises."
  },
  {
    "objectID": "foundations.html#goals-and-outcomes",
    "href": "foundations.html#goals-and-outcomes",
    "title": "1  Foundations",
    "section": "1.1 Goals and Outcomes",
    "text": "1.1 Goals and Outcomes\n\nUnderstand fundamental spatial data structures and libraries in R.\nBecome familiar with coordinate reference systems.\nPerform some fundamental spatial operations in R.\nGeographic I/O"
  },
  {
    "objectID": "foundations.html#spatial-data-structures-in-r",
    "href": "foundations.html#spatial-data-structures-in-r",
    "title": "1  Foundations",
    "section": "1.4 Spatial Data Structures in R",
    "text": "1.4 Spatial Data Structures in R\nFirst we’ll walk through spatial data structures in R and review some key components of data structures in R that inform our use of spatial data in R.\n\n\n\n\n\n\nNote\n\n\n\nA few core libraries underpin spatial libraries in R (and Python!) and in GIS software applications such as QGIS and ArcPro. Spatial data structures across languages and applications are primarily organized through OSgeo and OGC). These core libraries include:\n\nGDAL –> For raster and feature abstraction and process\nPROJ –> A library for coordinate transformations and projections\nGEOS –> A Planar geometry engine for operations (measures, relations) such as calculating buffers and centroids on data with a projected CRS\nS2 –> a spherical geometry engine written in C++ developed by Google and adapted in R with the s2 package\n\n\n\nWe’ll see how these core libraries are called and used in the R packages we explore throughout this workshop.\n\n1.4.1 Simple features and geospatial grids\n\n*Vector data are comprised of points, lines, and polygons that represent discrete spatial entities, such as a river, watershed, or stream gauge.\nRaster data divides spaces into rectilinear cells (pixels) to represent spatially continuous phenomena, such as elevation or the weather. The cell size (or resolution) defines the fidelity of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.2 Vector Data Model\nFor Vector data, Simple Features (officially Simple Feature Access) is both an OGC and International Organization for Standardization (ISO) standard that specifies how (mostly) two-dimensional geometries can represent and describe objects in the real world. The Simple Features specification includes:\n\na class hierarchy\na set of operations\nbinary and text encodings\n\nIt describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.\nIt outlines how the spatial elements of POINTS (XY locations with a specific coordinate reference system) extend to LINES, POLYGONS and GEOMETRYCOLLECTION(s).\nThe “simple” adjective also refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines that do not self-intersect.\n\n\n\n\n\n\n\n\n\n\n1.4.2.1 Points\n\nA point in sf is composed of one coordinate pair (XY) in a specific coordinate system.\nPOINTS can have a Z or M dimensions as well but we won’t delve into -\n\nz is height, m is measure\nyou often have to remove to perform spatial operations with sf\n\nA POINT has no length, no area and a dimension of 0.\n\n\n# POINT defined as numeric vector\n(sf::st_dimension(sf::st_point(c(0,1))))\n#> [1] 0\n\n\nlibrary(ggplot2)\nggplot() + \n  geom_point(aes(x = c(1,2,3), y = c(3,1,2))) + \n  labs(x = \"X\", y = \"Y\")\n\n\n\n\n\n\n1.4.2.2 Lines\n\nA polyline is composed of a ordered sequence of two or more POINTs\nPoints in a line are called vertices/nodes and explicitly define the connection between two points.\nA LINESTRING has a length, has no area and has a dimension of 1 (length)\n\n\n# LINESTRING defined by matrix\n(sf::st_dimension(sf::st_linestring(matrix(1:4, nrow = 2))))\n#> [1] 1\n\n\nggplot() + \n  geom_line(aes(x = c(1,2,3), y = c(3,1,2))) + \n  geom_point(aes(x = c(1,2,3), y = c(3,1,2)), col = \"red\") + \n  labs(x = \"X\", y = \"Y\")\n\n\n\n\n\n\n1.4.2.3 Polygon\n\nA POLYGON is composed of 4 or more points whose starting and ending point are the same.\nA POLYGON is surface stored as a list of its exterior and interior rings.\nA POLYGON has length, area, and a dimension of 2. (area)\n\n\n# POLYGON defined by LIST (interior and exterior rings)\n(sf::st_dimension(sf::st_polygon(list(matrix(c(1:4, 1,2), nrow = 3, byrow = TRUE)))))\n#> [1] 2\n\n\nggplot() + \n  geom_polygon(aes(x = c(1,2,3,1), y = c(3,1,2,3)), fill = \"green\", alpha = .5) + \n  geom_line(aes(x = c(1,2,3), y = c(3,1,2))) + \n  geom_point(aes(x = c(1,2,3), y = c(3,1,2)), col = \"red\") +\n  labs(x = \"X\", y = \"Y\")\n\n\n\n\n\n\n1.4.2.4 Simple and valid geometries and ring direction\nThis breakdown of simple features follows for the most part this section in Spatial Data Science For linestrings to be considered simple they must not self-intersect:\n\nlibrary(sf)\n(ls <- st_linestring(rbind(c(0,0), c(1,1), c(2,2), c(0,2), c(1,1), c(2,0))))\n\n\n\n\n\n\n\n#> is_simple \n#>     FALSE\n\nFor polygons several other conditions have to be met to be simple:\n\npolygon rings are closed (the last point equals the first)\npolygon holes (inner rings) are inside their exterior ring\npolygon inner rings maximally touch the exterior ring in single points, not over a line\na polygon ring does not repeat its own path\nin a multi-polygon, an external ring maximally touches another exterior ring in single points, not over a line\n\nz and m coordinates As well as having the necessary X and Y coordinates, single point (vertex) simple features can have:\n\na Z coordinate, denoting altitude, and/or\nan M value, denoting some “measure”\n\nText and binary encodings A key part of the standard feature encoding is text and binary encodings. The well-known text (WKT) encoding we have shown above gives us a human-readable description of the geometry. The well-known binary (WKB) encoding is machine-readable, lossless, and faster to work with than text encoding. WKB is used for all interactions with GDAL and GEOS.\nOperations on geometries We can break down operations on geometries for vector features in the following way:\n\npredicates: a logical asserting a certain property is TRUE\nmeasures: a quantity (a numeric value, possibly with measurement unit)\ntransformations: newly generated geometries\n\nWe can look at these operations by what they operate on, whether the are single geometries, pairs, or sets of geometries:\n\nunary when it’s a single geometry\nbinary when it’s pairs of geometries\nn-ary when it’s sets of geometries\n\nUnary predicates work to describe a property of a geometry.\nA list of unary predicates:\n\n\n\npredicate\nmeaning\n\n\n\n\nis\nTests if geometry belongs to a particular class\n\n\nis_simple\nTests whether geometry is simple\n\n\nis_valid\nTest whether geometry is valid\n\n\nis_empty\nTests if geometry is empty\n\n\n\nA list of binary predicates is:\n\n\n\n\n\n\n\n\npredicate\nmeaning\ninverse of\n\n\n\n\ncontains\nNone of the points of A are outside B\nwithin\n\n\ncontains_properly\nA contains B and B has no points in common with the boundary of A\n\n\n\ncovers\nNo points of B lie in the exterior of A\ncovered_by\n\n\ncovered_by\nInverse of covers\n\n\n\ncrosses\nA and B have some but not all interior points in common\n\n\n\ndisjoint\nA and B have no points in common\nintersects\n\n\nequals\nA and B are topologically equal: node order or number of nodes may differ; identical to A contains B and A within B\n\n\n\nequals_exact\nA and B are geometrically equal, and have identical node order\n\n\n\nintersects\nA and B are not disjoint\ndisjoint\n\n\nis_within_distance\nA is closer to B than a given distance\n\n\n\nwithin\nNone of the points of B are outside A\ncontains\n\n\ntouches\nA and B have at least one boundary point in common, but no interior points\n\n\n\noverlaps\nA and B have some points in common; the dimension of these is identical to that of A and B\n\n\n\nrelate\nGiven a mask pattern, return whether A and B adhere to this pattern\n\n\n\n\nSee the Geometries chapter of Spatial Data Science for a full treatment that also covers **unary and binary measures* as well as unary, binary and n-ary transformers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nPlot the iris data\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(iris)\n\n\n\n\n\n\n\n\n\n\n1.4.3 Coordinate Reference Systems\n\n\n1.4.4 Raster\n\n\n1.4.5 Geographic Data I/O"
  },
  {
    "objectID": "geoprocessing.html#goals-and-outcomes",
    "href": "geoprocessing.html#goals-and-outcomes",
    "title": "2  Geoprocessing",
    "section": "2.1 Goals and Outcomes",
    "text": "2.1 Goals and Outcomes\n\nThis is goal 1.\nThis is goal 2.\nThis is goal 3.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may click on any of the functions in this book to be directed to their respective documentation. For example, clicking on st_join() takes you to the documentation page for the st_join() function on the sf website."
  },
  {
    "objectID": "geoprocessing.html#spatial-subsetting",
    "href": "geoprocessing.html#spatial-subsetting",
    "title": "2  Geoprocessing",
    "section": "2.2 Spatial Subsetting",
    "text": "2.2 Spatial Subsetting"
  },
  {
    "objectID": "geoprocessing.html#spatial-join",
    "href": "geoprocessing.html#spatial-join",
    "title": "2  Geoprocessing",
    "section": "2.3 Spatial Join",
    "text": "2.3 Spatial Join"
  },
  {
    "objectID": "geoprocessing.html#dissolve",
    "href": "geoprocessing.html#dissolve",
    "title": "2  Geoprocessing",
    "section": "2.4 Dissolve",
    "text": "2.4 Dissolve"
  },
  {
    "objectID": "geoprocessing.html#extract-and-zonal-statistics",
    "href": "geoprocessing.html#extract-and-zonal-statistics",
    "title": "2  Geoprocessing",
    "section": "2.5 Extract and Zonal Statistics",
    "text": "2.5 Extract and Zonal Statistics"
  },
  {
    "objectID": "geoprocessing.html#fixing-topology-errors",
    "href": "geoprocessing.html#fixing-topology-errors",
    "title": "2  Geoprocessing",
    "section": "2.6 Fixing topology errors",
    "text": "2.6 Fixing topology errors\n\nshow typical fixes for topology errors\n\n\n\n\n\n\n\nNote\n\n\n\nYou may encounter errors like this when running geoprocessing operations like st_join in R:\nError in wk_handle.wk_wkb(wkb, s2_geography_writer(oriented\n= oriented,  :  Loop 0 is not valid: Edge 772 crosses edge 774\nRunning st_make_valid might not fix.\nYou may need to turn off spherical geometry - sf_use_s2(TRUE), run st_make_valid, and then turn spherical geometry back on - sf_use_s2(FALSE) See background on S2 here and discussion of S2 related issues here"
  },
  {
    "objectID": "visualization.html#goals-and-outcomes",
    "href": "visualization.html#goals-and-outcomes",
    "title": "3  Visualization",
    "section": "3.1 Goals and Outcomes",
    "text": "3.1 Goals and Outcomes\n\nUnderstand how to use several of the most popular libraries for plotting and visualizing spatial data inR\nThis is goal 2.\nThis is goal 3.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may click on any of the functions in this book to be directed to their respective documentation. For example, clicking on st_join() takes you to the documentation page for the st_join() function on the sf website."
  },
  {
    "objectID": "visualization.html#mapview",
    "href": "visualization.html#mapview",
    "title": "3  Visualization",
    "section": "3.2 mapview",
    "text": "3.2 mapview"
  },
  {
    "objectID": "visualization.html#leaflet",
    "href": "visualization.html#leaflet",
    "title": "3  Visualization",
    "section": "3.3 leaflet",
    "text": "3.3 leaflet"
  },
  {
    "objectID": "visualization.html#ggplot2",
    "href": "visualization.html#ggplot2",
    "title": "3  Visualization",
    "section": "3.4 ggplot2",
    "text": "3.4 ggplot2"
  },
  {
    "objectID": "visualization.html#tmap",
    "href": "visualization.html#tmap",
    "title": "3  Visualization",
    "section": "3.5 tmap",
    "text": "3.5 tmap\nIt uses the same syntax as ggplot: the grammar of graphics - it supports both static and interactive modes"
  },
  {
    "objectID": "advanced_applications.html#goals-and-outcomes",
    "href": "advanced_applications.html#goals-and-outcomes",
    "title": "4  Advanced Applications",
    "section": "4.1 Goals and Outcomes",
    "text": "4.1 Goals and Outcomes\n\nThis is goal 1.\nThis is goal 2.\nThis is goal 3.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may click on any of the functions in this book to be directed to their respective documentation. For example, clicking on st_join() takes you to the documentation page for the st_join() function on the sf website."
  },
  {
    "objectID": "advanced_applications.html#advanced-applications",
    "href": "advanced_applications.html#advanced-applications",
    "title": "4  Advanced Applications",
    "section": "4.2 Advanced Applications",
    "text": "4.2 Advanced Applications\n\n4.2.1 Using Web Services"
  },
  {
    "objectID": "resources.html#r-spatial-resources",
    "href": "resources.html#r-spatial-resources",
    "title": "Resources",
    "section": "R Spatial Resources",
    "text": "R Spatial Resources\n\nR Spatial - Spatial Data Science with R\nGeocomputation with R\nR Spatial Task View\nModern Geospatial Data Analysis with R by Zev Ross\nSIGR2021 Summer School\nSpatial Data Science - Pebesma and Bivand\nSpatial Data Science Course- Prof. Adam Wilson\nIntroduction to Mapping and Spatial Analysis with R\nR Spatial Workshop for EPA R User Group\nIntro to GIS and Spatial Analysis by Manuel Gimond\nFOSS4G2019 R for Geospatial Processing\nAn Introduction to Spatial Analysis and Mapping in R\nEarth Analytics Spatial Data in R\nHydroinformatics at VT: Extensive Notes and exercises for a course on data analysis techniques in hydrology using the programming language R"
  },
  {
    "objectID": "resources.html#r-vector-processing-simple-features-resources",
    "href": "resources.html#r-vector-processing-simple-features-resources",
    "title": "Resources",
    "section": "R Vector Processing / Simple Features Resources",
    "text": "R Vector Processing / Simple Features Resources\n\nSimple Features for R\nSpatial Data in R: New Directions\nsp-sf Migration\nAn Exploration of Simple Features for R\nSimple Features: Building Spatial Data Pipelines in R\nTidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf"
  },
  {
    "objectID": "resources.html#r-raster-resources",
    "href": "resources.html#r-raster-resources",
    "title": "Resources",
    "section": "R Raster Resources",
    "text": "R Raster Resources\n\nterra\nSpatial Data Science with R and terra\nstars - spatiotemporal arrays\nWageningen University Intro to Raster\nWageningen University Advanced Raster Analysis\nThe Visual Raster Cheat Sheet GitHub Repo\nRastervis"
  },
  {
    "objectID": "resources.html#r-mapping-resources",
    "href": "resources.html#r-mapping-resources",
    "title": "Resources",
    "section": "R Mapping Resources",
    "text": "R Mapping Resources\n\nmapview\nLeaflet for R\ntmap\nZev Ross Creating beautiful demographic maps in R with the tidycensus and tmap packages\nGeocomputation with R: Making maps with R\nNico Hahn: Making Maps with R R"
  },
  {
    "objectID": "resources.html#web-services-in-r",
    "href": "resources.html#web-services-in-r",
    "title": "Resources",
    "section": "Web Services in R",
    "text": "Web Services in R\n\nAccessing REST API (JSON data) using httr and jsonlite\nWorking with Geospatial Hydrologic Data Using Web Services (R)"
  },
  {
    "objectID": "resources.html#general-r-resources",
    "href": "resources.html#general-r-resources",
    "title": "Resources",
    "section": "General R Resources",
    "text": "General R Resources\n\nGoogle R Style Guide\nAdvanced R by Hadley Wickham"
  },
  {
    "objectID": "foundations.html#why-r-for-spatial-analysis",
    "href": "foundations.html#why-r-for-spatial-analysis",
    "title": "1  Foundations",
    "section": "1.3 Why R for Spatial Analysis",
    "text": "1.3 Why R for Spatial Analysis\n\nR is:\n\nlightweight\nopen-source\ncross-platform\n\nWorks with contributed packages - currently 19897\n\nprovides extensibility\n\nAutomation and recording of workflow\nprovides reproducibility\nOptimized work flow - data manipulation, analysis and visualization all in one place\n\nprovides integration\n\nR does not alter underlying data - manipulation and visualization in memory\nR is great for repetitive graphics\nR is great for integrating different aspects of analysis - spatial and statistical analysis in one environment\n\nagain, integration\n\nLeverage statistical power of R (i.e. modeling spatial data, data visualization, statistical exploration)\nCan handle vector and raster data, as well as work with spatial databases and pretty much any data format spatial data comes in\nR’s GIS capabilities growing rapidly right now - new packages added monthly - currently about 275 spatial packages (depending on how you categorize)\n\nSome drawbacks to using R for GIS work\n\nR not as good for interactive use as desktop GIS applications like ArcGIS or QGIS (i.e. editing features, panning, zooming, and analysis on selected subsets of features)\nExplicit coordinate system handling by the user\n\nno on-the-fly projection support\n\nIn memory analysis does not scale well with large GIS vector and tabular data\nSteep learning curve\nUp to you to find packages to do what you need - help not always great"
  },
  {
    "objectID": "foundations.html#background-on-data-structures",
    "href": "foundations.html#background-on-data-structures",
    "title": "1  Foundations",
    "section": "1.2 Background on data structures",
    "text": "1.2 Background on data structures\n\n\n\n\n\n\nNote\n\n\n\nMuch of this background on data structures is borrowed from Mike Johnson’s Introduction to Spatial Data Science and lecture material from our AWRA 2022 Geo Workshop\n\n\nBefore we dive into spatial libraries, it’s worth a quick review of relevant data structures in R - this will be a whirlwind overview, assuming most everyone is familiar with using R already.\n\nYou, computers, and software ‘understand’ values in particular and different ways\nComputers convert bytes –> hex –> value\n\nHumans read values\nSoftware reads Hex bytes\nHardware reads Binary bytes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat’s the difference between 10 and ‘10’?\n\nTo us: meaning\nTo software: how it is handled\nTo a computer: nothing\n\nWe need human-defined (computer-guessable) data types\n\n\nFundamental data types\n\n# Numeric\ntypeof(1.9)\n#> [1] \"double\"\n# Integer\ntypeof(1L)\n#> [1] \"integer\"\n# Boolean\ntypeof(TRUE)\n#> [1] \"logical\"\n# Character\ntypeof(\"Welcome\")\n#> [1] \"character\"\n\nStoring more than one value requires a structure.\nValues can be structured in ways such as:\n\nvectors\ndimensions/attributes\ndata.frames\n\nAnd data.frames can be operated on with functions such as:\n\nfilter\nselect\nmutate\nsummarize\ngroup_by\n\n\n1.2.1 Vectors\n\n\n\n\n\n\nNote\n\n\n\n‘vector’ has two meanings when working with GIS data and working in R!\n\ngeographic vector data (which we’ll explore)\nvector class (what we’re talking about here)\n\ngeographic vector data is a data model, and the vector class is an R class like data.frame and matrix\n\n\nVectors come in two flavors:\n\natomic\nlist\n\n\natomic vectors elements must have the same type\nlists elements can have different types\n\n\n1.2.1.1 Atomic vectors\n\n# Numeric\ndbl_vec = c(1.9, 2, 3.5)\ntypeof(dbl_vec)\n#> [1] \"double\"\nlength(dbl_vec)\n#> [1] 3\n# Logical\nlg_vec = c(TRUE, FALSE, F, T)\ntypeof(lg_vec)\n#> [1] \"logical\"\nlength(lg_vec)\n#> [1] 4\n\nCoercion\n\ntype is a property of a vector\nWhen you try to combine different types they’ll be coerced in the following fixed order:\n\ncharacter => double => integer => logical\n\nCoercion occurs automatically but generates a warning and a missing value when it fails\n\n\nc(\"a\", 1)\n#> [1] \"a\" \"1\"\nc(\"a\", TRUE)\n#> [1] \"a\"    \"TRUE\"\nc(4.5, 1L)\n#> [1] 4.5 1.0\nc(\"1\", 18, \"GIS\")\n#> [1] \"1\"   \"18\"  \"GIS\"\nas.numeric(c(\"1\", 18, \"GIS\"))\n#> [1]  1 18 NA\nas.logical(c(\"1\", 18, \"GIS\"))\n#> [1] NA NA NA\n\nSubsetting atomic vectors\n\n# Atomic numeric vector\n(x = c(3.4, 7, 18, 9.6))\n#> [1]  3.4  7.0 18.0  9.6\n\n# Third Value\nx[3]\n#> [1] 18\n\n# Third and Fourth value\nx[c(3,4)]\n#> [1] 18.0  9.6\n\n# Drop the third value\nx[-3]\n#> [1] 3.4 7.0 9.6\n\n# Keep the 1 and 2 value, but drop 3 and 4\nx[c(T,T,F,F)]\n#> [1] 3.4 7.0\n\n\n\n\n1.2.2 Matrix\n\nA matrix is 2D atomic (row, column)\n\nSame data types\nSame column length\n\n\nThis is how spatial raster data is structured\nSubsetting matrices uses row,column (i,j) syntax\n\n(x = matrix(1:9, nrow = 3))\n#>      [,1] [,2] [,3]\n#> [1,]    1    4    7\n#> [2,]    2    5    8\n#> [3,]    3    6    9\nx[3,]\n#> [1] 3 6 9\nx[,3]\n#> [1] 7 8 9\nx[3,3]\n#> [1] 9\n\n\n\n1.2.3 Array\n\nArray is a 3d Atomic (row, column, slice)\n\nThis is how spatial raster data with a time dimension is structured\n\n(array(c(1:12), dim = c(3,2,2)))\n#> , , 1\n#> \n#>      [,1] [,2]\n#> [1,]    1    4\n#> [2,]    2    5\n#> [3,]    3    6\n#> \n#> , , 2\n#> \n#>      [,1] [,2]\n#> [1,]    7   10\n#> [2,]    8   11\n#> [3,]    9   12\n\nSubsetting arrays uses row, column, slice syntax (i,j,z)\n\n(x = array(1:12, dim = c(2,2,3)))\n#> , , 1\n#> \n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\n#> \n#> , , 2\n#> \n#>      [,1] [,2]\n#> [1,]    5    7\n#> [2,]    6    8\n#> \n#> , , 3\n#> \n#>      [,1] [,2]\n#> [1,]    9   11\n#> [2,]   10   12\nx[1,,]\n#>      [,1] [,2] [,3]\n#> [1,]    1    5    9\n#> [2,]    3    7   11\nx[,1,]\n#>      [,1] [,2] [,3]\n#> [1,]    1    5    9\n#> [2,]    2    6   10\nx[,,1]\n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\n\n\n\n1.2.4 Lists\nEash list element can be any data type\n\n(my_list <- list(\n  matrix(1:4, nrow = 2), \n  \"GIS is great!\", \n  c(TRUE, FALSE, TRUE), \n  c(2.3, 5.9)\n))\n#> [[1]]\n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\n#> \n#> [[2]]\n#> [1] \"GIS is great!\"\n#> \n#> [[3]]\n#> [1]  TRUE FALSE  TRUE\n#> \n#> [[4]]\n#> [1] 2.3 5.9\n\n\ntypeof(my_list)\n#> [1] \"list\"\n\nSubsetting Lists\n\nEach element of a list can be accessed with the [[ operator\n\n\nmy_list[[1]]\n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\nmy_list[[1]][1,2]\n#> [1] 3\n\n\n\n1.2.5 Data Frames\n\ndata.frame is built on the list structure in R\nlength of each atomic or list vector has to be the same\nThis gives data.frame objects rectangular structure so they share properties of both matrices and lists\n\n\n(df1 <- data.frame(name = c(\"Me\", \"Tim\", \"Sarah\"),\n                  age  = c(53,15,80),\n                  retired = c(F,F,T)))\n#>    name age retired\n#> 1    Me  53   FALSE\n#> 2   Tim  15   FALSE\n#> 3 Sarah  80    TRUE\ntypeof(df1)\n#> [1] \"list\"\n\nSubsetting a data.frame\n\ndf1[1,2]\n#> [1] 53\n\n# or like a list\ndf1[[2]]\n#> [1] 53 15 80\n\n# or with column name operator\ndf1$name\n#> [1] \"Me\"    \"Tim\"   \"Sarah\"\n\n\n\n1.2.6 Data manipulation\n\ndata.frame manipulation is all based on SQL queries\nR abstracts the SQL logic and provides function-ized methods\ndplyr in the tidyverse ecosystem provides the ’grammar of data manipulation` approach we’ll use in this workshop\n\nData manipulation verbs:\n\nPrimary:\n\nselect(): keeps or removes variables based on names\nfilter(): keeps or removes observations based on values _ Manipulation:\nmutate(): adds new variables that are functions of existing variables\nsummarise(): reduces multiple values down to a single summary\narrange(): changes ordering of the rows\n\nGrouping:\n\ngroup_by(): combine with any or all of the above to perform manipulation ‘by group’\n\n\n\n\n1.2.7 Pipe operator\n\nThe pipe operator (native R pipe operator |> or magrittr pipe operator %>%) provides a more concise and expressive coding experience\nThe pipe passes the object on the left hand side of the pipe into the first argument of the right hand function\nTo be |> compatible, the data.frame is ALWAYS the fist argument to dplyr verbs\n\nA demonstration using dataRetrieval package stream gage data from USGS:\n\nflows <- dataRetrieval::readNWISdv(siteNumbers = '14187200',\n                   parameterCd = \"00060\")  |>  \n  dataRetrieval::renameNWISColumns() \ndplyr::glimpse(flows)\n#> Rows: 18,326\n#> Columns: 5\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-…\n#> $ Flow      <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 92…\n#> $ Flow_cd   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the :: after the package name and before the function in the R code above doing? How would you find out more about it?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt disambiguates the function or method by selecting a definition from a particular namespace.\nYou can get help on it the same way you get help on anything in R:\n\n?'::'\n# or\nhelp(\"::\")\n\n\n\n\n\n\n1.2.8 Filter\n\nfilter() takes logical (binary) expressions and returns the rows in which all conditions are TRUE.\n\nFilter on a single condition:\n\nflows |> \ndplyr::filter(Flow > 900) |> \n  dplyr::glimpse()\n#> Rows: 14,435\n#> Columns: 5\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 1973-08-04, 1973-08-05, 1973-08-06, 1973-08-07, 1973-08-…\n#> $ Flow      <dbl> 930, 939, 939, 944, 932, 927, 925, 927, 928, 945, 938, 94…\n#> $ Flow_cd   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n\nOr multiple conditions:\n\nflows  |> \ndplyr::filter(Flow > 900, Date > as.Date(\"2010-01-01\"))  |>  \n  dplyr::glimpse()\n#> Rows: 4,385\n#> Columns: 5\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 2010-01-02, 2010-01-03, 2010-01-04, 2010-01-05, 2010-01-…\n#> $ Flow      <dbl> 7870, 6920, 5860, 7860, 10000, 10100, 9760, 9130, 8600, 7…\n#> $ Flow_cd   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n\n\n\n1.2.9 Select\n\nSubset variables (columns) you want to keep or exlcude by name\n\nJust keep three columns\n\nflows |> \ndplyr::select(Date, Flow)  |> \n  dplyr::glimpse()\n#> Rows: 18,326\n#> Columns: 2\n#> $ Date <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-05, 1…\n#> $ Flow <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 928, 94…\n\nExclude just one\n\nflows |> \ndplyr::select(-Flow_cd)  |> \n  dplyr::glimpse()\n#> Rows: 18,326\n#> Columns: 4\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-…\n#> $ Flow      <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 92…\n\n..And can rename while selecting\n\nflows |> \ndplyr::select(Date, flow_cfs = Flow)  |> \n  dplyr::glimpse()\n#> Rows: 18,326\n#> Columns: 2\n#> $ Date     <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-0…\n#> $ flow_cfs <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 928…\n\n\n\n1.2.10 Mutate\n\nmutate() defines and inserts new variables into a existing data.frame\nmutate() builds new variables sequentially so you can reference earlier ones when defining later ones\n\nWe can extract Year and Month as new variables from the Date variable using date time\n\nflows  |> \n  dplyr::select(Date, Flow)  |>  \n  dplyr::filter(Date > as.Date('2010-01-01'))  |> \n  dplyr::mutate(Year  = format(Date, \"%Y\"),\n         Month = format(Date, \"%m\"))  |>  \n  dplyr::glimpse()\n#> Rows: 5,023\n#> Columns: 4\n#> $ Date  <date> 2010-01-02, 2010-01-03, 2010-01-04, 2010-01-05, 2010-01-06, …\n#> $ Flow  <dbl> 7870, 6920, 5860, 7860, 10000, 10100, 9760, 9130, 8600, 7040,…\n#> $ Year  <chr> \"2010\", \"2010\", \"2010\", \"2010\", \"2010\", \"2010\", \"2010\", \"2010…\n#> $ Month <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"…\n\n\n\n1.2.11 Summarize and Group_By\n\nsummarize() allowa you to summarize across all observations\ngroup_by() allows you to apply any of these manipulation verbs by group in your data\n\n\nflows |> \n dplyr::select(Date, Flow) |> \n dplyr::mutate(Year  = format(Date, \"%Y\"))  |> \n dplyr::group_by(Year) |> \n dplyr::summarize(meanQ = mean(Flow),\n           maxQ = max(Flow))\n#> # A tibble: 51 × 3\n#>    Year  meanQ  maxQ\n#>    <chr> <dbl> <dbl>\n#>  1 1973  4669. 13200\n#>  2 1974  3659. 14400\n#>  3 1975  3611. 14100\n#>  4 1976  2340. 15000\n#>  5 1977  2860. 16200\n#>  6 1978  2206. 11300\n#>  7 1979  2378. 12000\n#>  8 1980  2548. 14700\n#>  9 1981  2976. 17000\n#> 10 1982  3424. 15100\n#> # ℹ 41 more rows\n\n\n\n\n\n\n\nNote\n\n\n\nYou may click on any of the functions in this book to be directed to their respective documentation. For example, clicking on st_join() takes you to the documentation page for the st_join() function on the sf website."
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "3  Visualization",
    "section": "",
    "text": "Understand how to use several of the most popular libraries for plotting and visualizing spatial data inR\nThis is goal 2.\nThis is goal 3."
  }
]