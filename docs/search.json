[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Data Workshop",
    "section": "",
    "text": "Welcome\nHello and welcome! The purpose of this site is to provide workshop materials for the Spatial Data workshop at the 2023 EPA R User Group Workshop. Slides that accompany this workshop are available for download linked here by clicking the “Download raw file” button via the ellipsis or downward arrow symbol on the right side of the screen."
  },
  {
    "objectID": "index.html#workshop-agenda",
    "href": "index.html#workshop-agenda",
    "title": "Spatial Data Workshop",
    "section": "Workshop Agenda",
    "text": "Workshop Agenda\n\n1:00pm - 1:35pm EDT: Introduction and Spatial Data Structures in R\n1:35pm - 2:05pm EDT: Vector Data Model\n2:05pm - 2:10pm EDT: Break\n2:10pm - 2:30pm EDT: Raster Data Model\n2:30pm - 3:15pm EDT: Coordinate Reference Systems and I / O\n3:15pm - 3:25pm EDT: Break\n3:25pm - 3:45pm EDT: Spatial Data Visualization\n3:45pm - 4:15pm EDT: Geoprocecessing\n4:15pm - 4:20pm EDT: Break\n4:20pm - 5:00pm EDT: Advanced Applications"
  },
  {
    "objectID": "index.html#author-introduction",
    "href": "index.html#author-introduction",
    "title": "Spatial Data Workshop",
    "section": "Author Introduction",
    "text": "Author Introduction\nMarc Weber is a geographer at the Pacific Ecological Systems Division (PESD) at the United States Environmental Protection Agency (USEPA). His work supports various aspects of the USEPA’s National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States, and he was one of the developers of and maintains the StreamCat and LakeCat datasets. His work focuses on spatial analysis in R and Python, Geographic Information Science (GIS), aquatic ecology, remote sensing, open source science and environmental modeling.\nMichael Dumelle is a statistician for the United States Environmental Protection Agency (USEPA). He works primarily on facilitating the survey design and analysis of USEPA’s National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States. His primary research interests are in spatial statistics, survey design, environmental and ecological applications, and software development."
  },
  {
    "objectID": "index.html#set-up",
    "href": "index.html#set-up",
    "title": "Spatial Data Workshop",
    "section": "Set Up",
    "text": "Set Up\nThe packages that we use throughout this workshop are listed below. To install them run:\n\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"cranlogs\")\ninstall.packages(\"sf\")\ninstall.packages(\"cowplot\")\ninstall.packages(\"remotes\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"spData\")\ninstall.packages(\"spDataLarge\", repos = \"https://nowosad.r-universe.dev\")\nremotes::install_github(\"mhweber/Rspatialworkshop\")\nremotes::install_github(\"mhweber/awra2020spatial\")\ninstall.packages(\"rnaturalearth\")\ndevtools::install_github(\"ropensci/rnaturalearthdata\")\ndevtools::install_github(\"ropensci/rnaturalearthhires\")\ninstall.packages(\"osmdata\")\ninstall.packages(\"mapview\")\ninstall.packages(\"tigris\")\ninstall.packages(\"tidycensus\")\ninstall.packages(\"tmap\")\ninstall.packages(\"tmaptools\")\nremotes::install_github(\"mikejohnson51/AOI\")\ninstall.packages(\"ggspatial\")\ninstall.packages(\"dataRetrieval\")\ninstall.packages(\"stars\")\ninstall.packages(\"elevatr\")\ninstall.packages(\"nhdplusTools\")\ninstall.packages(\"terra\")\ninstall.packages(\"raster\")"
  },
  {
    "objectID": "index.html#how-to-follow-along-with-material",
    "href": "index.html#how-to-follow-along-with-material",
    "title": "Spatial Data Workshop",
    "section": "How to follow along with material",
    "text": "How to follow along with material\nThis workshop was built using Quarto and rendered to html. If you are familiar with using git and GitHub, you can fork and clone this repository, or simply clone directly and open the corresponding .qmd files to follow along with material in RStudio. You can also copy code snippets from the rendered book site and paste into your code files in RStudio.\nQuarto is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and it can render most existing Rmd files without modification. Like R Markdown the benefits include:\n\nAllows for reproducible reporting from R\nYou write your document in markdown and embed executable code chunks using the knitr syntax\nYou can update your document at any time by re-knitting the code chunks and convert your document to a number of formats (i.e. html, pdf, word documents)\nWe assume everyone in the workshop is familiar with using RStudio"
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Spatial Data Workshop",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe views expressed in this manuscript are those of the authors and do not necessarily represent the views or policies of the U.S. Environmental Protection Agency. Any mention of trade names, products, or services does not imply an endorsement by the U.S. government or the U.S. Environmental Protection Agency. The U.S. Environmental Protection Agency does not endorse any commercial products, services, or enterprises."
  },
  {
    "objectID": "foundations.html#goals-and-outcomes",
    "href": "foundations.html#goals-and-outcomes",
    "title": "1  Foundations",
    "section": "\n1.1 Goals and Outcomes",
    "text": "1.1 Goals and Outcomes\n\nUnderstand fundamental spatial data structures and libraries in R.\nBecome familiar with coordinate reference systems.\nGeographic I/O (input/output)"
  },
  {
    "objectID": "foundations.html#spatial-data-structures-in-r",
    "href": "foundations.html#spatial-data-structures-in-r",
    "title": "1  Foundations",
    "section": "\n1.4 Spatial Data Structures in R",
    "text": "1.4 Spatial Data Structures in R\nFirst we’ll walk through spatial data structures in R and review some key components of data structures in R that inform our use of spatial data in R.\n\n\n\n\n\n\nNote\n\n\n\nA few core libraries underpin spatial libraries in R (and Python!) and in GIS software applications such as QGIS and ArcPro. Spatial data structures across languages and applications are primarily organized through OSgeo and OGC). These core libraries include:\n\n\nGDAL –> For raster and feature abstraction and processing\n\nPROJ –> A library for coordinate transformations and projections\n\nGEOS –> A Planar geometry engine for operations (measures, relations) such as calculating buffers and centroids on data with a projected CRS\n\nS2 –> a spherical geometry engine written in C++ developed by Google and adapted in R with the s2 package\n\n\n\nWe’ll see how these core libraries are called and used in the R packages we explore throughout this workshop.\n\n\n\n\n\n\nNote\n\n\n\n\nVector data are comprised of points, lines, and polygons that represent discrete spatial entities, such as a river, watershed, or stream gauge.\nRaster data divides spaces into rectilinear cells (pixels) to represent spatially continuous phenomena, such as elevation or the weather. The cell size (or resolution) defines the fidelity of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.1 Vector Data Model\nFor Vector data, Simple Features (officially Simple Feature Access) is both an OGC and International Organization for Standardization (ISO) standard that specifies how (mostly) two-dimensional geometries can represent and describe objects in the real world. The Simple Features specification includes:\n\na class hierarchy\na set of operations\nbinary and text encodings\n\nIt describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.\nIt outlines how the spatial elements of POINTS (XY locations with a specific coordinate reference system) extend to LINES, POLYGONS and GEOMETRYCOLLECTION(s).\nThe “simple” adjective also refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines that do not self-intersect.\n\n\n\n\nFigure 1.2: The simple features data model.\n\n\n\n\nWe’ll load the sf library to load the classes and functions for working with spatial vector data and to explore how sf handles vector spatial data in R:\n\nlibrary(sf)          \n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the message you see in your console the first time you load sf? Hint: If you don’t see a message, you probably already have the sf library attached - you can uncheck it in your packages pane in RStudio, or you can run detach(\"package:sf\", unload = TRUE)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe output from library(sf) reports which versions of key geographic libraries such as GEOS the package is using\n\n\n\nYou can report on versions of external software used by sf with:\n\nsf::sf_extSoftVersion()\n#>           GEOS           GDAL         proj.4 GDAL_with_GEOS     USE_PROJ_H \n#>        \"3.9.3\"        \"3.5.2\"        \"8.2.1\"         \"true\"         \"true\" \n#>           PROJ \n#>        \"8.2.1\"\n\nWe’ll cover highlights of the functionality of sf but we encourage you to read the sf package website documentation r-spatial.github.io/sf which can be viewed offline in RStudio using:\n\nvignette(package = \"sf\") # see which vignettes are available\nvignette(\"sf1\")          # open the introduction vignette\n\nThe sfg class in sf represents the different simple feature geometry types in R: point, linestring, polygon (and ‘multi’ equivalents such as multipoints) or geometry collection. Vector data in R and sf can be broken down into three basic geometric primitives with equivalent sf functions to create them:\n\npoints - st_points()\n\nlinestrings - st_linestring()\n\npolygons - st_polygon()\n\n\nWe buld up the more complex simple feature geometries with: - st_multipoint() for multipoints - st_multilinestring() for multilinestrings - st_multipolygon() for multipolygons - st_geometrycollection() for geometry collections\nTypically we don’t build these up from scratch ourselves but it’s important to understand these building blocks since primitives are the building blocks for all vector features in sf\n\n\n\n\n\n\n\nAll functions and methods in sf that operate on spatial data are prefixed by st_, which refers to spatial type.\nThis is similar to PostGIS\n\nThis makes them sf functions easily discoverable by command-line completion.\nSimple features are also native R data, using simple data structures (S3 classes, lists, matrix, vector).\n\n\n\n\nThese primitives can all be broken down into set(s) of numeric x,y coordinates with a known coordinate reference system (crs). #### Points - A point in sf is composed of one coordinate pair (XY) in a specific coordinate system. - A POINT has no length, no area and a dimension of 0. - z and m coordinates As well as having the necessary X and Y coordinates, single point (vertex) simple features can have:\n\na Z coordinate, denoting altitude, and/or\nan M value, denoting some “measure”\n\n\n# create sf object as point\npt1 <- st_point(c(1,3))\npt2 <- st_point(c(2,1))\npt3 <- st_point(c(3,2))\nsf_point <- st_sfc(pt1, pt2, pt3)\n\n# POINT defined as numeric vector\n(sf::st_dimension(sf_point))\n#> [1] 0 0 0\n\n\nlibrary(ggplot2)\nggplot(sf_point) + \n  geom_sf(color = \"red\") +\n  labs(x = \"X\", y = \"Y\")  \n\n\n\n\n\n1.4.1.1 Lines\n\nA polyline is composed of an ordered sequence of two or more POINTs\nPoints in a line are called vertices/nodes and explicitly define the connection between two points.\nA LINESTRING has a length, has no area and has a dimension of 1 (length)\n\n\n# create sf object as linestring\nline1 <- sf::st_linestring(rbind(pt1, pt2))\nline2 <- sf::st_linestring(rbind(pt2, pt3))\nsf_linestring <- st_sfc(line1, line2)\n\n# dimension\n(sf::st_dimension(sf_linestring))\n#> [1] 1 1\n\n\nggplot(sf_linestring) + \n  geom_sf() +\n  labs(x = \"X\", y = \"Y\")\n\n\n\n\n\n1.4.1.2 Polygon\n\nA POLYGON is composed of 4 or more points whose starting and ending point are the same.\nA POLYGON is a surface stored as a list of its exterior and interior rings.\nA POLYGON has length, area, and a dimension of 2. (area)\n\n\npt4 <- pt1\npoly1 <- st_polygon(list(rbind(pt1, pt2, pt3, pt4)))\nsf_polygon <- st_sfc(poly1)\n\n# dimension\nsf::st_dimension(sf_polygon)\n#> [1] 2\n\n\nggplot(sf_polygon) + \n  geom_sf(fill = \"grey\")\n\n\n\n\nThese geometries as we created above are structure using the WKT format. sf usesWKTandWKB` formats to store vector geometry.\n\n1.4.1.3 Points, Lines, Polygons\nWe visualize all geometries together by running\n\nggplot() +\n  geom_sf(data = sf_point, color = \"red\") +\n  geom_sf(data = sf_linestring) +\n  geom_sf(data = sf_polygon, fill = \"grey\")\n\n\n\n\n:::{.callout-note} WKT (Well-Known Text) and WKB\n-Well-known text (WKT) encoding provide a human-readable description of the geometry. - The well-known binary (WKB) encoding is machine-readable, lossless, and faster to work with than text encoding.\nWKB is used for all interactions with GDAL and GEOS.\n\n1.4.1.4 Spatial Data Frames\nBuilding up from basic points, lines and strings, simple feature objects are stored in a data.frame, with geometry data in a special list-column, usually named ‘geom’ or ‘geometry’ - this is the same convention used in the popular Python package GeoPandas.\nLet’s look at an example spatial data set that comes in the spData package, us_states:\n\nlibrary(spData)\ndata(\"us_states\")\nclass(us_states)\n#> [1] \"sf\"         \"data.frame\"\nnames(us_states)\n#> [1] \"GEOID\"        \"NAME\"         \"REGION\"       \"AREA\"        \n#> [5] \"total_pop_10\" \"total_pop_15\" \"geometry\"\n\nWe see that it’s an sf data frame with both attribute columns and the special geometry column we mentioned earlier - in this case named geometry.\nus_states$geometry is the ‘list column’ that contains the all the coordinate information for the US state polygons in this dataset.\nTo reiterate, us_states is an sf simple feature collection which we can verify with:\n\nus_states\n#> Simple feature collection with 49 features and 6 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -124.7042 ymin: 24.55868 xmax: -66.9824 ymax: 49.38436\n#> Geodetic CRS:  NAD83\n#> First 10 features:\n#>    GEOID        NAME   REGION             AREA total_pop_10 total_pop_15\n#> 1     01     Alabama    South 133709.27 [km^2]      4712651      4830620\n#> 2     04     Arizona     West 295281.25 [km^2]      6246816      6641928\n#> 3     08    Colorado     West 269573.06 [km^2]      4887061      5278906\n#> 4     09 Connecticut Norteast  12976.59 [km^2]      3545837      3593222\n#> 5     12     Florida    South 151052.01 [km^2]     18511620     19645772\n#> 6     13     Georgia    South 152725.21 [km^2]      9468815     10006693\n#> 7     16       Idaho     West 216512.66 [km^2]      1526797      1616547\n#> 8     18     Indiana  Midwest  93648.40 [km^2]      6417398      6568645\n#> 9     20      Kansas  Midwest 213037.08 [km^2]      2809329      2892987\n#> 10    22   Louisiana    South 122345.76 [km^2]      4429940      4625253\n#>                          geometry\n#> 1  MULTIPOLYGON (((-88.20006 3...\n#> 2  MULTIPOLYGON (((-114.7196 3...\n#> 3  MULTIPOLYGON (((-109.0501 4...\n#> 4  MULTIPOLYGON (((-73.48731 4...\n#> 5  MULTIPOLYGON (((-81.81169 2...\n#> 6  MULTIPOLYGON (((-85.60516 3...\n#> 7  MULTIPOLYGON (((-116.916 45...\n#> 8  MULTIPOLYGON (((-87.52404 4...\n#> 9  MULTIPOLYGON (((-102.0517 4...\n#> 10 MULTIPOLYGON (((-92.01783 2...\n\nWe see information on\n\nthe geometry type\nthe dimensionality\nthe bounding box\nthe coordinate reference system\nthe header of the attributes\n\nRemember that sf simple feature collections are composed of:\n\n\n\n\nThe simple features data sructure.\n\n\n\n\n\nrows of simple features (sf) that contain attributes and geometry - in green\neach of which have a simple feature geometry list-column (sfc) - in red\nwhich contains the underlying simple feature geometry (sfg) for each simple feature - in blue\n\nsf extends the generic plot function so that plot will naturally work with sf objects.\n\nplot(us_states)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nIs this the result you expected from plot()?\nInstead of making a single default map of your geometry, plot() in sf maps each variable in the dataset.\nHow would you plot just the geometry? Or just the two population variables? Try these variations on plot() with us_states.\nRead more about plotting in sf by running ?plot.sf()\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(us_states$geometry)\nplot(us_states[,c('total_pop_10','total_pop_15')])\n#or\nus_states |>\n  dplyr::select(total_pop_10, total_pop_15) |>\n  plot() \n\n\n\n\nThe alternative solution above highlights a key concept with sf - sf objects are ‘tidy-compliant’ and geometry is sticky - it carries along with objects implicitly when performing any tidy chained operations.\n\n\n\n\nThe simple features data data sructure.\n\n\n\n\nKeeping the geometry as part of regular data.frame objects in R (and in Python with GeoPandas!) has numerous advantages - for instance:\n\nsummary(us_states['total_pop_15'])\n#>   total_pop_15               geometry \n#>  Min.   :  579679   MULTIPOLYGON :49  \n#>  1st Qu.: 1869365   epsg:4269    : 0  \n#>  Median : 4625253   +proj=long...: 0  \n#>  Mean   : 6415823                     \n#>  3rd Qu.: 6985464                     \n#>  Max.   :38421464\n\nThe geometry is sticky, which means that it is carried along by default!\nYou can read sf objects from shapefiles using st_read() or read_sf().\n\n1.4.2 Raster Data Model\nRaster data can be continuous (e.g. elevation, precipitation, atmospheric deposition) or it can be categorical (e.g. land use, soil type, geology type). Raster data can also be image based rasters which can be single-band or multi-band. You can also have a temporal component with space-time cubes.\n\n\n\n\nThe raster data model.\n\n\n\n\nSupport for gridded data in R in recent year has been best implemented with the raster package by Robert Hijmans. The raster package allowed you to:\n\nread and write almost any commonly used raster data format\nperform typical raster processing operations such as resampling, projecting, filtering, raster math, etc.\nwork with files on disk that are too big to read into memory in R\nrun operations quickly since the package relies on back-end C code\n\nThe terra package is the replacement for the raster package and has now superceeded it and we will largely focus on terra here. Examples here draw from both Spatial Data Science with R and terra and An introduction to terra in Geocomputation with R. Use help(“terra-package”) in the console for full list of available terra functions and comparison to / changes from raster.\nRaster representation is currently in flux a bit in R now with three choices of packages - raster and now terra which we’ve mentioned, as well as stars (spatiotemporal tidy arrays with R).\nTo familiarize ourselves with the terra package, let’s create an empty SpatRaster object - in order to do this we have to:\n\ndefine the matrix (rows and columns)\ndefine the spatial bounding box\n\n\n\n\n\n\n\nExercise\n\n\n\nNote that typically we would be reading raster data in from a file rather than creating a raster from scratch.\nRun the code steps below to familiarize yourself with constructing a RasterLayer from scratch.\nTry providing a different bounding box for an area of your choosing or changing the dimensions and view result (by simply typing myrast in the console).\n\nlibrary(terra)\nmyrast <- rast(ncol = 10, nrow = 10, xmax = -116, xmin = -126, ymin = 42, ymax = 46)\nmyrast\n#> class       : SpatRaster \n#> dimensions  : 10, 10, 1  (nrow, ncol, nlyr)\n#> resolution  : 1, 0.4  (x, y)\n#> extent      : -126, -116, 42, 46  (xmin, xmax, ymin, ymax)\n#> coord. ref. : lon/lat WGS 84\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# just an example:\nmyrast <- rast(ncol=40, nrow = 40, xmax=-120, xmin=-136, ymin=52, ymax=56)\n\n\n\n\nterra uses an S4 slot structure with a SpatRaster object\n\nstr(myrast)\n#> S4 class 'SpatRaster' [package \"terra\"]\nisS4(myrast)\n#> [1] TRUE\n\nterra has dedicated functions addressing each of the following components: - dim(my_rast) returns the number of rows, columns and layers - ncell() returns the number of cells (pixels) - res() returns the spatial resolution - ext() returns spatial extent - crs() returns the coordinate reference system\n\n\n\n\n\n\nExercise\n\n\n\nExploring raster objects\n\nwhat is the minimal data required to define a SpatRaster?\nWhat is the CRS of our SpatRaster?\nHow do we pull out just the CRS for our SpatRaster?\nBuilding on this, what is the code to pull out just our xmin value in our extent, or bounding box?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnumber columns, number rows, and extent - terra will fill in defaults if values aren’t provided\n\n\nt <- rast()\nt\n#> class       : SpatRaster \n#> dimensions  : 180, 360, 1  (nrow, ncol, nlyr)\n#> resolution  : 1, 1  (x, y)\n#> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#> coord. ref. : lon/lat WGS 84\n\n\nWe didn’t provide one - terra uses default crs of WGS84 if you don’t provide a crs\n\n\n\ncrs(myrast)\n#> [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ID[\\\"EPSG\\\",6326]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n        ID[\\\"EPSG\\\",8901]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"longitude\\\",east,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433,\\n                ID[\\\"EPSG\\\",9122]]],\\n        AXIS[\\\"latitude\\\",north,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433,\\n                ID[\\\"EPSG\\\",9122]]]]\"\n\n\n\n\n\next(myrast)\n#> SpatExtent : -126, -116, 42, 46 (xmin, xmax, ymin, ymax)\n# just grab xmin\next(myrast)[1]\n#> xmin \n#> -126\n# we can see that extent is actually a c++ object in the ptr slot of our spatRaster object\n# names(myrast@ptr)\nmyrast$lyr.1@pnt$extent\n#> C++ object <000001d6ef132fe0> of class 'SpatExtent' <000001d6e116c090>\n\n\n\n\n\n1.4.2.1 Manipulating terra objects\nSo far we’ve just constructed a raster container with no values (try plotting what we have so far) - let’s provide values to the cells using the runif function to derive random values from the uniform distribution:\n\n#show we have no values\nhasValues(myrast)\n#> [1] FALSE\nvalues(myrast) <- runif(n=ncell(myrast))\nmyrast\n#> class       : SpatRaster \n#> dimensions  : 10, 10, 1  (nrow, ncol, nlyr)\n#> resolution  : 1, 0.4  (x, y)\n#> extent      : -126, -116, 42, 46  (xmin, xmax, ymin, ymax)\n#> coord. ref. : lon/lat WGS 84 \n#> source(s)   : memory\n#> name        :       lyr.1 \n#> min value   : 0.007553187 \n#> max value   : 0.983861732\n\nAn important point to make here is that objects in the terra package (and previously in raster) can be either in memory or on disk - note the value for our spatRaster r of ‘source’. If this were a large raster on disk, the value would be the path to the file on disk.\n\nmyrast$lyr.1@pnt$inMemory\n#> [1] TRUE\nhasValues(myrast)\n#> [1] TRUE\nmyrast$lyr.1@pnt$nlyr() # we just have one layer in our object\n#> [1] 1\n# or\nnlyr(myrast)\n#> [1] 1\n\nterra also provides plot method for it’s classes:\n\nplot(myrast)\n\n\n\n\nWe can also overwrite the cell values for our raster:\n\nvalues(myrast) <- 1:ncell(myrast)\nvalues(myrast)[1:15]\n#>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\nYou can access raster values via direct indexing or line, column indexing - take a minute to see how this works using raster r we just created - the syntax is:\nmyrast[i]\nmyrast[line, column]\n\n\n\n\n\n\nExercise\n\n\n\nHow is terra data storage unlike a matrix in R? Try creating a matrix with same dimensions and values and compare with the terra raster you’ve created:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nm <- matrix (1:100, nrow=10, ncol=10)\nm[1,10]\n#> [1] 91\nmyrast[1,10]\n#>   lyr.1\n#> 1    10\nmyrast[10]\n#>   lyr.1\n#> 1    10\n\n\n\n\n\n1.4.2.2 Reading existing rasters on disk\n\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nmy_rast = rast(raster_filepath)\nnlyr(my_rast)\n#> [1] 1\nplot(my_rast)\n\n\n\n\n\n1.4.3 Multiband rasters\nThe spatRaster object in terra can hold multiple layers (similar to RasterBrick and RasterStack which were two additional classes in the raster package). These layers correspond to multispectral satellite imagery or a time-series raster.\n\nlandsat = system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nlandsat = rast(landsat)\nlandsat\n#> class       : SpatRaster \n#> dimensions  : 1428, 1128, 4  (nrow, ncol, nlyr)\n#> resolution  : 30, 30  (x, y)\n#> extent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\n#> coord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \n#> source      : landsat.tif \n#> names       : landsat_1, landsat_2, landsat_3, landsat_4 \n#> min values  :      7550,      6404,      5678,      5252 \n#> max values  :     19071,     22051,     25780,     31961\nplot(landsat)\n\n\n\n\n\n1.4.3.1 Raster data with stars\n\nstars works with and stands for spatio-temporal arrays and can deal with more complex data types than either raster or terra such as rotated grids.\nstars integrates with sf and many sf functions have methods for stars objects (i.e. st_bbox and st_transform) - this makes sense since they are both written by Edzer Pebesma. terra unfortunately has poor / no integration with sf - this is a big issue for me personally and I will likely look to stars long-term for my raster processing.\nBasic example shown in stars vignette - reading in the 30m bands of a Landsat-7 image that comes with the stars package:\n\nlibrary(stars)\ntif = system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nls7 = read_stars(tif)\nplot(ls7, axes = TRUE)\n\n\n\n\nls7 (landsat7) is an object with 3 dimensions (x,y and band) and 1 attribute\n\nls7\n#> stars object with 3 dimensions and 1 attribute\n#> attribute(s):\n#>              Min. 1st Qu. Median     Mean 3rd Qu. Max.\n#> L7_ETMs.tif     1      54     69 68.91242      86  255\n#> dimension(s):\n#>      from  to  offset delta                     refsys point x/y\n#> x       1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\n#> y       1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n#> band    1   6      NA    NA                         NA    NA\n\nNote when we plotted above that the plot method with stars uses histogram stretching across all bands - we can also do stretching for each band individually:\n\nplot(ls7, axes = TRUE, join_zlim = FALSE)\n\n\n\n\nstars uses tidyverse methods for spatio-temporal arrays - an example of this is pulling out one band of an image using slice\n\nls7  |>  dplyr::slice(band, 6) -> band6\nband6\n#> stars object with 2 dimensions and 1 attribute\n#> attribute(s):\n#>              Min. 1st Qu. Median     Mean 3rd Qu. Max.\n#> L7_ETMs.tif     1      32     60 59.97521      88  255\n#> dimension(s):\n#>   from  to  offset delta                     refsys point x/y\n#> x    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\n#> y    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\nThis gives us a lower-dimensional array of just band 6 from the Landsat7 image."
  },
  {
    "objectID": "geoprocessing.html#goals-and-outcomes",
    "href": "geoprocessing.html#goals-and-outcomes",
    "title": "3  Geoprocessing",
    "section": "\n3.1 Goals and Outcomes",
    "text": "3.1 Goals and Outcomes\n\nLearn about fundamental spatial operations in R using spatial predicates in sf for:\n\nsubsetting\nspatial join\nbuffer\nlogical set operations - union / intersection\nclipping\ngenerating centroids and ‘casting’ to other geometry types\nother raster from geomputation?\nchapter 4 in geocomputation?\n\n\nExplore performing ‘map algebra’ type operations with raster data in R\nLearn how to do extract and zonal operations in R"
  },
  {
    "objectID": "geoprocessing.html#spatial-subsetting",
    "href": "geoprocessing.html#spatial-subsetting",
    "title": "3  Geoprocessing",
    "section": "\n3.4 Spatial Subsetting",
    "text": "3.4 Spatial Subsetting\nSpatial subsetting is analogous to attribute subsetting - with sf objects, we can use square bracket ([]) notation to take a spatial object and return a new object that contains only the features that relate in space to another spatial object (i.e. are within, intersect, are within distance of, are spatially disjoint, etc.).\nWe use the simple syntax of x[y,] to perform a spatial subset with the default operation of ‘intersects’: x[y,] is identical to x[y, , op=st_intersects]. We could also provide a different spatial predicate such as x[y, , op = st_disjoint].\nLet’s run through a couple examples.\nFirst we can demonstrate some very simple spatial subsetting examples using a particular county (polygon) in Oregon and Oregon cities (points).\n\nlibrary(AOI)\nlibrary(mapview)\nlibrary(sf) |> suppressPackageStartupMessages()\nmapviewOptions(fgb=FALSE)\nor_cities <- read_sf(system.file(\"extdata/cities.shp\", package = \"Rspatialworkshop\"))\nMultCnt<- aoi_get(state = \"OR\", county= \"Multnomah\") |> \n  st_transform(st_crs(or_cities)) # project counties to cities\nmapview(MultCnt, alpha.regions=.07, color='black', lwd=2) + mapview(or_cities)\n\n\n\n\n\n\nSpatially subset cities within Multnomah County:\n\nmult_cities <- or_cities[MultCnt,]\n# or\nmult_cities <- or_cities[MultCnt, , op=st_intersects]\nmapview(MultCnt, alpha.regions=.07, color='black', lwd=2) + mapview(mult_cities)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nGiven our explanation above of using different spatial predicates in subsetting operations (e.g. st_disjoint, st_contains, st_is_within_distance) try:\n\nselecting and mapping cities outside of Multnomah county\nselecting and mapping cities within 50 kilometers of Multnomah county (or whatever distance you like)\nusing an attribute selection to select a city (or several cities) by name somewhere else in the state. Using these cities, how would we select and map just the county the contains this city / cities? Hint: we’ll need to pull in all counties of Oregon by modifying AOI_get()\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOutside Multnomah county:\n\nout_mult_cities <- or_cities[MultCnt, , op=st_disjoint]\nmapview(MultCnt, alpha.regions=.07, color='black', lwd=2) + mapview(out_mult_cities)\n\n\n\n\n\n\nCities within 50km of Multnomah county:\n\nclose_mult_cities <- or_cities[MultCnt, , op=st_is_within_distance,dist=50000]\nmapview(MultCnt, alpha.regions=.07, color='black', lwd=2) + mapview(close_mult_cities)\n\n\n\n\n\n\nCounty that contains selected cities:\n\ncounties <- aoi_get(state='Oregon', county='all') |> \n  st_transform(st_crs(or_cities))\nmy_cities <- or_cities |> \n  dplyr::filter(CITY %in% c('ASHLAND','BURNS','HOOD RIVER'))\nsel_counties <- counties[my_cities, , op=st_contains]\nmapview(sel_counties, alpha.regions=.07, color='black', lwd=2) + mapview(my_cities)"
  },
  {
    "objectID": "geoprocessing.html#spatial-join",
    "href": "geoprocessing.html#spatial-join",
    "title": "3  Geoprocessing",
    "section": "\n3.5 Spatial Join",
    "text": "3.5 Spatial Join\nOften we want to join information from one spatial dataset to another based on a spatial relationship rather than attribute relationships - joining attribute data should be a familiar concept to everyone and many of the principles are the same. st_join will add new columns from from a source spatial dataset to the target spatial dataset.\nBy default st_join performs a left join (all rows in the target including rows with no match in the source data) - but you can also do an inner join by setting left=FALSE.\nWe can demonstrate the most basic spatial join using our Oregon counties and cities data - here we simply get the county name for every city based on what county each city lands in.\n\ncty_cnty <- st_join(or_cities, counties['name'])\ndplyr::glimpse(cty_cnty)\n\nRows: 898\nColumns: 8\n$ AREA      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ PERIMETER <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ CITIES_   <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ CITIES_ID <dbl> 1658, 1368, 1366, 1382, 1384, 1380, 1376, 1370, 1378, 1386, …\n$ CITY      <chr> \"MULINO\", \"HAMMOND\", \"FORT STEVENS\", \"GLIFTON\", \"BRADWOOD\", …\n$ FLAG      <int> 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, …\n$ geometry  <POINT [foot]> POINT (776899.8 1272019), POINT (439320.8 1638725),…\n$ name      <chr> \"Clackamas\", \"Clatsop\", \"Clatsop\", \"Clatsop\", \"Clatsop\", \"Cl…\n\n\nThis is equivalent to:\n\ncty_cnty <- st_join(or_cities, counties['name'], .predicate=st_intersects)\ndplyr::glimpse(cty_cnty)\n\nRows: 898\nColumns: 8\n$ AREA      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ PERIMETER <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ CITIES_   <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ CITIES_ID <dbl> 1658, 1368, 1366, 1382, 1384, 1380, 1376, 1370, 1378, 1386, …\n$ CITY      <chr> \"MULINO\", \"HAMMOND\", \"FORT STEVENS\", \"GLIFTON\", \"BRADWOOD\", …\n$ FLAG      <int> 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, …\n$ geometry  <POINT [foot]> POINT (776899.8 1272019), POINT (439320.8 1638725),…\n$ name      <chr> \"Clackamas\", \"Clatsop\", \"Clatsop\", \"Clatsop\", \"Clatsop\", \"Cl…\n\n\n\n\n\n\n\n\nExercise\n\n\n\nOften we want to find what features are within a certain specified distance of other features, or what features are closest to a set of features. st_join can answer these questions by supplying the st_is_within_distance\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nSetting the projection for the desired units\n\n\ngages_ft <- st_transform(gages, 2994)\nst_distance(gages_ft[1,], gages_ft[2,])\n\nUnits: [foot]\n         [,1]\n[1,] 193550.5"
  },
  {
    "objectID": "geoprocessing.html#dissolve",
    "href": "geoprocessing.html#dissolve",
    "title": "3  Geoprocessing",
    "section": "\n3.5 Dissolve",
    "text": "3.5 Dissolve\nClarify what dissolve is and it’s not using st_dissolve, it’s just using tidyverse operations ## Fixing topology errors - show typical fixes for topology errors\n\n\n\n\n\n\nNote\n\n\n\nYou may encounter errors like this when running geoprocessing operations like st_join in R:\nError in wk_handle.wk_wkb(wkb, s2_geography_writer(oriented\n= oriented,  :  Loop 0 is not valid: Edge 772 crosses edge 774\nRunning st_make_valid might not fix.\nYou may need to turn off spherical geometry - sf_use_s2(TRUE), run st_make_valid, and then turn spherical geometry back on - sf_use_s2(FALSE) See background on S2 here and discussion of S2 related issues here"
  },
  {
    "objectID": "geoprocessing.html#extract-and-zonal-statistics",
    "href": "geoprocessing.html#extract-and-zonal-statistics",
    "title": "3  Geoprocessing",
    "section": "\n3.10 Extract and Zonal Statistics",
    "text": "3.10 Extract and Zonal Statistics"
  },
  {
    "objectID": "geoprocessing.html#fixing-topology-errors",
    "href": "geoprocessing.html#fixing-topology-errors",
    "title": "3  Geoprocessing",
    "section": "\n3.7 Fixing topology errors",
    "text": "3.7 Fixing topology errors\n\nshow typical fixes for topology errors\n\n\n\n\n\n\n\nNote\n\n\n\nYou may encounter errors like this when running geoprocessing operations like st_join in R:\nError in wk_handle.wk_wkb(wkb, s2_geography_writer(oriented\n= oriented,  :  Loop 0 is not valid: Edge 772 crosses edge 774\nRunning st_make_valid might not fix.\nYou may need to turn off spherical geometry - sf_use_s2(TRUE), run st_make_valid, and then turn spherical geometry back on - sf_use_s2(FALSE) See background on S2 here and discussion of S2 related issues here"
  },
  {
    "objectID": "visualization.html#goals-and-outcomes",
    "href": "visualization.html#goals-and-outcomes",
    "title": "2  Visualization",
    "section": "\n2.1 Goals and Outcomes",
    "text": "2.1 Goals and Outcomes\n\nGain familiarity with plotting and visualizing spatial data in R\nWork with four specific visualization and plotting libraries:\n\nggplot2\nleaflet\nmapview\ntmap\n\n\n\nR is fantastic for making publication quality static maps, and for generating repetitive graphics through scripts; we’ve already seen examples of how to make simple maps using base plotting,ggplot, and tmap. There are also several packages in R that link R code to plotting libraries developed in Javascript (or other languages) for interactive plotting and web integration.\nIt can be hard to decide which mapping packages to learn and use - some nice advice from Martin Tennekes who created tmap:\n\nIf you know some ggplot, don’t care about interactive maps, and don’t want to spend a lot of time learning new packages, use ggplot\n\nIf you want interactive maps as flexible as possible, use leaflet\n\nIf you want to simply explore spatial objects ineractively as easily as possible, use mapview\n\nOtherwise, use tmap!\n\nAlso, as pointed out in Spatial Data Science by Edzar Pebesma and Roger Bivand, ‘Every plot is a projection’ so it’s essential to have an understanding of coordinate reference systems and projections when visualizing spatial data in R - as they point out ‘any time we visualize, in any way, the world on a flat device, we project: we convert ellipsoidal coordinates into Cartesian coordinate’."
  },
  {
    "objectID": "visualization.html#mapview",
    "href": "visualization.html#mapview",
    "title": "2  Visualization",
    "section": "\n2.4 mapview",
    "text": "2.4 mapview\nMapview is a package designed for quick and easy interactive visualizations of spatial data - it makes use of leaflet but simplifies mapping functions compared to the leaflet package.\nIt’s easy to layer features with mapview - you can supply a list of objects to mapview or use + syntax as with ggplot.\nHere we’ll plot stream gages within Benton County:\n\nlibrary(Rspatialworkshop)\nlibrary(mapview)\nmapviewOptions(fgb=FALSE)\nlibrary(readr)\nlibrary(sf)\nlibrary(tigris)\ncounties <- counties(\"Oregon\", cb = TRUE)\nbenton <- counties[counties$NAME=='Benton',]\nfpath <- system.file(\"extdata\", \"Gages_flowdata.csv\", package=\"Rspatialworkshop\")\n\ngages <- read_csv(fpath,show_col_types = FALSE)\ngages_sf <- gages %>%\n  st_as_sf(coords = c(\"LON_SITE\", \"LAT_SITE\"), crs = 4269, remove = FALSE) \nst_crs(gages_sf)==st_crs(benton)\n# remember spatial indexing from Geoprocessing section?\ngages_benton <- gages_sf[benton,]\nmapview(gages_benton) + benton\n\nWe can also use handy convenience packages like the AOI package by Mike Johnson for flexible, term based geocoding to return areas of interest as sf objects and map them:\n\nAOI::aoi_get(list(\"Corvallis, OR\", 10, 10)) |> mapview()\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nGlance through the mapview basics and adjust legend and attributes. Take a look at mapview advanced controls as well and try plotting stations and the county polygon together, this time modifying features such as thicker black outline and transparent fill for the county outline and colorizing gages by their average flow (‘AVE’).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmapview(gages_benton,zcol='AVE') + mapview(benton, alpha.regions=.07, color='black', lwd=2)\n\n\n\n\n\n\n\n\n\n\n2.4.1 Adding Web Map services in mapview\n\nWe’ll visualize data with mapview and load a web map service layers alongside using mapview and underlying leaflet functionality.\nFirst we load load an excel file containing coordinate information in a known projection and promote to an sf spatial data frame.\n\nlibrary(Rspatialworkshop)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(mapview)\nfpath <- system.file(\"extdata\", \"Station_Locations.xlsx\", package=\"Rspatialworkshop\")\nstations <- read_xlsx(fpath)\nglimpse(stations)\n\nRows: 31\nColumns: 3\n$ Station <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",…\n$ x       <dbl> -2140749, -2140111, -2124688, -2125545, -1664112, 1606578, -17…\n$ y       <dbl> 2502887, 2469697, 2533842, 2556987, 2770644, 2698398, 2664873,…\n\nsummary(stations$x)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-2259078 -2124688 -1561956 -1630593 -1454137  1606578        2 \n\n# common clean up steps for spatial data - we can't use data missing coordinates so drop those records\nstations <- stations[complete.cases(stations),]\n# often spatial data in projected coordinates will have missing negative values for x values - common thing to fix:\nstations$x[stations$x > 0] <- 0 - stations$x[stations$x > 0]\nstations <- stations  |>  \n  st_as_sf(coords = c(\"x\", \"y\"), remove = FALSE)\n\n# in this case we know the particular Albers projection and have the information as a proj string\nst_crs(stations) <- \"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\" \n\nBasic interactive map of our spatial stations with mapview:\n\nmapview(stations)\n\n\n\n\n\n\nHere we’ll load a web map servcice (WMS) for the National Hydrography dataset. We’re looking at stream stations so imagine we want to visualize how closely these sites match a known rivers and stream network:\n\nlibrary(leaflet)\n# create a mapview object with our stations:\nm <- mapview(stations, legend=FALSE)\n\n# we configure the map attribute of our mapview object - try:\n# 'attributes(m) \n# to see those attributes\n\n#  The map attribute for mapview accepts leaflet methods - in this case we use addWMSTiles to add web map service tiles to the map\nm@map <- m@map  |>  addWMSTiles(group = 'NHDPlus',\n                              \"https://watersgeo.epa.gov/arcgis/services/NHDPlus_NP21/NHDSnapshot_NP21/MapServer/WmsServer?\",\n                              layers  = 4,\n                              options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n                              attribution = \"\")  |>  addWMSTiles(group = 'NHDPlusHR',\n                                                                \"https://hydro.nationalmap.gov/arcgis/services/NHDPlus_HR/MapServer/WMSServer?\",\n                                                                layers  = 9,\n                                                                options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n                                                                attribution = \"\")   |>  mapview:::mapViewLayersControl(names = c(\"NHDPlus\",\"NHDPlusHR\"))\nm"
  },
  {
    "objectID": "visualization.html#leaflet",
    "href": "visualization.html#leaflet",
    "title": "2  Visualization",
    "section": "\n2.3 leaflet",
    "text": "2.3 leaflet\nLeaflet is an extremely popular open-source javascript library for interactive web mapping, and the leaflet R package allows R users to create Leaflet maps from R. Leaflet can plot sf or sp objects, or x / y coordinates, and can plot points, lines or polygons. There are a number of base layers you can choose from. It’s worth spending some time exploring the excellent Leaflet for R site.\nHere we make a simple leaflet map of our the location of the EPA Pacific Ecological Systems Division lab in Corvallis where I work with a custom popup we create:\n\nlibrary(leaflet)\ncontent <- paste(sep = \"<br/>\",\n  \"<b><a href='https://www.epa.gov/greeningepa/pacific-ecological-systems-division-pesd-laboratory'>EPA Lab Corvallis</a></b>\",\n  \"200 S.W. 35th Street \",\n  \"Corvallis, OR 97333 \"\n)\n\nleaflet()  |>  addTiles()  |> \n  addPopups(-123.290391, 44.565548,  content,\n    options = popupOptions(closeButton = FALSE)\n  )"
  },
  {
    "objectID": "visualization.html#ggplot2",
    "href": "visualization.html#ggplot2",
    "title": "2  Visualization",
    "section": "\n2.2 ggplot2",
    "text": "2.2 ggplot2\nggplot2 now has support for geom_sf that was developed in conjunction with the development of sf and helps creating publication quality maps directly using sf objects. An introduction to this is found in Moreno and Basille (2018).\nHere we’ll show some of the useful functionality of ggplot2 with geom_sf objects pulling census and American Community Survey data using the tidycensus package.\n\n\n\n\n\n\nNote\n\n\n\nNote that to use tidycensus you’ll need to set your Census API key. A key can be obtained from here.\n\n\n\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(cowplot)\nmult_tracts <- get_acs(state='OR',county='Multnomah',geography='tract', variables=c('B19013_001','B16010_028','B01003_001'), geometry=TRUE)  \n\n# tidy data\nmult_wide <- mult_tracts |> \n  sf::st_transform(2153) |> #UTM 11N\n  dplyr::filter(!is.na(estimate)) |> \n  tidyr::pivot_wider(\n    names_from = c(\"variable\"),\n    values_from = c(\"estimate\",\"moe\")\n  ) |> \n  dplyr::rename(Median_Income=estimate_B19013_001, College_Education=estimate_B16010_028,Total_Pop=estimate_B01003_001) |> \n  dplyr::mutate(Perc_College_Ed = (College_Education / Total_Pop) * 100) |> \n  dplyr::filter(!is.na(Median_Income) & !is.na(Perc_College_Ed))\n  \n    \n# Median Income\nmult_wide |> \n  ggplot() + geom_sf(aes(fill = Median_Income)) + \n  scale_y_continuous() +\n  scale_fill_viridis_c(option = \"mako\") +\n  theme_minimal_grid(12)\n\n\n\n\n\nmult_wide |> \n  ggplot() + geom_sf(aes(fill = Perc_College_Ed)) + \n  scale_y_continuous() +\n  scale_fill_viridis_c(option = \"mako\") +\n  theme_minimal_grid(12)\n\n\n\n\nA great resource for mapping with ggplot2 is this blog post from 2018.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the blog post link above, get counties for your state using tigris (or other!) package. Ignore the extra packages and some of the bells and whistles in the blog post, but try using a few of their ideas and adding a few of the ggplot parameters to make an interesting map using county data. You can try loading city data as well.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne idea\n\n# library(tmap)\n# library(tmaptools)\nlibrary(ggplot2)\nlibrary(tigris)\nlibrary(sf, quietly = T)\ncounties <- tigris::counties(\"Oregon\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\ncounties <- counties |> \n  sf::st_transform(2991) |> \n  dplyr::mutate(area=as.numeric(st_area(counties)))\n\nggplot() +\n    geom_sf(data = counties, fill = NA, color = gray(.5)) \n\n\n\n\n\nlibrary(ggspatial)\nggplot() +\n    geom_sf(data = counties, aes(fill = area))  +\n  scale_fill_viridis_c(trans = \"sqrt\", alpha = .4) +\n    \n    theme(panel.grid.major = element_line(color = gray(0.5), linetype = \"dashed\", \n        size = 0.5), panel.background = element_rect(fill = \"aliceblue\")) +\n    annotation_scale(location = \"bl\", width_hint = 0.4) +\n    annotation_north_arrow(location = \"tr\", which_north = \"true\",\n        style = north_arrow_fancy_orienteering)"
  },
  {
    "objectID": "visualization.html#tmap",
    "href": "visualization.html#tmap",
    "title": "2  Visualization",
    "section": "\n2.5 tmap",
    "text": "2.5 tmap\ntamp uses the same syntax as ggplot: the grammar of graphics - and it supports both static and interactive modes. Like ggplot2, each dataset added to a tmap plot can be mapped in a number of different ways such as location, color, size, etc. The [Making maps with R section of Geocomputation with R] has an excellent and in depth treatment of using tmap.\nWe can explore the basics using the counties of Oregon data we’ve been using in previous examples and exersices.\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.2.3\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n# Add fill layer to Oregon counties\ntm_shape(counties) +\n  tm_fill() \n\n\n\n# Add border layer to Oregon counties\ntm_shape(counties) +\n  tm_borders() \n\n\n\n# Add fill and border layers to Oregon counties\ntm_shape(counties) +\n  tm_fill() +\n  tm_borders() \n\n\n\n\n\n2.5.1 Basemaps with tmap\n\n\ndata(metro)\noregon_cnties <- st_transform(counties, 4326) \nmetro |> \n  st_transform(4326) |> \n  st_crop(st_bbox(oregon_cnties))  -> or_metro\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_basemap(\"Stamen.Watercolor\") +\ntm_shape(or_metro) + tm_bubbles(size = \"pop2020\", col = \"red\") +\ntm_tiles(\"Stamen.TonerLabels\")\n\nLegend for symbol sizes not available in view mode.\n\n\n\n\n\n\n\n\n2.5.2 Choropleths with tmap\n\n\nlibrary(tmap)\ntm_shape(mult_wide) + tm_polygons(\"Median_Income\")\n\n\n\n\n\n\n\ntm_shape(mult_wide) + tm_polygons(\"Perc_College_Ed\")\n\n\n\n\n\n\nThat’s a pretty basic map - we can adjust a number of settings such as:\n\nbreaks: we can set different breaks for our map\nbins: we can control the number of bins\npalette: we can change the color palette\nlayout: put the legend outside of the map, increase legend width\n\nThese are just a few - let’s play with those to start with.\n\nbreaks = c(0, 20, 40, 80)\nt1 <- tm_shape(mult_wide) + tm_polygons(\"Perc_College_Ed\", breaks=breaks,palette = \"BuGn\") + tm_layout(legend.outside=TRUE, legend.outside.position = \"right\", legend.outside.size=.5)\nt2 <- tm_shape(mult_wide) + tm_polygons(\"Median_Income\", n=3,palette = \"BuGn\") + tm_layout(legend.outside=TRUE, legend.outside.position = \"right\", legend.outside.size=.5)\ntmap_arrange(t1, t2, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.3 Faceting\n\n# Set mode to interactive\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n# Plot it out\ntm_shape(mult_wide) + tm_polygons(c(\"Median_Income\", \"Perc_College_Ed\")) + tm_facets(sync = TRUE, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 Plotting rasters and vectors with tmap\nBring in boundary and elevation of Crater Lake NP (datasets in Rspatialworkshop package) and plot with tmap\n\nlibrary(Rspatialworkshop)\nlibrary(tmap)\nlibrary(terra)\n# Set mode to plot\ntmap_mode(\"plot\")\ndata(CraterLake)\nraster_filepath <- system.file(\"extdata\", \"elevation.tif\", package = \"Rspatialworkshop\")\nelevation <- rast(raster_filepath)\n\nmap_crlk <- tm_shape(CraterLake) + tm_polygons(lwd = 2)\nmap_crlkel = map_crlk +\n  tm_shape(elevation) + tm_raster(alpha = 0.7,palette = terrain.colors(12)) + tm_layout(legend.position = c(\"left\",\"bottom\"),\n          legend.width = 1)\n\nmap_crlkel\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTake a few minutes to read through Making maps with R in Geocomputation with R or the tmap GitHub pages and try to find a few modifications to plotting the elevation and the Crater Lake park boundary you can come up with to make a more interesting map with tmap.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntm_shape(elevation) +\n  tm_raster(title = \"Elevation\", \n            style = \"cont\",\n            palette = \"-Spectral\") +\n  tm_shape(CraterLake) +\n  tm_borders(col = \"black\", \n             lwd = 1)+ \n  tm_scale_bar(breaks = c(0, 10, 20),\n               text.size = .5,\n               position = c(\"left\", \"bottom\")) +\n  tm_grid(n.x = 4, n.y = 3) +\n  tm_compass(position = c(\"right\", \"top\"), \n             type = \"arrow\", \n             size = 1.5)"
  },
  {
    "objectID": "advanced_applications.html#goals-and-outcomes",
    "href": "advanced_applications.html#goals-and-outcomes",
    "title": "4  Advanced Applications",
    "section": "4.1 Goals and Outcomes",
    "text": "4.1 Goals and Outcomes\n\nThis is goal 1.\nThis is goal 2.\nThis is goal 3.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may click on any of the functions in this book to be directed to their respective documentation. For example, clicking on st_join() takes you to the documentation page for the st_join() function on the sf website."
  },
  {
    "objectID": "advanced_applications.html#advanced-applications",
    "href": "advanced_applications.html#advanced-applications",
    "title": "4  Advanced Applications",
    "section": "4.2 Advanced Applications",
    "text": "4.2 Advanced Applications\n\n4.2.1 Using Web Services"
  },
  {
    "objectID": "resources.html#r-spatial-resources",
    "href": "resources.html#r-spatial-resources",
    "title": "Resources",
    "section": "R Spatial Resources",
    "text": "R Spatial Resources\n\nR Spatial - Spatial Data Science with R\nGeocomputation with R\nR Spatial Task View\nModern Geospatial Data Analysis with R by Zev Ross\nSIGR2021 Summer School\nSpatial Data Science - Pebesma and Bivand\nSpatial Data Science Course- Prof. Adam Wilson\nIntroduction to Mapping and Spatial Analysis with R\nR Spatial Workshop for EPA R User Group\nIntro to GIS and Spatial Analysis by Manuel Gimond\nFOSS4G2019 R for Geospatial Processing\nAn Introduction to Spatial Analysis and Mapping in R\nEarth Analytics Spatial Data in R\nHydroinformatics at VT: Extensive Notes and exercises for a course on data analysis techniques in hydrology using the programming language R"
  },
  {
    "objectID": "resources.html#r-vector-processing-simple-features-resources",
    "href": "resources.html#r-vector-processing-simple-features-resources",
    "title": "Resources",
    "section": "R Vector Processing / Simple Features Resources",
    "text": "R Vector Processing / Simple Features Resources\n\nSimple Features for R\nSpatial Data in R: New Directions\nsp-sf Migration\nAn Exploration of Simple Features for R\nSimple Features: Building Spatial Data Pipelines in R\nTidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf"
  },
  {
    "objectID": "resources.html#r-raster-resources",
    "href": "resources.html#r-raster-resources",
    "title": "Resources",
    "section": "R Raster Resources",
    "text": "R Raster Resources\n\nterra\nSpatial Data Science with R and terra\nstars - spatiotemporal arrays\nWageningen University Intro to Raster\nWageningen University Advanced Raster Analysis\nThe Visual Raster Cheat Sheet GitHub Repo\nRastervis"
  },
  {
    "objectID": "resources.html#r-mapping-resources",
    "href": "resources.html#r-mapping-resources",
    "title": "Resources",
    "section": "R Mapping Resources",
    "text": "R Mapping Resources\n\nmapview\nLeaflet for R\ntmap\nZev Ross Creating beautiful demographic maps in R with the tidycensus and tmap packages\nGeocomputation with R: Making maps with R\nNico Hahn: Making Maps with R R"
  },
  {
    "objectID": "resources.html#web-services-in-r",
    "href": "resources.html#web-services-in-r",
    "title": "Resources",
    "section": "Web Services in R",
    "text": "Web Services in R\n\nAccessing REST API (JSON data) using httr and jsonlite\nWorking with Geospatial Hydrologic Data Using Web Services (R)"
  },
  {
    "objectID": "resources.html#general-r-resources",
    "href": "resources.html#general-r-resources",
    "title": "Resources",
    "section": "General R Resources",
    "text": "General R Resources\n\nGoogle R Style Guide\nAdvanced R by Hadley Wickham"
  },
  {
    "objectID": "foundations.html#why-r-for-spatial-analysis",
    "href": "foundations.html#why-r-for-spatial-analysis",
    "title": "1  Foundations",
    "section": "\n1.3 Why R for Spatial Analysis",
    "text": "1.3 Why R for Spatial Analysis\nAdvantages\n\nR is:\n\nlightweight\nopen-source\ncross-platform\n\n\nWorks with contributed packages - currently 19926\n\nprovides extensibility\n\n\n\nAutomation and recording of workflow\nprovides reproducibility\n\nOptimized work flow - data manipulation, analysis and visualization all in one place\n\nprovides integration\n\n\n\nR does not alter underlying data - manipulation and visualization in memory\n\nR is great for repetitive graphics\nR is great for integrating different aspects of analysis - spatial and statistical analysis in one environment\n\nagain, integration\n\n\n\nLeverage statistical power of R (i.e. modeling spatial data, data visualization, statistical exploration)\nCan handle vector and raster data, as well as work with spatial databases and pretty much any data format spatial data comes in\nR’s GIS capabilities growing rapidly right now - new packages added monthly - currently about 275 spatial packages (depending on how you categorize)\n\nDrawbacks for usig R for GIS work\n\nR is not as good for interactive use as desktop GIS applications like ArcGIS or QGIS (i.e. editing features, panning, zooming, and analysis on selected subsets of features)\nExplicit coordinate system handling by the user\n\nno on-the-fly projection support\n\n\nIn memory analysis does not scale well with large GIS vector and tabular data\nSteep learning curve\nUp to you to find packages to do what you need - help not always great\n\n\n1.3.1 A Couple Motivating Examples\nHere’s a quick example demonstrating R’s flexibility and current strength for plotting and interactive mapping with very simple, expressive code:\n\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(dplyr)\n\nmy_town <-tmaptools::geocode_OSM(\"Corvallis OR USA\", \n        as.sf = TRUE)  |>  \n  glimpse()\n#> Rows: 1\n#> Columns: 9\n#> $ query   <chr> \"Corvallis OR USA\"\n#> $ lat     <dbl> 44.56457\n#> $ lon     <dbl> -123.262\n#> $ lat_min <dbl> 44.51993\n#> $ lat_max <dbl> 44.60724\n#> $ lon_min <dbl> -123.3362\n#> $ lon_max <dbl> -123.231\n#> $ bbox    <POLYGON [°]> POLYGON ((-123.3362 44.5199...\n#> $ point   <POINT [°]> POINT (-123.262 44.56457)\n\n\n\n\n\n\n\nQuick Exercise\n\n\n\nRun and examine code chunk above and try geocoding examples you think of\n\nWhat is the double colon doing?\nWhat is the geocode_OSM function doing?\nExplain how the code runs together using the |> chaining operator\nWhat is glimpse? How is it useful compared to the head function?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nIt specifies using geocode_OSM from the tmaptools package. R gives namespace preference to packages in order loaded; some packages share function names; so it’s good practice to disambiguate your functions with the double-colon\nIt is looking up a named feature in OpenStreetMap and returning the coordinates (bonus - we’ll delve into more in next section - what coordinate reference system are coordinates in and how do you find out?)\nYou would translate code using the |> operator from:\n\n\ndo this |> do that |> do that\n\nTo\n\ndo this then do that then do that\n\n\nTechnically, it’s a transposed version of print - columns run down page, data across like rows - it additionally gives you the number of observations and variables, and the data type of each column. Note that glimpse omits geometry information - you would type the name of your object at the console to get full information, or use print.sf() or str().\n\n\nbonus: how would you quickly learn more about glimpse from the console?\n\n\n\n\n\n1.3.1.1 Choropleth map\nThe tigris package can be used to get census boundaries such as states and counties as vector sf objects in R\n\nlibrary(tigris)\nlibrary(sf, quietly = T)\ncounties <- counties(\"Oregon\", cb = TRUE)\ncounties$area <- as.numeric(st_area(counties))\nglimpse(counties)\n\ntm_shape(counties) +\n  tm_polygons(\"area\", \n              style=\"quantile\", \n              title=\"Area of Oregon Counties\")\n\n\n\n\n\n1.3.1.2 Interactive mapping\n\nlibrary(mapview)\nmapviewOptions(fgb=FALSE)\nmapview(my_town, col=\"red\", col.regions = \"red\") + mapview(counties, alpha.regions = .1)"
  },
  {
    "objectID": "foundations.html#background-on-data-structures",
    "href": "foundations.html#background-on-data-structures",
    "title": "1  Foundations",
    "section": "\n1.2 Background on data structures",
    "text": "1.2 Background on data structures\n\n\n\n\n\n\nNote\n\n\n\nMuch of this background on data structures is borrowed from Mike Johnson’s Introduction to Spatial Data Science and lecture material from our AWRA 2022 Geo Workshop\n\n\nBefore we dive into spatial libraries, it’s worth a quick review of relevant data structures in R - this will be a whirlwind overview, assuming most everyone is familiar with using R already.\n\nYou, computers, and software ‘understand’ values in particular and different ways\n\nComputers convert bytes –> hex –> value\n\nHumans read values\nSoftware reads Hex bytes\nHardware reads Binary bytes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat’s the difference between 10 and ‘10’?\n\n\nTo us: meaning\n\nTo software: how it is handled\n\nTo a computer: nothing\n\nWe need human-defined (computer-guessable) data types\n\n\nFundamental data types\n\n# Numeric\ntypeof(1.9)\n#> [1] \"double\"\n# Integer\ntypeof(1L)\n#> [1] \"integer\"\n# Boolean\ntypeof(TRUE)\n#> [1] \"logical\"\n# Character\ntypeof(\"Welcome\")\n#> [1] \"character\"\n\nStoring more than one value requires a structure.\nValues can be structured in ways such as:\n\nvectors\ndimensions/attributes\ndata.frames\n\nAnd data.frames can be operated on with functions such as:\n\nfilter\nselect\nmutate\nsummarize\ngroup_by\n\n\n1.2.1 Vectors\n\n\n\n\n\n\nNote\n\n\n\n‘vector’ has two meanings when working with GIS data and working in R!\n\ngeographic vector data (which we’ll explore)\n\nvector class (what we’re talking about here)\n\ngeographic vector data is a data model, and the vector class is an R class like data.frame and matrix\n\n\nVectors come in two flavors:\n\natomic\nlist\n\n\natomic vectors elements must have the same type\nlists elements can have different types\n\n1.2.2 Atomic vectors\n\n# Numeric\ndbl_vec = c(1.9, 2, 3.5)\ntypeof(dbl_vec)\n#> [1] \"double\"\nlength(dbl_vec)\n#> [1] 3\n# Logical\nlg_vec = c(TRUE, FALSE, F, T)\ntypeof(lg_vec)\n#> [1] \"logical\"\nlength(lg_vec)\n#> [1] 4\n\nCoercion\n\n\ntype is a property of a vector\nWhen you try to combine different types they’ll be coerced in the following fixed order:\n\ncharacter => double => integer => logical\n\n\nCoercion occurs automatically but generates a warning and a missing value when it fails\n\n\nc(\"a\", 1)\n#> [1] \"a\" \"1\"\nc(\"a\", TRUE)\n#> [1] \"a\"    \"TRUE\"\nc(4.5, 1L)\n#> [1] 4.5 1.0\nc(\"1\", 18, \"GIS\")\n#> [1] \"1\"   \"18\"  \"GIS\"\nas.numeric(c(\"1\", 18, \"GIS\"))\n#> [1]  1 18 NA\nas.logical(c(\"1\", 18, \"GIS\"))\n#> [1] NA NA NA\n\nSubsetting atomic vectors\n\n# Atomic numeric vector\n(x = c(3.4, 7, 18, 9.6))\n#> [1]  3.4  7.0 18.0  9.6\n\n# Third Value\nx[3]\n#> [1] 18\n\n# Third and Fourth value\nx[c(3,4)]\n#> [1] 18.0  9.6\n\n# Drop the third value\nx[-3]\n#> [1] 3.4 7.0 9.6\n\n# Keep the 1 and 2 value, but drop 3 and 4\nx[c(T,T,F,F)]\n#> [1] 3.4 7.0\n\n\n1.2.3 Matrix\n\nA matrix is 2D atomic (row, column)\n\nSame data types\nSame column length\n\n\n\nThis is how spatial raster data is structured\nSubsetting matrices uses row,column (i,j) syntax\n\n(x = matrix(1:9, nrow = 3))\n#>      [,1] [,2] [,3]\n#> [1,]    1    4    7\n#> [2,]    2    5    8\n#> [3,]    3    6    9\nx[3,]\n#> [1] 3 6 9\nx[,3]\n#> [1] 7 8 9\nx[3,3]\n#> [1] 9\n\n\n1.2.4 Array\n\nArray is a 3d Atomic (row, column, slice)\n\nThis is how spatial raster data with a time dimension is structured\n\n(array(c(1:12), dim = c(3,2,2)))\n#> , , 1\n#> \n#>      [,1] [,2]\n#> [1,]    1    4\n#> [2,]    2    5\n#> [3,]    3    6\n#> \n#> , , 2\n#> \n#>      [,1] [,2]\n#> [1,]    7   10\n#> [2,]    8   11\n#> [3,]    9   12\n\nSubsetting arrays uses row, column, slice syntax (i,j,z)\n\n(x = array(1:12, dim = c(2,2,3)))\n#> , , 1\n#> \n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\n#> \n#> , , 2\n#> \n#>      [,1] [,2]\n#> [1,]    5    7\n#> [2,]    6    8\n#> \n#> , , 3\n#> \n#>      [,1] [,2]\n#> [1,]    9   11\n#> [2,]   10   12\nx[1,,]\n#>      [,1] [,2] [,3]\n#> [1,]    1    5    9\n#> [2,]    3    7   11\nx[,1,]\n#>      [,1] [,2] [,3]\n#> [1,]    1    5    9\n#> [2,]    2    6   10\nx[,,1]\n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\n\n\n1.2.5 Lists\nEash list element can be any data type\n\n(my_list <- list(\n  matrix(1:4, nrow = 2), \n  \"GIS is great!\", \n  c(TRUE, FALSE, TRUE), \n  c(2.3, 5.9)\n))\n#> [[1]]\n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\n#> \n#> [[2]]\n#> [1] \"GIS is great!\"\n#> \n#> [[3]]\n#> [1]  TRUE FALSE  TRUE\n#> \n#> [[4]]\n#> [1] 2.3 5.9\n\n\ntypeof(my_list)\n#> [1] \"list\"\n\nSubsetting Lists\n\nEach element of a list can be accessed with the [[ operator\n\n\nmy_list[[1]]\n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\nmy_list[[1]][1,2]\n#> [1] 3\n\n\n1.2.6 Data Frames\n\n\ndata.frame is built on the list structure in R\nlength of each atomic or list vector has to be the same\nThis gives data.frame objects rectangular structure so they share properties of both matrices and lists\n\n\n(df1 <- data.frame(name = c(\"Me\", \"Tim\", \"Sarah\"),\n                  age  = c(53,15,80),\n                  retired = c(F,F,T)))\n#>    name age retired\n#> 1    Me  53   FALSE\n#> 2   Tim  15   FALSE\n#> 3 Sarah  80    TRUE\ntypeof(df1)\n#> [1] \"list\"\n\nSubsetting a data.frame\n\ndf1[1,2]\n#> [1] 53\n\n# or like a list\ndf1[[2]]\n#> [1] 53 15 80\n\n# or with column name operator\ndf1$name\n#> [1] \"Me\"    \"Tim\"   \"Sarah\"\n\n\n1.2.7 Data manipulation\n\n\ndata.frame manipulation is all based on SQL queries\nR abstracts the SQL logic and provides function-ized methods\n\ndplyr in the tidyverse ecosystem provides the ’grammar of data manipulation` approach we’ll use in this workshop\n\nData manipulation verbs:\n\nPrimary:\n\n\nselect(): keeps or removes variables based on names\n\nfilter(): keeps or removes observations based on values _ Manipulation:\n\nmutate(): adds new variables that are functions of existing variables\n\nsummarise(): reduces multiple values down to a single summary\n\narrange(): changes ordering of the rows\n\n\nGrouping:\n\n\ngroup_by(): combine with any or all of the above to perform manipulation ‘by group’\n\n\n\n1.2.8 Pipe operator\n\nThe pipe operator (native R pipe operator |> or magrittr pipe operator %>%) provides a more concise and expressive coding experience\nThe pipe passes the object on the left hand side of the pipe into the first argument of the right hand function\nTo be |> compatible, the data.frame is ALWAYS the fist argument to dplyr verbs\n\nA demonstration using dataRetrieval package stream gage data from USGS:\n\nlibrary(dataRetrieval)\nflows <- dataRetrieval::readNWISdv(\n  siteNumbers = '14187200',\n  parameterCd = \"00060\"\n)  |>  \n  dataRetrieval::renameNWISColumns()\n  \ndplyr::glimpse(flows)\n#> Rows: 18,337\n#> Columns: 5\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-…\n#> $ Flow      <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 92…\n#> $ Flow_cd   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the :: after the package name and before the function in the R code above doing? How would you find out more about it?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n:: specifies the namespace (i.e., package) associated with a particular function.\nYou can get help on it the same way you get help on anything in R:\n\n?'::'\n# or\nhelp(\"::\")\n\n\n\n\n\n1.2.9 Filter\n\n\nfilter() takes logical (binary) expressions and returns the rows in which all conditions are TRUE.\n\nFilter on a single condition:\n\nflows |> \n  dplyr::filter(Flow > 900) |>\n  dplyr::glimpse()\n#> Rows: 14,446\n#> Columns: 5\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 1973-08-04, 1973-08-05, 1973-08-06, 1973-08-07, 1973-08-…\n#> $ Flow      <dbl> 930, 939, 939, 944, 932, 927, 925, 927, 928, 945, 938, 94…\n#> $ Flow_cd   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n\nOr multiple conditions:\n\nflows  |> \n  dplyr::filter(Flow > 900, Date > as.Date(\"2010-01-01\"))  |>\n  dplyr::glimpse()\n#> Rows: 4,396\n#> Columns: 5\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 2010-01-02, 2010-01-03, 2010-01-04, 2010-01-05, 2010-01-…\n#> $ Flow      <dbl> 7870, 6920, 5860, 7860, 10000, 10100, 9760, 9130, 8600, 7…\n#> $ Flow_cd   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n\n\n1.2.10 Select\n\nSubset variables (columns) you want to keep or exclude by name\n\nJust keep three columns\n\nflows |> \n  dplyr::select(Date, Flow)  |> \n  dplyr::glimpse()\n#> Rows: 18,337\n#> Columns: 2\n#> $ Date <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-05, 1…\n#> $ Flow <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 928, 94…\n\nExclude just one\n\nflows |> \n  dplyr::select(-Flow_cd)  |> \n  dplyr::glimpse()\n#> Rows: 18,337\n#> Columns: 4\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-…\n#> $ Flow      <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 92…\n\n..And can rename while selecting\n\nflows |> \n  dplyr::select(Date, flow_cfs = Flow)  |> \n  dplyr::glimpse()\n#> Rows: 18,337\n#> Columns: 2\n#> $ Date     <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-0…\n#> $ flow_cfs <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 928…\n\n\n1.2.11 Mutate\n\n\nmutate() defines and inserts new variables into a existing data.frame\n\n\nmutate() builds new variables sequentially so you can reference earlier ones when defining later ones\n\nWe can extract Year and Month as new variables from the Date variable using date time\n\nflows  |> \n  dplyr::select(Date, Flow)  |>  \n  dplyr::filter(Date > as.Date('2010-01-01'))  |> \n  dplyr::mutate(Year  = format(Date, \"%Y\"), Month = format(Date, \"%m\"))  |>  \n  dplyr::glimpse()\n#> Rows: 5,034\n#> Columns: 4\n#> $ Date  <date> 2010-01-02, 2010-01-03, 2010-01-04, 2010-01-05, 2010-01-06, …\n#> $ Flow  <dbl> 7870, 6920, 5860, 7860, 10000, 10100, 9760, 9130, 8600, 7040,…\n#> $ Year  <chr> \"2010\", \"2010\", \"2010\", \"2010\", \"2010\", \"2010\", \"2010\", \"2010…\n#> $ Month <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"…\n\n\n1.2.12 Summarize and Group_By\n\n\nsummarize() allows you to summarize across all observations\n\ngroup_by() allows you to apply any of these manipulation verbs by group in your data\n\n\nflows |> \n dplyr::select(Date, Flow) |> \n dplyr::mutate(Year  = format(Date, \"%Y\"))  |> \n dplyr::group_by(Year) |> \n dplyr::summarize(meanQ = mean(Flow),\n           maxQ = max(Flow))\n#> # A tibble: 51 × 3\n#>    Year  meanQ  maxQ\n#>    <chr> <dbl> <dbl>\n#>  1 1973  4669. 13200\n#>  2 1974  3659. 14400\n#>  3 1975  3611. 14100\n#>  4 1976  2340. 15000\n#>  5 1977  2860. 16200\n#>  6 1978  2206. 11300\n#>  7 1979  2378. 12000\n#>  8 1980  2548. 14700\n#>  9 1981  2976. 17000\n#> 10 1982  3424. 15100\n#> # ℹ 41 more rows\n\n\n\n\n\n\n\nNote\n\n\n\nYou may click on any of the functions in this book to be directed to their respective documentation. For example, clicking on st_join() takes you to the documentation page for the st_join() function on the sf website."
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "3  Visualization",
    "section": "",
    "text": "Understand how to use several of the most popular libraries for plotting and visualizing spatial data inR\nThis is goal 2.\nThis is goal 3."
  },
  {
    "objectID": "foundations.html#coordinate-reference-systems",
    "href": "foundations.html#coordinate-reference-systems",
    "title": "1  Foundations",
    "section": "\n1.5 Coordinate Reference Systems",
    "text": "1.5 Coordinate Reference Systems\nA CRS is made up of several components:\n\n\nCoordinate system: The x,y grid that defines where your data lies in space\n\nHorizontal and vertical units: The units describing grid along the x,y and possibly z axes\n\nDatum: The modeled version of the shape of the earth\n\nProjection details: If projected, the mathematical equation used to flatten objects from round surface (earth) to flat surface (paper or screen)\n\n\n\n\n\nSource: https://mgimond.github.io/Spatial/chp09_0.html\n\n\n\n\n\n1.5.1 The ellipsoid and geoid\nThe earth is a sphere, but more precisely, an ellipsoid, which is defined by two radii: - semi-major axis (equatorial radius) - semi-minor axis (polar radius)\nThe terms speroid and ellipsoid are used interchangeably. One particular spheroid is distinguished from another by the lengths of the semimajor and semiminor axes. Both GRS80 and WGS84 spheroids use semimajor and semiminor axes of 6,378,137 meters and 6,356,752 meters respectively (the semiminor axis differs by thousandths of meters between the two). You’ll encounter older spheroid / ellipsoids out there such as Clark 1866.\nMore precisely than an ellipsoid, though, we know that earth is a geoid - it is not perfectly smooth - and modelling the the undulations due to changes in gravitational pull for different locations is crucial to accuracy in a GIS. This is where a datum comes in - a datum is built on top of the selected spheroid and can incorporate local variations in elevation. We have many datums to choose from based on location of interest - in the US we would typically choose NAD83\n\n1.5.2 Why you need to know about CRS working with spatial data in R:\n\nlibrary(Rspatialworkshop)\nlibrary(readr)\nlibrary(sf)\ndata(pnw)\n\ngages = read_csv(system.file(\"extdata/Gages_flowdata.csv\", package = \"Rspatialworkshop\"),show_col_types = FALSE)\n\ngages_sf <- gages  |> \n  st_as_sf(coords = c(\"LON_SITE\", \"LAT_SITE\"), crs = 4269, remove = FALSE)  |> \n  dplyr::select(STATION_NM,LON_SITE, LAT_SITE)\n\n# Awesome, let's plot our gage data and state boundaries!\nplot(pnw$geometry, axes=TRUE)\nplot(gages_sf$geometry, col='red', add=TRUE)\n\n\n\n# um, what?\n\nThere is no ‘on-the-fly’ projection in R - you need to make sure you specify the CRS of your objects, and CRS needs to match for any spatial operations or you’ll get an error\n\nspatialreference.org is your friend in R - chances are you will use it frequently working with spatial data in R.\nprojection Wizard is also really useful, as is epsg.io\n\n1.5.3 Defining your coordinate reference system\nA coordinate reference system for spatial data can be defined in different ways - for instance for the standard WGS84 coordinate system you could use:\n\nProj4\n\n+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\nOGC WKT\n\nGEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]]\n\n\nESRI WKT\n\nGEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]]\n\n\n‘authority:code’ identifier\n\nEPSG:4326\n\n\n\nProj4 used to be the standard crs identifier in R but the preferable way is to use the AUTHORITY:CODE method which sf as well as software like QGIS will recognize. The AUTHORITY:CODE method is durable and easily discoverable online.\nThe WKT representation of EPSG:4326 in sf is:\n\nsf::st_crs('EPSG:4326')\n#> Coordinate Reference System:\n#>   User input: EPSG:4326 \n#>   wkt:\n#> GEOGCRS[\"WGS 84\",\n#>     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#>         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]],\n#>         ENSEMBLEACCURACY[2.0]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Horizontal component of 3D system.\"],\n#>         AREA[\"World.\"],\n#>         BBOX[-90,-180,90,180]],\n#>     ID[\"EPSG\",4326]]\n\n\n1.5.4 Projected coordinate systems\nTypically we want to work with data that is projected. Projected coordinate systems (which are based on Cartesian coordinates) have: an origin, an x axis, a y axis, and a linear unit of measure. Going from geographic coordinates to a projected coordinate reference systems requires mathematical transformations.\nFour spatial properties of projected coordinate systems that are subject to distortion are: shape, area, distance and direction. A map that preserves shape is called conformal; one that preserves area is called equal-area; one that preserves distance is called equidistant; and one that preserves direction is called azimuthal (from https://mgimond.github.io/Spatial/chp09_0.html.\nThe takeaway from all this is you need to be aware of the crs for your objects in R, make sure they are projected if appropriate and in a projection that optimizes properties you are interested in, and objects you are analyzing or mapping together need to be in same crs.\nGoing back to our original example, we can transform crs of objects to work with them together:\n\nlibrary(ggplot2)\nlibrary(sf)\n# Check our coordinate reference systems\nst_crs(gages_sf)\n#> Coordinate Reference System:\n#>   User input: EPSG:4269 \n#>   wkt:\n#> GEOGCRS[\"NAD83\",\n#>     DATUM[\"North American Datum 1983\",\n#>         ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Geodesy.\"],\n#>         AREA[\"North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands. British Virgin Islands.\"],\n#>         BBOX[14.92,167.65,86.46,-47.74]],\n#>     ID[\"EPSG\",4269]]\nst_crs(pnw)\n#> Coordinate Reference System:\n#>   User input: +proj=aea +lat_1=41 +lat_2=47 +lat_0=44 +lon_0=-120 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs \n#>   wkt:\n#> PROJCRS[\"unknown\",\n#>     BASEGEOGCRS[\"unknown\",\n#>         DATUM[\"North American Datum 1983\",\n#>             ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#>                 LENGTHUNIT[\"metre\",1]],\n#>             ID[\"EPSG\",6269]],\n#>         PRIMEM[\"Greenwich\",0,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8901]]],\n#>     CONVERSION[\"unknown\",\n#>         METHOD[\"Albers Equal Area\",\n#>             ID[\"EPSG\",9822]],\n#>         PARAMETER[\"Latitude of false origin\",44,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8821]],\n#>         PARAMETER[\"Longitude of false origin\",-120,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8822]],\n#>         PARAMETER[\"Latitude of 1st standard parallel\",41,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8823]],\n#>         PARAMETER[\"Latitude of 2nd standard parallel\",47,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8824]],\n#>         PARAMETER[\"Easting at false origin\",0,\n#>             LENGTHUNIT[\"metre\",1],\n#>             ID[\"EPSG\",8826]],\n#>         PARAMETER[\"Northing at false origin\",0,\n#>             LENGTHUNIT[\"metre\",1],\n#>             ID[\"EPSG\",8827]]],\n#>     CS[Cartesian,2],\n#>         AXIS[\"(E)\",east,\n#>             ORDER[1],\n#>             LENGTHUNIT[\"metre\",1,\n#>                 ID[\"EPSG\",9001]]],\n#>         AXIS[\"(N)\",north,\n#>             ORDER[2],\n#>             LENGTHUNIT[\"metre\",1,\n#>                 ID[\"EPSG\",9001]]]]\n# Are they equal?\nst_crs(gages_sf)==st_crs(pnw)\n#> [1] FALSE\n# transform one to the other\ngages_sf <- st_transform(gages_sf, st_crs(pnw))\nggplot() + \n  geom_sf(data = gages_sf,  color=\"blue\") +\n  geom_sf(data = pnw,  color=\"black\", fill=NA) +\n  labs(title=\"USGS Stream Gages in the Pacific Northwest\") +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRe-project both the PNW state polygons and the stream gages to a different projected CRS - I suggest UTM zone 11, or perhaps Oregon Lambert, but choose your own. Use resources I list above to find a projection and get it’s specification to use in R to re-project both sf objects, then plot together either in base R or ggplot.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMy solution:\n\nlibrary(cowplot)\n# first I save the previous plot so we can view it alongside our update:\np1 <-ggplot() + \n  geom_sf(data=gages_sf,  color=\"blue\") +\n  geom_sf(data=pnw,  color=\"black\", fill=NA) +\n  labs(title=\"Albers Equal Area\") +\n  theme_bw()\n# You can fully specify the WKT:\nutmz11 <- 'PROJCS[\"NAD83(CSRS98) / UTM zone 11N (deprecated)\",GEOGCS[\"NAD83(CSRS98)\",DATUM[\"NAD83_Canadian_Spatial_Reference_System\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[\"EPSG\",\"6140\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9108\"]],AUTHORITY[\"EPSG\",\"4140\"]],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],AUTHORITY[\"EPSG\",\"2153\"],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'\n\n# or you can simply provide the EPSG code:\nutmz11 <- 2153\ngages_sf <- st_transform(gages_sf, utmz11)\npnw <- st_transform(pnw, utmz11)\np2 <- ggplot() + \n  geom_sf(data=gages_sf,  color=\"blue\") +\n  geom_sf(data=pnw,  color=\"black\", fill=NA) +\n  labs(title=\"UTM Zone 11\") +\n  theme_bw() \nplot_grid(p1, p2)\n\n\n\n\n\n\n\n\n1.5.5 Projecting\nFor spatial operations in R, raster or vector, we always need to make sure objects are in an appropriate crs and in the same crs as one another.\nWe’ll use the elevation data and a polygon feature for Crater Lake from the Rspatialworkshop package for this example.\n\nlibrary(Rspatialworkshop)\ndata(CraterLake)\nraster_filepath <- system.file(\"extdata\", \"elevation.tif\", package = \"Rspatialworkshop\")\nelevation <- terra::rast(raster_filepath)\n\n:::{.callout-note} #### Exercise 1. Find the projection of both the Crater Lake spatial polygon feature and the elevation raster 2. Are they the same? Should we project one to the other, or apply a new projection?\n:::{.callout-note collapse=“true”} #### Solution 1. Check the CRS of each feature and test for equality\n\ncrs(elevation)\n#> [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\nst_crs(CraterLake)\n#> Coordinate Reference System:\n#>   User input: WGS 84 \n#>   wkt:\n#> GEOGCRS[\"WGS 84\",\n#>     DATUM[\"World Geodetic System 1984\",\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     ID[\"EPSG\",4326]]\nst_crs(CraterLake) == crs(elevation)\n#> [1] FALSE\nst_crs(CraterLake)$wkt == crs(elevation)\n#> [1] FALSE\n\n\nThey share the same EPSG code, but are parameterized slightly differently for each - the crs function in terra does not include the input list item that st_crs in sf does - they are the same though as demonstrated when specifying the wkt item only from st_crs(CraterLake)\n\nEach feature is in WGS84, an unprojected CRS - for most operations, we would prefer to have them in a projected CRS\n\n\n\n\n\n\n\nExercise\n\n\n\nFind an appropriate area-preserving projection using Projection Wizard or spatialreference.org or any means you prefer and project both Crater Lake and elevation to this CRS.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere I’ll apply a custom Transverse Mercator I found the WKT representation for using Projection Wizard. Another good projection would be UTM zone 10N or UTM zone 11N\n\ntranvmerc <- 'PROJCS[\"ProjWiz_Custom_Transverse_Mercator\", GEOGCS[\"GCS_WGS_1984\", DATUM[\"D_WGS_1984\", SPHEROID[\"WGS_1984\",6378137.0,298.257223563]], PRIMEM[\"Greenwich\",0.0], UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"False_Easting\",500000],PARAMETER[\"False_Northing\",0.0],PARAMETER[\"Central_Meridian\",-483.2226562],PARAMETER[\"Scale_Factor\",0.9996],PARAMETER[\"Latitude_Of_Origin\",0.0],UNIT[\"Meter\",1.0]]'\nelevation_tm <- project(elevation, tranvmerc, method = \"bilinear\")\ncrs(elevation_tm)\n#> [1] \"PROJCRS[\\\"ProjWiz_Custom_Transverse_Mercator\\\",\\n    BASEGEOGCRS[\\\"WGS 84\\\",\\n        DATUM[\\\"World Geodetic System 1984\\\",\\n            ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                LENGTHUNIT[\\\"metre\\\",1]],\\n            ID[\\\"EPSG\\\",6326]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"Degree\\\",0.0174532925199433]]],\\n    CONVERSION[\\\"unnamed\\\",\\n        METHOD[\\\"Transverse Mercator\\\",\\n            ID[\\\"EPSG\\\",9807]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",0,\\n            ANGLEUNIT[\\\"Degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",-483.2226562,\\n            ANGLEUNIT[\\\"Degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",0.9996,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",500000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"(E)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"(N)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\nCraterLake_tm <- st_transform(CraterLake,tranvmerc)\n\n\n\n\n\n1.5.6 S2 class\nIt’s important to note a recent change in R spatial with spherical geometry. Since sf version 1.0.0, R supports spherical geometry operations by default via its interface to Google’s S2 spherical geometry engine and the s2 interface package. S2 is perhaps best known as an example of a Discrete Global Grid System (DGGS). Another example of this is the H3 global hexagonal hierarchical spatial index.\nsf can run with s2 on or off and by default the S2 geometry engine is turned on:\n\nsf::sf_use_s2()\n#> [1] TRUE\n\nSee the Geocomputation with R s2 section for further details\n\n\n\n\n\n\nNote\n\n\n\nThere are sometimes good reasons for turning S2 off during an R session. See issue 1771 in sf’s GitHub repo - the default behavior can make code that would work with S2 turned off (and with older versions of sf) fail.\nSituations that can cause problems include operations on polygons that are not valid according to S2’s stricter definition. If you see error message such as #> Error in s2_geography_from_wkb ... you may want to turn s2 off and try your operation again, perhaps turning of s2 and running a topology clean operation like st_make_valid()."
  },
  {
    "objectID": "foundations.html#geographic-data-io",
    "href": "foundations.html#geographic-data-io",
    "title": "1  Foundations",
    "section": "\n1.6 Geographic Data I/O",
    "text": "1.6 Geographic Data I/O\n\nThere are several ways we typically get spatial data into R:\n\nLoad spatial files we have on our machine or from remote source\nLoad spatial data that is part of an R package\nGrab data using API (often making use of particular R packages)\nConverting flat files with x,y data to spatial data\nGeocoding data (we saw example of this at beginning)\n\n\n\nFor reading and writing vector and raster data in R, the several main packages we’ll look at are:\n\n\nsffor vector formats such as ESRI Shapefiles, GeoJSON, and GPX - sf uses OGR, which is a library under the GDAL source tree,under the hood\n\nterra or stars for raster formats such as GeoTIFF or ESRI or ASCII grid using GDAL under the hood\n\nWe can quickly discover supported I/O vector formats either via sf or rgdal:\n\nlibrary(knitr)\nlibrary(sf)\nprint(paste0('There are ',st_drivers(\"vector\")  |>  nrow(), ' vector drivers available using st_read or read_sf'))\n#> [1] \"There are 67 vector drivers available using st_read or read_sf\"\nkable(head(st_drivers(what='vector'),n=5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nlong_name\nwrite\ncopy\nis_raster\nis_vector\nvsi\n\n\n\nESRIC\nESRIC\nEsri Compact Cache\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\n\n\nnetCDF\nnetCDF\nNetwork Common Data Format\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\nPDS4\nPDS4\nNASA Planetary Data System 4\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nVICAR\nVICAR\nMIPL VICAR file\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nJP2OpenJPEG\nJP2OpenJPEG\nJPEG-2000 driver based on OpenJPEG library\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\n\n\n\nAs well as I/O raster formats via sf:\n\nprint(paste0('There are ',st_drivers(what='raster')  |>  nrow(), ' raster drivers available'))\n#> [1] \"There are 141 raster drivers available\"\nkable(head(st_drivers(what='raster'),n=5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nlong_name\nwrite\ncopy\nis_raster\nis_vector\nvsi\n\n\n\nVRT\nVRT\nVirtual Raster\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nDERIVED\nDERIVED\nDerived datasets using VRT pixel functions\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\n\n\nGTiff\nGTiff\nGeoTIFF\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nCOG\nCOG\nCloud optimized GeoTIFF generator\nFALSE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nNITF\nNITF\nNational Imagery Transmission Format\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\n\n\n\n\n1.6.1 Reading in vector data\nsf can be used to read numerous file types:\n\nShapefiles\nGeodatabases\nGeopackages\nGeojson\nSpatial database files\n\n\n1.6.1.1 Shapefiles\nTypically working with vector GIS data we work with ESRI shapefiles or geodatabases - here we have an example of how one would read in a shapefile using sf:\n\ncitylims <- read_sf(system.file(\"extdata/city_limits.shp\", package = \"Rspatialworkshop\"))\n\noptions(scipen=3)\nplot(citylims$geometry, axes=T, main='Oregon City Limits') # plot it!\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nst_read versus read_sf - throughout this section I’ve used read_sf and st_read - I typically use read_sf - why would that be a good practice?\n\n\n:::{.callout-note collapse=“true”} #### Solution read_sf is an sf alternative to st_read (see this section 1.2.2). Try reading in citylims data above using read_sf and notice difference, and check out help(read_sf). read_sf and write_sf are simply aliases for st_read and st_write with modified default arguments. Big differences are:\n\nstringsAsFactors=FALSE\nquiet=TRUE\nas_tibble=TRUE\n\nNote that stringsAsFactors = FALSE is the new default in R versions >= 4.0\n\n1.6.1.2 Geodatabases\nWe use st_read or read_sf similarly for reading in an ESRI file geodatabase feature:\n\nlibrary(ggplot2)\n# List all feature classes in a file geodatabase\nst_layers(system.file(\"extdata/StateParkBoundaries.gdb\", package = \"Rspatialworkshop\"))\n#> Driver: OpenFileGDB \n#> Available layers:\n#>            layer_name geometry_type features fields                 crs_name\n#> 1 StateParkBoundaries Multi Polygon      431     15 WGS 84 / Pseudo-Mercator\nfgdb = system.file(\"extdata/StateParkBoundaries.gdb\", package = \"Rspatialworkshop\")\n# Read the feature class\nparks <- st_read(dsn=fgdb,layer=\"StateParkBoundaries\")\n#> Reading layer `StateParkBoundaries' from data source \n#>   `C:\\Program Files\\R\\R-4.2.2\\library\\Rspatialworkshop\\extdata\\StateParkBoundaries.gdb' \n#>   using driver `OpenFileGDB'\n#> Simple feature collection with 431 features and 15 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -13866300 ymin: 5159950 xmax: -13019620 ymax: 5818601\n#> Projected CRS: WGS 84 / Pseudo-Mercator\nggplot(parks) + geom_sf()\n\n\n\n\n\n1.6.2 Geopackages\nAnother spatial file format is the geopackage. Let’s try a quick read and write of geopackage data. First we’ll read in a geopackage using data that comes with sf using dplyr syntax just to show something a bit different and use read_sf as an alternative to st_read. You may want to try writing the data back out as a geopackage as well.\n\nlibrary(dplyr)\nnc <- system.file(\"gpkg/nc.gpkg\", package=\"sf\")  |>  read_sf() # reads in\nglimpse(nc)\n#> Rows: 100\n#> Columns: 15\n#> $ AREA      <dbl> 0.114, 0.061, 0.143, 0.070, 0.153, 0.097, 0.062, 0.091, 0…\n#> $ PERIMETER <dbl> 1.442, 1.231, 1.630, 2.968, 2.206, 1.670, 1.547, 1.284, 1…\n#> $ CNTY_     <dbl> 1825, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 183…\n#> $ CNTY_ID   <dbl> 1825, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 183…\n#> $ NAME      <chr> \"Ashe\", \"Alleghany\", \"Surry\", \"Currituck\", \"Northampton\",…\n#> $ FIPS      <chr> \"37009\", \"37005\", \"37171\", \"37053\", \"37131\", \"37091\", \"37…\n#> $ FIPSNO    <dbl> 37009, 37005, 37171, 37053, 37131, 37091, 37029, 37073, 3…\n#> $ CRESS_ID  <int> 5, 3, 86, 27, 66, 46, 15, 37, 93, 85, 17, 79, 39, 73, 91,…\n#> $ BIR74     <dbl> 1091, 487, 3188, 508, 1421, 1452, 286, 420, 968, 1612, 10…\n#> $ SID74     <dbl> 1, 0, 5, 1, 9, 7, 0, 0, 4, 1, 2, 16, 4, 4, 4, 18, 3, 4, 1…\n#> $ NWBIR74   <dbl> 10, 10, 208, 123, 1066, 954, 115, 254, 748, 160, 550, 124…\n#> $ BIR79     <dbl> 1364, 542, 3616, 830, 1606, 1838, 350, 594, 1190, 2038, 1…\n#> $ SID79     <dbl> 0, 3, 6, 2, 3, 5, 2, 2, 2, 5, 2, 5, 4, 4, 6, 17, 4, 7, 1,…\n#> $ NWBIR79   <dbl> 19, 12, 260, 145, 1197, 1237, 139, 371, 844, 176, 597, 13…\n#> $ geom      <MULTIPOLYGON [°]> MULTIPOLYGON (((-81.47276 3..., MULTIPOLYGON…\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat are a couple advantages of geopackages over shapefiles?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSome thoughts here, main ones probably:\n\ngeopackages avoid mult-file format of shapefiles\ngeopackages avoid the 2gb limit of shapefiles\ngeopackages are open-source and follow OGC standards\nlighter in file size than shapefiles\ngeopackages avoid the 10-character limit to column headers in shapefile attribute tables (stored in archaic .dbf files)\n\n\n\n\n\n1.6.2.1 Open spatial data sources\nThere is a wealth of open spatial data accessible online now via static URLs or APIs - a few examples include Data.gov, NASA SECAC Portal, Natural Earth, UNEP GEOdata, and countless others listed here at Free GIS Data\n\n1.6.2.2 Spatial data from R packages\nThere are also a number of R packages written specifically to provide access to geospatial data - below are a few and we’ll step through some examples of pulling in data using some of these packages.\n\n\n\nExample R packages for spatial data retrieval.\n\n\n\n\n\nPackage name\nDescription\n\n\n\nUSABoundaries\nProvide historic and contemporary boundaries of the US\n\n\ntigris\nDownload and use US Census TIGER/Line Shapefiles in R\n\n\ntidycensus\nUses Census American Community API to return tidyverse and optionally sf ready data frames\n\n\nFedData\nFunctions for downloading geospatial data from several federal sources\n\n\nelevatr\nAccess elevation data from various APIs (by Jeff Hollister)\n\n\ngetlandsat\nProvides access to Landsat 8 data.\n\n\nosmdata\nDownload and import of OpenStreetMap data.\n\n\nraster\nThe getData() function downloads and imports administrative country, SRTM/ASTER elevation, WorldClim data.\n\n\nrnaturalearth\nFunctions to download Natural Earth vector and raster data, including world country borders.\n\n\nrnoaa\nAn R interface to National Oceanic and Atmospheric Administration (NOAA) climate data.\n\n\nrWBclimate\nAn access to the World Bank climate data.\n\n\n\n\n\nBelow is an example of pulling in US states using the rnaturalearth package - note that the default is to pull in data as sp objects and we coerce to sf. Also take a look at the chained operation using dplyr. Try changing the filter or a parameter in ggplot.\n\nlibrary(rnaturalearth)\nlibrary(dplyr)\nstates <- ne_states(country = 'United States of America')\nstates_sf <- st_as_sf(states)\nstates_sf  |>  \n  dplyr::filter(!name %in% c('Hawaii','Alaska') & !is.na(name)) |> \n  ggplot() + geom_sf(fill=NA)\n\n\n\n\n\n1.6.2.3 Read in OpenStreetMap data\nThe osmdata package is a fantastic resource for leveraging the OpenStreetMap (OSM) database.\nFirst we’ll find available tags to get foot paths to plot\n\nlibrary(osmdata)\nlibrary(mapview)\nmapviewOptions(fgb=FALSE)\nhead(available_tags(\"highway\")) # get rid of head when you run - just used to truncate output\n#> # A tibble: 6 × 2\n#>   Key     Value       \n#>   <chr>   <chr>       \n#> 1 highway bridleway   \n#> 2 highway bus_guideway\n#> 3 highway bus_stop    \n#> 4 highway busway      \n#> 5 highway construction\n#> 6 highway corridor\n\n\nfootway <- opq(bbox = \"corvallis oregon\") %>% \n  add_osm_feature(key = \"highway\", value = c(\"footway\",\"cycleway\",\"path\", \"path\",\"pedestrian\",\"track\")) %>% \n  osmdata_sf()\nfootway <- footway$osm_lines\n\nrstrnts <- opq(bbox = \"corvallis oregon\") %>% \n    add_osm_feature(key = \"amenity\", value = \"restaurant\") %>%\n    osmdata_sf()\nrstrnts <- rstrnts$osm_points\n\nmapview(footway$geometry) + mapview(rstrnts)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTake a minute and try pulling in data of your own for your own area and plotting using osmdata\n\n\n\n1.6.3 Reading in Raster data\n\n1.6.4 terra package\nLoad stock elevation .tif file that comes with package\n\nlibrary(terra)\nf <- system.file(\"ex/elev.tif\", package=\"terra\")\nelev <- rast(f)\nbarplot(elev, digits=-1, las=2, ylab=\"Frequency\")\n\n\n\n\n\nplot(elev)\n\n\n\n\n\n1.6.5 stars package\nLoad stock Landsat 7 .tif file that comes with package\n\nlibrary(stars)\ntif = system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nread_stars(tif) |>\n  dplyr::slice(index = 1, along = \"band\") |>\n  plot()\n\n\n\n\n\n1.6.6 Convert flat files to spatial\nWe often have flat files, locally on our machine or accessed elsewhere, that have coordinate information which we would like to make spatial.\nIn the steps below, we\n\nread in a .csv file of USGS gages in the PNW that have coordinate columns\nUse st_as_sf function in sf to convert the data frame to an sf spatial simple feature collection by:\n\npassing the coordinate columns to the coords parameter\nspecifying a coordinate reference system (CRS)\nopting to retain the coordinate columns as attribute columns in the resulting sf feature collection.\n\n\nKeep only the coordinates and station ID in resulting sf feature collection, and\nPlotting our gages as spatial features with ggplot2 using geom_sf.\n\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(Rspatialworkshop)\ngages = read.csv(system.file(\"extdata/Gages_flowdata.csv\", package = \"Rspatialworkshop\"))\n\ngages_sf <- gages |> \n  st_as_sf(coords = c(\"LON_SITE\", \"LAT_SITE\"), crs = 4269, remove = FALSE)  |> \n  dplyr::select(STATION_NM,LON_SITE, LAT_SITE)\n\nggplot() + geom_sf(data=gages_sf)"
  },
  {
    "objectID": "visualization.html#adding-web-map-services-in-mapview",
    "href": "visualization.html#adding-web-map-services-in-mapview",
    "title": "2  Visualization",
    "section": "\n2.5 Adding Web Map services in mapview\n",
    "text": "2.5 Adding Web Map services in mapview\n\nWe’ll visualize data with mapview and load a web map service layers alongside using mapview and underlying leaflet functionality.\nFirst we load load an excel file containing coordinate information in a known projection and promote to an sf spatial data frame.\n\nlibrary(Rspatialworkshop)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(mapview)\nfpath <- system.file(\"extdata\", \"Station_Locations.xlsx\", package=\"Rspatialworkshop\")\nstations <- read_xlsx(fpath)\nglimpse(stations)\n\nRows: 31\nColumns: 3\n$ Station <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",…\n$ x       <dbl> -2140749, -2140111, -2124688, -2125545, -1664112, 1606578, -17…\n$ y       <dbl> 2502887, 2469697, 2533842, 2556987, 2770644, 2698398, 2664873,…\n\nsummary(stations$x)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-2259078 -2124688 -1561956 -1630593 -1454137  1606578        2 \n\n# common clean up steps for spatial data - we can't use data missing coordinates so drop those records\nstations <- stations[complete.cases(stations),]\n# often spatial data in projected coordinates will have missing negative values for x values - common thing to fix:\nstations$x[stations$x > 0] <- 0 - stations$x[stations$x > 0]\nstations <- stations  |>  \n  st_as_sf(coords = c(\"x\", \"y\"), remove = FALSE)\n\n# in this case we know the particular Albers projection and have the information as a proj string\nst_crs(stations) <- \"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\" \n\nBasic interactive map of our spatial stations with mapview:\n\nmapview(stations)\n\n\n\n\n\n\nHere we’ll load a web map servcice (WMS) for the National Hydrography dataset. We’re looking at stream stations so imagine we want to visualize how closely these sites match a known rivers and stream network:\n\nlibrary(leaflet)\n# create a mapview object with our stations:\nm <- mapview(stations, legend=FALSE)\n\n# we configure the map attribute of our mapview object - try:\n# 'attributes(m) \n# to see those attributes\n\n#  The map attribute for mapview accepts leaflet methods - in this case we use addWMSTiles to add web map service tiles to the map\nm@map <- m@map  |>  addWMSTiles(group = 'NHDPlus',\n                              \"https://watersgeo.epa.gov/arcgis/services/NHDPlus_NP21/NHDSnapshot_NP21/MapServer/WmsServer?\",\n                              layers  = 4,\n                              options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n                              attribution = \"\")  |>  addWMSTiles(group = 'NHDPlusHR',\n                                                                \"https://hydro.nationalmap.gov/arcgis/services/NHDPlus_HR/MapServer/WMSServer?\",\n                                                                layers  = 9,\n                                                                options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n                                                                attribution = \"\")   |>  mapview:::mapViewLayersControl(names = c(\"NHDPlus\",\"NHDPlusHR\"))\nm"
  },
  {
    "objectID": "foundations.html#why-r-for-spatial-data-analysis",
    "href": "foundations.html#why-r-for-spatial-data-analysis",
    "title": "1  Foundations",
    "section": "\n1.3 Why R for Spatial Data Analysis?",
    "text": "1.3 Why R for Spatial Data Analysis?\nAdvantages\n\nR is:\n\nlightweight\nopen-source\ncross-platform\n\n\nWorks with contributed packages - currently 19948\n\nprovides extensibility\n\n\n\nAutomation and recording of workflow\nprovides reproducibility\n\nOptimized work flow - data manipulation, analysis and visualization all in one place\n\nprovides integration\n\n\n\nR does not alter underlying data - manipulation and visualization in memory\n\nR is great for repetitive graphics\nR is great for integrating different aspects of analysis - spatial and statistical analysis in one environment\n\nagain, integration\n\n\n\nLeverage statistical power of R (i.e. modeling spatial data, data visualization, statistical exploration)\nCan handle vector and raster data, as well as work with spatial databases and pretty much any data format spatial data comes in\nR’s GIS capabilities growing rapidly right now - new packages added monthly - currently about 275 spatial packages (depending on how you categorize)\n\nDrawbacks for usig R for GIS work\n\nR is not as good for interactive use as desktop GIS applications like ArcGIS or QGIS (i.e. editing features, panning, zooming, and analysis on selected subsets of features)\nExplicit coordinate system handling by the user\n\nno on-the-fly projection support\n\n\nIn memory analysis does not scale well with large GIS vector and tabular data\nSteep learning curve\nUp to you to find packages to do what you need - help not always great\n\n\n1.3.1 A Couple Motivating Examples\nHere’s a quick example demonstrating R’s flexibility and current strength for plotting and interactive mapping with very simple, expressive code:\n\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(dplyr)\n\n# find my town!\nmy_town <-tmaptools::geocode_OSM(\"Corvallis OR USA\", as.sf = TRUE)\nglimpse(my_town)\n#> Rows: 1\n#> Columns: 9\n#> $ query   <chr> \"Corvallis OR USA\"\n#> $ lat     <dbl> 44.56457\n#> $ lon     <dbl> -123.262\n#> $ lat_min <dbl> 44.51993\n#> $ lat_max <dbl> 44.60724\n#> $ lon_min <dbl> -123.3362\n#> $ lon_max <dbl> -123.231\n#> $ bbox    <POLYGON [°]> POLYGON ((-123.3362 44.5199...\n#> $ point   <POINT [°]> POINT (-123.262 44.56457)\n\n\n\n\n\n\n\nExercise\n\n\n\nRun and examine code chunk above and try geocoding examples you think of\n\nWhat does :: do?\nWhat does geocode_OSM() do?\nExplain how the code runs together using the |> chaining operator\nWhat is glimpse()? How is it useful compared to head()?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n:: specifies using geocode_OSM from the tmaptools package. R gives namespace preference to packages in order loaded. Some packages share function names, so when there is ambiguity, it’s good practice to disambiguate your functions with the double-colon\n\ngeocode_OSM() looks up a named features in OpenStreetMap and returns the coordinates (bonus - we’ll delve into more in next section - what coordinate reference system are coordinates in and how do you find out?)\nYou would translate code using the |> operator from:\n\n\ndo this |> do that |> do that\n\nTo\n\ndo this then do that then do that\n\n\nTechnically, it’s a transposed version of print() - columns run down page, data across like rows - it additionally gives you the number of observations and variables, and the data type of each column. Note that glimpse() omits geometry information - you would type the name of your object at the console to get full information, or use print.sf() or str().\n\n\nbonus: how would you quickly learn more about glimpse() from the console?\n\n\n\n\n\n1.3.1.1 Choropleth map\nThe tigris package can be used to get census boundaries such as states and counties as vector sf objects in R\n\nlibrary(tigris)\nlibrary(sf)\ncounties <- counties(\"Oregon\", cb = TRUE)\ncounties$area <- as.numeric(st_area(counties))\nglimpse(counties)\n\ntm_shape(counties) +\n  tm_polygons(\"area\", style=\"quantile\", title=\"Area of Oregon Counties\")\n\n\n\nFigure 1.1: ?(caption)\n\n\n\n\n\n1.3.1.2 Interactive mapping\n\nlibrary(mapview)\nmapviewOptions(fgb=FALSE)\nmapview(my_town, col=\"red\", col.regions = \"red\") +\n  mapview(counties, alpha.regions = .1)"
  },
  {
    "objectID": "geoprocessing.html#measures-and-units",
    "href": "geoprocessing.html#measures-and-units",
    "title": "3  Geoprocessing",
    "section": "\n3.3 Measures and Units",
    "text": "3.3 Measures and Units\nMeasures (with sf) make use of the underlying GEOS library, as well as the R units library that provides measure units for R vectors. Once a coordinate reference system has been defined for features, we often want to ask questions of our data such as:\n\nHow long is a line or a polygon perimeter (unit)\nWhat is the area of a polygon (unit^2)\nHow far apart / close together are objects from each other (unit)\n\nSome examples using county and gage datasets we’ve seen in previous section (with refresher again on pulling in data from a .csv file with x and y information and making it spatial):\n\nlibrary(readr)\nlibrary(sf) |> suppressPackageStartupMessages()\ngages <- read_csv(system.file(\"extdata\", \"Gages_flowdata.csv\", package = \"awra2020spatial\")) |> \n  dplyr::select(SOURCE_FEA, STATE, LAT_SITE, LON_SITE) |> \n  st_as_sf(coords = c(\"LON_SITE\", \"LAT_SITE\"), crs = 4269)\nst_distance(gages[1,], gages[2,])\n\nUnits: [m]\n         [,1]\n[1,] 58822.95\n\n\n\nlibrary(tigris)\nlibrary(sf) |> suppressPackageStartupMessages()\ncounties <- counties(\"Oregon\", cb = TRUE)\noptions(scipen=3)\nprint(paste0('The total area of all counties in Oregon is: ',sum(st_area(counties))))\n\n\n\n\n\n\n\nExercise\n\n\n\nHow did st_distance know to return the distance between our 2 gages in meters? What if we want that distance in feet? Or kilometers? Or our total area in Oregon in km2? There are a couple approaches:\n\nyou could set a projection that uses the units you want reported\nyou could simply look up the conversion factor and apply it manually\nyou can make use of the units library to explicitly set your units for the data\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nSetting the projection for the desired units\n\n\ngages_ft <- st_transform(gages, 2994)\nst_distance(gages_ft[1,], gages_ft[2,])\n\nUnits: [foot]\n         [,1]\n[1,] 193550.5"
  },
  {
    "objectID": "geoprocessing.html#operations-on-geometries",
    "href": "geoprocessing.html#operations-on-geometries",
    "title": "3  Geoprocessing",
    "section": "\n3.2 Operations on Geometries",
    "text": "3.2 Operations on Geometries\nThis breakdown of simple features follows for the most part this section in Spatial Data Science\n** Simple** and valid geometries - Certain conditions have to be met with simple features: + For linestrings to be considered simple they must not self-intersect:\n\nlibrary(sf) |> suppressPackageStartupMessages()\n\nWarning: package 'sf' was built under R version 4.2.3\n\n(ls <- st_linestring(rbind(c(0,0), c(1,1), c(2,2), c(0,2), c(1,1), c(2,0))))\n\nLINESTRING (0 0, 1 1, 2 2, 0 2, 1 1, 2 0)\n\n\n\n\n\n\n\n\n\nis_simple \n    FALSE \n\n\n\n\nFor polygons several other conditions have to be met to be simple:\n\npolygon rings are closed (the lastpoint equals the first)\npolygon holes (inner rings) are inside their exterior ring\npolygon inner rings maximally touch the exterior ring in single points, not over a line\na polygon ring does not repeat its own path\nin a multi-polygon, an external ring maximally touches another exterior ring in single points, not over a line\n\n\n\nWe can break down operations on geometries for vector features in the following way:\n\n\npredicates: a logical asserting a certain property is TRUE\n\n\nmeasures: a quantity (a numeric value, possibly with measurement unit)\n\ntransformations: newly generated geometries\n\nWe can look at these operations by what they operate on, whether the are single geometries, pairs, or sets of geometries:\n\n\nunary when it’s a single geometry\n\nbinary when it’s pairs of geometries\n\nn-ary when it’s sets of geometries\n\nUnary predicates work to describe a property of a geometry.\nA list of unary predicates:\n\n\npredicate\nmeaning\n\n\n\nis\nTests if geometry belongs to a particular class\n\n\nis_simple\nTests whether geometry is simple\n\n\nis_valid\nTest whether geometry is valid\n\n\nis_empty\nTests if geometry is empty\n\n\n\nA list of binary predicates is:\n\n\n\n\n\n\n\npredicate\nmeaning\ninverse of\n\n\n\ncontains\nNone of the points of A are outside B\nwithin\n\n\ncontains_properly\nA contains B and B has no points in common with the boundary of A\n\n\n\ncovers\nNo points of B lie in the exterior of A\ncovered_by\n\n\ncovered_by\nInverse of covers\n\n\n\n\ncrosses\nA and B have some but not all interior points in common\n\n\n\ndisjoint\nA and B have no points in common\nintersects\n\n\nequals\nA and B are topologically equal: node order or number of nodes may differ; identical to A contains B and A within B\n\n\n\nequals_exact\nA and B are geometrically equal, and have identical node order\n\n\n\nintersects\nA and B are not disjoint\ndisjoint\n\n\nis_within_distance\nA is closer to B than a given distance\n\n\n\nwithin\nNone of the points of B are outside A\ncontains\n\n\ntouches\nA and B have at least one boundary point in common, but no interior points\n\n\n\noverlaps\nA and B have some points in common; the dimension of these is identical to that of A and B\n\n\n\nrelate\nGiven a mask pattern, return whether A and B adhere to this pattern\n\n\n\n\nSee the Geometries chapter of Spatial Data Science for a full treatment that also covers **unary and binary measures* as well as unary, binary and n-ary transformers"
  },
  {
    "objectID": "geoprocessing.html#buffer-and-dissolve",
    "href": "geoprocessing.html#buffer-and-dissolve",
    "title": "3  Geoprocessing",
    "section": "\n3.6 Buffer and Dissolve",
    "text": "3.6 Buffer and Dissolve\nTypical GIS operations creating buffers around features (points, lines or polygons) and dissolving polygon boundaries based on a common attribute - here we show how to do these common GIS tasks in an R workflow.\nWe’ll demonstrate dissolving using both tidyverse functions in conjunction with simple features as well as using the st_union() function in sf.\nLet’s use our Oregon cities and counties data again for this.\nFirst we’ll demonstrate buffering which is a simpler operation using st_buffer with a supplied distance.\nWe pick our largest cities and buffer them by some arbitrary distance - we’ll say 20 miles (note below we convert feet to miles).\n\nlibrary(sf) |> suppressPackageStartupMessages()\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(mapview)\nmapviewOptions(fgb=FALSE)\n# Filter for metro areas\nmetros <- or_cities |> \n  dplyr::filter(CITY %in% c('PORTLAND','SALEM','EUGENE','CORVALLIS','BEND','MEDFORD'))\n# Buffer larger cities\nmetro_areas <- metros |> \n  st_buffer(105600)\nmapview(metro_areas)\n\n\n\n\n\n\nNext we show both methods to dissolve, categorizing Oregon counties as urban or rural and dissolving on these categories.\n\nlibrary(sf) |> suppressPackageStartupMessages()\n# urban area\ncounties <- st_transform(counties, st_crs(metro_areas)) # crs same\n\nurban_counties <- counties[metro_areas, ,op=st_intersects] # subsetting\n\nurban <- urban_counties |>  # Dissolve\n  st_union() |> # unite geometries\n  st_sf() |> # promote geometry back to data frame\n  dplyr::mutate(urban=TRUE) # assign urban\n\n# rural area\n# here we use tidyverse method to dissolve\nrural <- counties |> \n  dplyr::filter(!name %in% urban_counties$name) |> \n  dplyr::group_by(state_abbr) |> # just group by state - all the same\n  dplyr::mutate(AREA= geometry |> st_area()) |> \n  dplyr::summarise(AREA = sum(AREA)) # this is the cool part!\n\nmapview(urban, col.region='red') + mapview(rural)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWe made use of the summarise method above with our sf features to perform a dissolve - by - summarise has a special argument - do_union which is set to TRUE by default and unions grouped geometries.\nReview and experiment with different tidyverse methods that honor sf objects using the counties and cities data. These methods include:\n\nfilter\nselect\ngroup_by\nungroup\nmutate\ntransmute\nrowwise\nrename\nslice\nsummarise\ndistinct\n\ngather\n\npivot_longer\nspread\nnest\nunnest\nunite\nseparate\nseparate_rows\nsample_n\nsample_frac\n\nAn exhaustive list - when used with sf objects they simply preserve geometry - remember geometry is sticky and sf is tidyverse-compliant. You can use st_drop_geometry if you want to return tidyverse operations on sf features as tibbles or data.frames (or corece features to data.frames or tibbles).\nThis section in Spatial Data Science provides a nice background, as well as this vignette from the Geocomputation with R book on pitfalls to be aware of with spatial data in conjunction with tidyverse."
  },
  {
    "objectID": "geoprocessing.html#clipping",
    "href": "geoprocessing.html#clipping",
    "title": "3  Geoprocessing",
    "section": "\n3.7 Clipping",
    "text": "3.7 Clipping\nClipping is another extremely common GIS operation, and it’s simply a form of spatial subsetting that makes changes to the geometry list columns of affected features - it only applies to more complex geometries like lines, polygons and multi-lines and multi-polygons.\nLet’s use our city buffer for Corvallis and counties to demonstrate clipping. We’ll ask for just the portions of counties that intersect our ‘metro’ buffer around Corvallis.\nLet’s see what it looks like first:\n\nplot(metro_areas[2,c('geometry')],col = \"lightgrey\", axes=TRUE) \nplot(counties[metro_areas[2,],c('geometry')],border = \"grey\", add = TRUE)\n\n\n\n\nClip and show results:\n\nmetro_county_area <- st_intersection(metro_areas[2,], counties)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(metro_county_area), border = \"grey\", axes = TRUE) # intersecting area\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may encounter errors like this when running geoprocessing operations like st_join in R:\nError in wk_handle.wk_wkb(wkb, s2_geography_writer(oriented\n= oriented,  :  Loop 0 is not valid: Edge 772 crosses edge 774\nRunning st_make_valid might not fix.\nYou may need to turn off spherical geometry - sf_use_s2(TRUE), run st_make_valid, and then turn spherical geometry back on - sf_use_s2(FALSE) See background on S2 here and discussion of S2 related issues here"
  },
  {
    "objectID": "geoprocessing.html#centroids-and-type-transformations",
    "href": "geoprocessing.html#centroids-and-type-transformations",
    "title": "3  Geoprocessing",
    "section": "\n3.8 Centroids and Type Transformations",
    "text": "3.8 Centroids and Type Transformations\nWe can extract the centroid of features a couple different ways - typically we are looking for the geographic centroid which is the center of mass of a feature and can fall outside the boundaries of a feature for complex features. If we want our centroids to always be inside our features we need to ask for a point on surface. Lastly, casting in sf allows us to perform type transformations from one geometry type to another.\nWe’ll use our county data and bring in some river flow line data using the dataRetrieval package as well to demonstrate with linear features.\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.2.3\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\nSouthSantiam <- dataRetrieval::findNLDI(nwis = \"14187200\", nav = \"UT\", find = \"flowlines\", distance_km=10)\n  \nSouthSantiam <- SouthSantiam$UT_flowlines\nSouthSantiam <- SouthSantiam |> \n  st_transform(st_crs(counties))\n\nriv_cent1 <- st_centroid(SouthSantiam)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nriv_cent2 <- st_point_on_surface(SouthSantiam)\n\nWarning: st_point_on_surface assumes attributes are constant over geometries\n\nsel_counties <- counties |> \n  dplyr::filter(name %in% c('Marion','Yamhill','Polk','Benton','Multnomah','Clackamas','Linn','Washington'))\n\ncnt_cent1 <- st_centroid(sel_counties)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\ncnt_cent2 <- st_point_on_surface(sel_counties)\n\nWarning: st_point_on_surface assumes attributes are constant over geometries\n\np1 = tm_shape(sel_counties) + tm_borders() +\n  tm_shape(cnt_cent1) + tm_symbols(shape = 1, col = \"black\", size = 0.5) +\n  tm_shape(cnt_cent2) + tm_symbols(shape = 1, col = \"red\", size = 0.5)  \np2 = tm_shape(SouthSantiam) + tm_lines() +\n  tm_shape(riv_cent1) + tm_symbols(shape = 1, col = \"black\", size = 0.5) +\n  tm_shape(riv_cent2) + tm_symbols(shape = 1, col = \"red\", size = 0.5)  \ntmap_arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "geoprocessing.html#simplification",
    "href": "geoprocessing.html#simplification",
    "title": "3  Geoprocessing",
    "section": "\n3.9 Simplification",
    "text": "3.9 Simplification\nWith simplification we can generalize vector line and polygon objects - this is often useful to improve cartographic display as well as to reduce memory use and disk space used by complex geometric features. st_simplify in sf provides basic simplification implementing the Douglas-Peucker algorithm via GEOS to prune vertices. See rmapshaper and mapshaper for more extensive simplification algorithms.\n\nSant_simp = st_simplify(SouthSantiam, dTolerance = 500)  # 500 m\np1 = tm_shape(SouthSantiam) + tm_lines() \np2 = tm_shape(Sant_simp) + tm_lines() \ntmap_arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "geoprocessing.html#raster-operations",
    "href": "geoprocessing.html#raster-operations",
    "title": "3  Geoprocessing",
    "section": "\n3.10 Raster operations",
    "text": "3.10 Raster operations\n\n3.10.1 Cropping\nWe’ll use some built in data from a previous workshop to demonstrate a number of simple raster geoprocessing operations.\nWe’ll start with an elevation raster and Crater Lake boundary and project both to an local conformal projection (UTM Z10 North)\n\nlibrary(Rspatialworkshop)\nlibrary(terra)  |> suppressPackageStartupMessages()\nlibrary(sf)  |> suppressPackageStartupMessages()\ndata(CraterLake)\nraster_filepath <- system.file(\"extdata\", \"elevation.tif\", package = \"Rspatialworkshop\")\nelevation <- rast(raster_filepath)\nelevation <- project(elevation, \"EPSG:26910\", method = \"bilinear\")\nCraterLake <- st_transform(CraterLake,crs(elevation))\nplot(elevation)\nplot(CraterLake, add=TRUE, col=NA, border='blue')\n\n\n\n\nHere we’ll use crop to crop the elevation raster to the bounding box of our Crater Lake polygon feature\n\nelev_crop = crop(elevation, vect(CraterLake))\nplot(elev_crop)\nplot(CraterLake, add=TRUE, col=NA, border='blue')\n\n\n\n\nAnd finally we can use mask to mask the raster to just inside the polygon outline of Crater Lake National Park.\nNote - if you have a large raster, it makes a HUGE difference to use crop first, then mask - mask is a much more computationally intensive operation so it will pay off to crop first then mask. An interesting twitter thread regarding this just the other day:\n\n\n\n\n\n\n\n\n\n\n\nelev_mask = mask(elevation, vect(CraterLake))\nplot(elev_mask)\nplot(CraterLake, add=TRUE, col=NA, border='blue')\n\n\n\n\n\n3.10.2 Map Algebra\nWe can divide map algebra into a couple of categories:\n\nLocal - per-cell operations\n\nraster calculator\nreplacing values\nreclassifying\ncalculating indices\n\n\nFocal (neighborhood operations) - summarizing output cell value as the result of a window (such as a3 x 3 input cell block) Zonal operations - summarizing raster values for some zones (either another raster or a vector feature) Global - summarizing values over entire raster(s)\n\n\n3.10.2.1 Local Operations\nSay we want to convert our elevation raster from meters to feet:\n\nelev_feet = elevation * 3.28084\nelev_feet\n\nclass       : SpatRaster \ndimensions  : 981, 883, 1  (nrow, ncol, nlyr)\nresolution  : 78.76638, 78.76638  (x, y)\nextent      : 536109.7, 605660.4, 4714704, 4791974  (xmin, xmax, ymin, ymax)\ncoord. ref. : NAD83 / UTM zone 10N (EPSG:26910) \nsource(s)   : memory\nname        : srtm_12_04 \nmin value   :   1846.880 \nmax value   :   8910.399 \n\n\n\n\n\n\n\n\nHow would we verify the units of our projection? Take a minute to explore crs for terra\n\n\n\nOur max value is 8890, which makes sense - the high point in Crater Lake National Park is Mount Scott at 8,929’.\nWhat if we want to make elevation bins, or classify some elevations as NA with our elevation raster?\n\nreclass <- matrix(c(0, 500, 1, 500, 1000, 2, 1000, 1500, 3, 1500 , 2000, 4, 2000, 2700, 5), ncol = 3, byrow = TRUE)\nreclass\n\n     [,1] [,2] [,3]\n[1,]    0  500    1\n[2,]  500 1000    2\n[3,] 1000 1500    3\n[4,] 1500 2000    4\n[5,] 2000 2700    5\n\nelev_recl = classify(elevation, rcl = reclass)\nplot(elevation)\n\n\n\nplot(elev_recl)\n\n\n\n\n\nelev_new = elevation\nelev_new[elev_new > 2000] = NA\nplot(elev_new)\n\n\n3.10.2.2 Focal Operations\nA simple focal window operation\n\nelev_focal_mean = focal(elevation, \n                   w = matrix(1, nrow = 25, ncol = 25), \n                   fun = mean)\nplot(elev_focal_mean)\n\n\n3.10.2.3 Global Operations\n\nterra::global(elev_mask, fun=\"mean\", na.rm=TRUE)\nterra::global(elev_mask, fun=\"sum\", na.rm=TRUE)\n\n\n3.10.2.4 Zonal Operations\nHere we demonstrate using the zonal function in terra to summarize a value raster of elevation, using an srtm.tif from spDataLarge, by the zones of NLCD classes using nlcd.tif raster also in the spDataLarge package. We’ll expand more on zonal statistics including for categorical data in the final section of the workshop.\n\nlibrary(raster)\nsrtm_path = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nsrtm_path\nsrtm = rast(srtm_path)\nsrtm\n\nnlcd = rast(system.file(\"raster/nlcd2011.tif\", package = \"spDataLarge\"))\nsrtm_utm = project(srtm, nlcd, method = \"bilinear\")\nsrtm_zonal = zonal(srtm_utm, nlcd, na.rm = TRUE, fun = \"mean\")\nsrtm_zonal"
  }
]