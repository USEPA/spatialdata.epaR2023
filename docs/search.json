[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Data Workshop",
    "section": "",
    "text": "Welcome\nHello and welcome! The purpose of this site is to provide workshop materials for the Spatial Data workshop at the 2023 EPA R User Group Workshop. Slides that accompany this workshop are available for download linked here by clicking the “Download raw file” button via the ellipsis or downward arrow symbol on the right side of the screen."
  },
  {
    "objectID": "index.html#workshop-agenda",
    "href": "index.html#workshop-agenda",
    "title": "Spatial Data Workshop",
    "section": "Workshop Agenda",
    "text": "Workshop Agenda\n\n1:00pm - 1:45pm EDT: Introduction and Spatial Data Structures in R\n1:45pm - 2:15pm EDT: Vector Data Model\n2:15pm - 2:20pm Break\n2:20pm - 2:35pm EDT: Raster Data Model\n2:35pm - 2:45pm Break\n2:45pm - 3:15pm EDT:\n3:15pm - 3:20pm Break\n3:20pm - 3:45pm EDT:\n3:45pm - 4:15pm EDT:\n4:15pm - 4:45pm EDT:\n4:45pm - 5:00pm EDT:"
  },
  {
    "objectID": "index.html#author-introduction",
    "href": "index.html#author-introduction",
    "title": "Spatial Data Workshop",
    "section": "Author Introduction",
    "text": "Author Introduction\nMarc Weber is a geographer at the Pacific Ecological Systems Division (PESD) at the United States Environmental Protection Agency (USEPA). His work supports various aspects of the USEPA’s National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States, as he helped develop and maintains the StreamCat and LakeCat datasets. His work focuses on spatial analysis in R and Python, Geographic Information Science (GIS), aquatic ecology, remote sensing, open source science and environmental modeling\nMichael Dumelle is a statistician for the United States Environmental Protection Agency (USEPA). He works primarily on facilitating the survey design and analysis of USEPA’s National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States. His primary research interests are in spatial statistics, survey design, environmental and ecological applications, and software development."
  },
  {
    "objectID": "index.html#set-up",
    "href": "index.html#set-up",
    "title": "Spatial Data Workshop",
    "section": "Set Up",
    "text": "Set Up\nThe packages that we use throughout this workshop are listed below. To install and load them, run:\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"cranlogs\")\ninstall.packages(\"sf\")\ninstall.packages(\"lubridate\")"
  },
  {
    "objectID": "index.html#how-to-follow-along-with-material",
    "href": "index.html#how-to-follow-along-with-material",
    "title": "Spatial Data Workshop",
    "section": "How to follow along with material",
    "text": "How to follow along with material\nThis workshop was built using Quarto and bookdown and rendered to html. If you are familiar with using git and GitHub, you can fork and clone this repository, or simply clone directly and open the corresponding .qmd files to follow along with material in RStudio. You can also copy code snippets from the rendered book site and paste into your code files in RStudio.\nQuarto is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and it can render most existing Rmd files without modification. Like RMarkdown the benefits include:\n\nAllows for reproducible reporting from R\nYou write your document in markdown and embed executable code chunks using the knitr syntax\nYou can update your document at any time by re-knitting the code chunks and convert your document to a number of formats (i.e. html, pdf, word documents)\nWe assume everyone in the workshop is familiar with using RStudio"
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Spatial Data Workshop",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe views expressed in this manuscript are those of the authors and do not necessarily represent the views or policies of the U.S. Environmental Protection Agency. Any mention of trade names, products, or services does not imply an endorsement by the U.S. government or the U.S. Environmental Protection Agency. The U.S. Environmental Protection Agency does not endorse any commercial products, services, or enterprises."
  },
  {
    "objectID": "foundations.html#goals-and-outcomes",
    "href": "foundations.html#goals-and-outcomes",
    "title": "1  Foundations",
    "section": "\n1.1 Goals and Outcomes",
    "text": "1.1 Goals and Outcomes\n\nUnderstand fundamental spatial data structures and libraries in R.\nBecome familiar with coordinate reference systems.\nGeographic I/O"
  },
  {
    "objectID": "foundations.html#spatial-data-structures-in-r",
    "href": "foundations.html#spatial-data-structures-in-r",
    "title": "1  Foundations",
    "section": "\n1.4 Spatial Data Structures in R",
    "text": "1.4 Spatial Data Structures in R\nFirst we’ll walk through spatial data structures in R and review some key components of data structures in R that inform our use of spatial data in R.\n\n\n\n\n\n\nNote\n\n\n\nA few core libraries underpin spatial libraries in R (and Python!) and in GIS software applications such as QGIS and ArcPro. Spatial data structures across languages and applications are primarily organized through OSgeo and OGC). These core libraries include:\n\n\nGDAL –> For raster and feature abstraction and process\n\nPROJ –> A library for coordinate transformations and projections\n\nGEOS –> A Planar geometry engine for operations (measures, relations) such as calculating buffers and centroids on data with a projected CRS\n\nS2 –> a spherical geometry engine written in C++ developed by Google and adapted in R with the s2 package\n\n\n\nWe’ll see how these core libraries are called and used in the R packages we explore throughout this workshop.\n\n\n\n\n\n\nNote\n\n\n\n\nVector data are comprised of points, lines, and polygons that represent discrete spatial entities, such as a river, watershed, or stream gauge.\nRaster data divides spaces into rectilinear cells (pixels) to represent spatially continuous phenomena, such as elevation or the weather. The cell size (or resolution) defines the fidelity of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.1 Vector Data Model\nFor Vector data, Simple Features (officially Simple Feature Access) is both an OGC and International Organization for Standardization (ISO) standard that specifies how (mostly) two-dimensional geometries can represent and describe objects in the real world. The Simple Features specification includes:\n\na class hierarchy\na set of operations\nbinary and text encodings\n\nIt describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.\nIt outlines how the spatial elements of POINTS (XY locations with a specific coordinate reference system) extend to LINES, POLYGONS and GEOMETRYCOLLECTION(s).\nThe “simple” adjective also refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines that do not self-intersect.\n\n\n\n\n\n\n\n\nWe’ll load the sf library to load the classes and functions for working with spatial vector data and to explore how sf handles vector spatial data in R:\n\nlibrary(sf)          \n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the message you see in your console the first time you load sf? Hint: If you don’t see a message, you probably already have the sf library attached - you can uncheck it in your packages pane in RStudio, or you can run detach(\"package:sf\", unload = TRUE)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe output from library(sf) reports which versions of key geographic libraries such as GEOS the package is using\n\n\n\nYou can report on versions of external software used by sf with:\n\nsf::sf_extSoftVersion()\n#>           GEOS           GDAL         proj.4 GDAL_with_GEOS     USE_PROJ_H \n#>        \"3.9.3\"        \"3.5.2\"        \"8.2.1\"         \"true\"         \"true\" \n#>           PROJ \n#>        \"8.2.1\"\n\nWe’ll cover highlights of the functionality of sf but we encourage you to read the sf package website documentation r-spatial.github.io/sf which can be viewed offline in RStudio using:\n\nvignette(package = \"sf\") # see which vignettes are available\nvignette(\"sf1\")          # open the introduction vignette\n\nThe sfg class in sf represents the different simple feature geometry types in R: point, linestring, polygon (and ‘multi’ equivalents such as multipoints) or geometry collection. Vector data in R and sf can be broken down into three basic geometric primitives with equivalent sf functions to create them:\n\npoints - st_points()\n\nlinestrings - st_linestring()\n\npolygons - st_polygon()\n\n\nWe buld up the more complex simple feature geometries with: - st_multipoint() for multipoints - st_multilinestring() for multilinestrings - st_multipolygon() for multipolygons - st_geometrycollection() for geometry collections\nTypically we don’t build these up from scratch ourselves but it’s important to understnand these building blocks since primitives are the building blocks for all vector features in sf\n\n\n\n\n\n\n\nAll functions and methods in sf that operate on spatial data are prefixed by st_, which refers to spatial type.\nThis is similar to PostGIS\n\nThis makes them sf functions easily discoverable by command-line completion.\nSimple features are also native R data, using simple data structures (S3 classes, lists, matrix, vector).\n\n\n\n\nThese primitives can all be broken down into set(s) of numeric x,y coordinates with a known coordinate reference system (crs). #### Points - A point in sf is composed of one coordinate pair (XY) in a specific coordinate system. - A POINT has no length, no area and a dimension of 0. - z and m coordinates As well as having the necessary X and Y coordinates, single point (vertex) simple features can have:\n\na Z coordinate, denoting altitude, and/or\nan M value, denoting some “measure”\n\n\n# POINT defined as numeric vector\n(sf::st_dimension(sf::st_point(c(0,1))))\n#> [1] 0\n\n\nlibrary(ggplot2)\nggplot() + \n  geom_point(aes(x = c(1,2,3), y = c(3,1,2))) + \n  labs(x = \"X\", y = \"Y\")\n\n\n\n\n\n1.4.1.1 Lines\n\nA polyline is composed of an ordered sequence of two or more POINTs\nPoints in a line are called vertices/nodes and explicitly define the connection between two points.\nA LINESTRING has a length, has no area and has a dimension of 1 (length)\n\n\n# LINESTRING defined by matrix\n(sf::st_dimension(sf::st_linestring(matrix(1:4, nrow = 2))))\n#> [1] 1\n\n\nggplot() + \n  geom_line(aes(x = c(1,2,3), y = c(3,1,2))) + \n  geom_point(aes(x = c(1,2,3), y = c(3,1,2)), col = \"red\") + \n  labs(x = \"X\", y = \"Y\")\n\n\n\n\n\n1.4.1.2 Polygon\n\nA POLYGON is composed of 4 or more points whose starting and ending point are the same.\nA POLYGON is a surface stored as a list of its exterior and interior rings.\nA POLYGON has length, area, and a dimension of 2. (area)\n\n\n# POLYGON defined by LIST (interior and exterior rings)\n(sf::st_dimension(sf::st_polygon(list(matrix(c(1:4, 1,2), nrow = 3, byrow = TRUE)))))\n#> [1] 2\n\n\nggplot() + \n  geom_polygon(aes(x = c(1,2,3,1), y = c(3,1,2,3)), fill = \"green\", alpha = .5) + \n  geom_line(aes(x = c(1,2,3), y = c(3,1,2))) + \n  geom_point(aes(x = c(1,2,3), y = c(3,1,2)), col = \"red\") +\n  labs(x = \"X\", y = \"Y\")\n\n\n\n\nThese geometries as we created above are structure using the WKT format. sf usesWKTandWKB` formats to store vector geometry.\n:::{.callout-note} WKT (Well-Known Text) and WKB\n-Well-known text (WKT) encoding provide a human-readable description of the geometry. - The well-known binary (WKB) encoding is machine-readable, lossless, and faster to work with than text encoding.\nWKB is used for all interactions with GDAL and GEOS.\n\n1.4.1.3 Spatial Data Frames\nBuilding up from basic points, lines and strings, simple feature objects are stored in a data.frame, with geometry data in a special list-column, usually named ‘geom’ or ‘geometry’ - this is the same convention used in the popular Python package GeoPandas.\nLet’s look at an example spatial data set that comes in the spData package, us_states:\n\nlibrary(spData)\ndata(us_states)\nclass(us_states)\n#> [1] \"sf\"         \"data.frame\"\nnames(us_states)\n#> [1] \"GEOID\"        \"NAME\"         \"REGION\"       \"AREA\"        \n#> [5] \"total_pop_10\" \"total_pop_15\" \"geometry\"\n\nWe see that it’s an sf data frame with both attribute columns and the special geometry column we mentioned earlier - in this case named geometry.\nus_states$geometry is the ‘list column’ that contains the all the coordinate information for the US state polygons in this dataset.\nTo reiterateus_statesis ansfsimple feature collection which we can verify with:\n\nus_states\n#> Simple feature collection with 49 features and 6 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -124.7042 ymin: 24.55868 xmax: -66.9824 ymax: 49.38436\n#> Geodetic CRS:  NAD83\n#> First 10 features:\n#>    GEOID        NAME   REGION             AREA total_pop_10 total_pop_15\n#> 1     01     Alabama    South 133709.27 [km^2]      4712651      4830620\n#> 2     04     Arizona     West 295281.25 [km^2]      6246816      6641928\n#> 3     08    Colorado     West 269573.06 [km^2]      4887061      5278906\n#> 4     09 Connecticut Norteast  12976.59 [km^2]      3545837      3593222\n#> 5     12     Florida    South 151052.01 [km^2]     18511620     19645772\n#> 6     13     Georgia    South 152725.21 [km^2]      9468815     10006693\n#> 7     16       Idaho     West 216512.66 [km^2]      1526797      1616547\n#> 8     18     Indiana  Midwest  93648.40 [km^2]      6417398      6568645\n#> 9     20      Kansas  Midwest 213037.08 [km^2]      2809329      2892987\n#> 10    22   Louisiana    South 122345.76 [km^2]      4429940      4625253\n#>                          geometry\n#> 1  MULTIPOLYGON (((-88.20006 3...\n#> 2  MULTIPOLYGON (((-114.7196 3...\n#> 3  MULTIPOLYGON (((-109.0501 4...\n#> 4  MULTIPOLYGON (((-73.48731 4...\n#> 5  MULTIPOLYGON (((-81.81169 2...\n#> 6  MULTIPOLYGON (((-85.60516 3...\n#> 7  MULTIPOLYGON (((-116.916 45...\n#> 8  MULTIPOLYGON (((-87.52404 4...\n#> 9  MULTIPOLYGON (((-102.0517 4...\n#> 10 MULTIPOLYGON (((-92.01783 2...\n\nWe see information on\n\nthe geometry type\nthe dimensionality\nthe bounding box\nthe coordinate reference system\nthe header of the attributes\n\nRemember that sf simple feature collections are composed of:\n\n\n\n\nThe simple features data data sructure.\n\n\n\n\n\nrows of simple features (sf) that contain attributes and geometry - in green\neach of which have a simple feature geometry list-column (sfc) - in red\nwhich contains the underlying simple feature geometry (sfg) for each simple feature - in blue\n\nsf extends the generic plot function so that plot will naturally work with sf objects.\n\nplot(us_states)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nIs this the result you expected from plot()?\nInstead of making a single default map of your geometry, plot() in sf maps each variable in the dataset.\nHow would you plot just the geometry? Or just the two population variables? Try these variations on plot() with us_states\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(us_states$geometry)\nplot(us_states[,c('total_pop_10','total_pop_15')])\n#or\nus_states  |> dplyr::select(total_pop_10, total_pop_15) |> plot() \n\n\n\n\nThe alternative solution above highlights a key concept with sf - sf objects are ‘tidy-compliant’ and geometry is sticky - it carries along with objects implicitly when performing any tidy chained operations.\n\n\n\n\nThe simple features data data sructure.\n\n\n\n\nKeeping the geometry as part of regular data.frame objects in R (and in Python with `GeoPandas!) has numerous advantages - for instance:\n\nsummary(us_states['total_pop_15'])\n#>   total_pop_15               geometry \n#>  Min.   :  579679   MULTIPOLYGON :49  \n#>  1st Qu.: 1869365   epsg:4269    : 0  \n#>  Median : 4625253   +proj=long...: 0  \n#>  Mean   : 6415823                     \n#>  3rd Qu.: 6985464                     \n#>  Max.   :38421464\n\nThe geometry is sticky!\n\n1.4.2 Raster Data Model\nRaster data can be continuous (e.g. elevation, precipitation, atmospheric deposition) or it can be categorical (e.g. land use, soil type, geology type). Raster data can also be image based rasters which can be single-band or multi-band. You can also have a temporal component with space-time cubes.\n\n\n\n\nThe raster data model.\n\n\n\n\nSupport for gridded data in R in recent year has been best implemented with the raster package by Robert Hijmans. The raster package allowed you to:\n\nread and write almost any commonly used raster data format\nperform typical raster processing operations such as resampling, projecting, filtering, raster math, etc.\nwork with files on disk that are too big to read into memory in R\nrun operations quickly since the package relies on back-end C code\n\nThe terra package is the replacement for the raster package and has now superceeded it and we will largely focus on terra here. Examples here draw from both Spatial Data Science with R and terra and An introduction to terra in Geocomputation with R. Use help(“terra-package”) in the console for full list of available terra functions and comparison to / changes from raster.\nRaster representation is currently in flux a bit in R now with three choices of packages - raster and now terra which we’ve mentioned, as well as stars (spatiotemporal tidy arrays with R).\nTo familiarize ourselves with the terra package, let’s create an empty SpatRaster object - in order to do this we have to:\n\ndefine the matrix (rows and columns)\ndefine the spatial bounding box\n\n\n\n\n\n\n\nExercise\n\n\n\nNote that typically we would be reading raster data in from a file rather than creating a raster from scratch.\nRun the code steps below to familiarize yourself with constructing a RasterLayer from scratch.\nTry providing a different bounding box for an area of your choosing or changing the dimensions and view result (by simply typing myrast in the console).\n\nlibrary(terra)\nmyrast <- rast(ncol=10, nrow = 10, xmax=-116,xmin=-126,ymin=42,ymax=46)\nmyrast\n#> class       : SpatRaster \n#> dimensions  : 10, 10, 1  (nrow, ncol, nlyr)\n#> resolution  : 1, 0.4  (x, y)\n#> extent      : -126, -116, 42, 46  (xmin, xmax, ymin, ymax)\n#> coord. ref. : lon/lat WGS 84\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# just an example:\nmyrast <- rast(ncol=40, nrow = 40, xmax=-120,xmin=-136,ymin=52,ymax=56)\n\n\n\n\nterra uses an S4 slot structure with a SpatRaster object\n\nstr(myrast)\n#> S4 class 'SpatRaster' [package \"terra\"]\nisS4(myrast)\n#> [1] TRUE\n\nterra has dedicated functions addressing each of the following components: - dim(my_rast) returns the number of rows, columns and layers - ncell() returns the number of cells (pixels) - res() returns the spatial resolution - ext() returns spatial extent - crs() returns the coordinate reference system\n\n\n\n\n\n\nExercise\n\n\n\nExploring raster objects\n\nwhat is the minimal data required to define a SpatRaster?\nWhat is the CRS of our SpatRaster?\nHow do we pull out just the CRS for our SpatRaster?\nBuilding on this, what is the code to pull out just our xmin value in our extent, or bounding box?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnumber columns, number rows, and extent - terra will fill in defaults if values aren’t provided\n\n\nt <- rast()\nt\n#> class       : SpatRaster \n#> dimensions  : 180, 360, 1  (nrow, ncol, nlyr)\n#> resolution  : 1, 1  (x, y)\n#> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#> coord. ref. : lon/lat WGS 84\n\n\nWe didn’t provide one - terra uses default crs of WGS84 if you don’t provide a crs\n\n\n\ncrs(myrast)\n#> [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ID[\\\"EPSG\\\",6326]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n        ID[\\\"EPSG\\\",8901]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"longitude\\\",east,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433,\\n                ID[\\\"EPSG\\\",9122]]],\\n        AXIS[\\\"latitude\\\",north,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433,\\n                ID[\\\"EPSG\\\",9122]]]]\"\n\n\n\n\n\next(myrast)\n#> SpatExtent : -126, -116, 42, 46 (xmin, xmax, ymin, ymax)\n# just grab xmin\next(myrast)[1]\n#> xmin \n#> -126\n# we can see that extent is actually a c++ object in the ptr slot of our spatRaster object\n# names(myrast@ptr)\nmyrast$lyr.1@pnt$extent\n#> C++ object <000001c8a79deb70> of class 'SpatExtent' <000001c8a23709b0>\n\n\n\n\n\n1.4.2.1 Manipulating terra objects\nSo far we’ve just constructed a raster container with no values (try plotting what we have so far) - let’s provide values to the cells using the runif function to derive random values from the uniform distribution:\n\n#show we have no values\nhasValues(myrast)\n#> [1] FALSE\nvalues(myrast) <- runif(n=ncell(myrast))\nmyrast\n#> class       : SpatRaster \n#> dimensions  : 10, 10, 1  (nrow, ncol, nlyr)\n#> resolution  : 1, 0.4  (x, y)\n#> extent      : -126, -116, 42, 46  (xmin, xmax, ymin, ymax)\n#> coord. ref. : lon/lat WGS 84 \n#> source(s)   : memory\n#> name        :      lyr.1 \n#> min value   : 0.02395742 \n#> max value   : 0.99920712\n\nAn important point to make here is that objects in the terra package (and previously in raster) can be either in memory or on disk - note the value for our spatRaster r of ‘source’. If this were a large raster on disk, the value would be the path to the file on disk.\n\nmyrast$lyr.1@pnt$inMemory\n#> [1] TRUE\nhasValues(myrast)\n#> [1] TRUE\nmyrast$lyr.1@pnt$nlyr() # we just have one layer in our object\n#> [1] 1\n# or\nnlyr(myrast)\n#> [1] 1\n\nterra also provides plot method for it’s classes:\n\nplot(myrast)\n\n\n\n\nWe can also overwrite the cell values for our raster:\n\nvalues(myrast) <- 1:ncell(myrast)\nvalues(myrast)[1:15]\n#>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\nYou can access raster values via direct indexing or line, column indexing - take a minute to see how this works using raster r we just created - the syntax is:\nmyrast[i]\nmyrast[line, column]\n\n\n\n\n\n\nExercise\n\n\n\nHow is terra data storage unlike a matrix in R? Try creating a matrix with same dimensions and values and compare with the terra raster you’ve created:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nm <- matrix (1:100, nrow=10, ncol=10)\nm[1,10]\n#> [1] 91\nmyrast[1,10]\n#>   lyr.1\n#> 1    10\nmyrast[10]\n#>   lyr.1\n#> 1    10\n\n\n\n\n\n1.4.2.2 Reading existing rasters on disk\n\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nmy_rast = rast(raster_filepath)\nnlyr(my_rast)\n#> [1] 1\nplot(my_rast)\n\n\n\n\n\n1.4.3 Multiband rasters\nThe spatRaster object in terra can hold multiple layers (similar to RasterBrick and RasterStack which were two additional classes in the raster package). These layers correspond to multispectral satellite imagery or a time-series raster.\n\nlandsat = system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nlandsat = rast(landsat)\nlandsat\n#> class       : SpatRaster \n#> dimensions  : 1428, 1128, 4  (nrow, ncol, nlyr)\n#> resolution  : 30, 30  (x, y)\n#> extent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\n#> coord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \n#> source      : landsat.tif \n#> names       : landsat_1, landsat_2, landsat_3, landsat_4 \n#> min values  :      7550,      6404,      5678,      5252 \n#> max values  :     19071,     22051,     25780,     31961\nplot(landsat)\n\n\n\n\n\n1.4.3.1 Raster data with stars\n\nstars works with and stands for spatio-temporal arrays and can deal with more complex data types than either raster or terra such as rotated grids.\nstars integrates with sf and many sf functions have methods for stars objects (i.e. st_bbox and st_transform) - this makes sense since they are both written by Edzer Pebesma. terra unfortunately has poor / no integration with sf - this is a big issue for me personally and I will likely look to stars long-term for my raster processing.\nBasic example shown in stars vignette - reading in the 30m bands of a Landsat-7 image that comes with the stars package:\n\nlibrary(stars)\ntif = system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nls7 = read_stars(tif)\nplot(ls7, axes = TRUE)\n\n\n\n\nls7 (landsat7) is an object with 3 dimensions (x,y and band) and 1 attribute\n\nls7\n#> stars object with 3 dimensions and 1 attribute\n#> attribute(s):\n#>              Min. 1st Qu. Median     Mean 3rd Qu. Max.\n#> L7_ETMs.tif     1      54     69 68.91242      86  255\n#> dimension(s):\n#>      from  to  offset delta                     refsys point x/y\n#> x       1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\n#> y       1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n#> band    1   6      NA    NA                         NA    NA\n\nNote when we plotted above that the plot method with stars uses histogram stretching across all bands - we can also do stretching for each band individually:\n\nplot(ls7, axes = TRUE, join_zlim = FALSE)\n\n\n\n\nstars uses tidyverse methods for spatio-temporal arrays - an example of this is pulling out one band of an image using slice\n\nls7  |>  dplyr::slice(band, 6) -> band6\nband6\n#> stars object with 2 dimensions and 1 attribute\n#> attribute(s):\n#>              Min. 1st Qu. Median     Mean 3rd Qu. Max.\n#> L7_ETMs.tif     1      32     60 59.97521      88  255\n#> dimension(s):\n#>   from  to  offset delta                     refsys point x/y\n#> x    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE [x]\n#> y    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE [y]\n\nThis gives us a lower-dimensional array of just band 6 from the Landsat7 image."
  },
  {
    "objectID": "geoprocessing.html#goals-and-outcomes",
    "href": "geoprocessing.html#goals-and-outcomes",
    "title": "2  Geoprocessing",
    "section": "2.1 Goals and Outcomes",
    "text": "2.1 Goals and Outcomes\n\nThis is goal 1.\nThis is goal 2.\nThis is goal 3.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may click on any of the functions in this book to be directed to their respective documentation. For example, clicking on st_join() takes you to the documentation page for the st_join() function on the sf website."
  },
  {
    "objectID": "geoprocessing.html#spatial-subsetting",
    "href": "geoprocessing.html#spatial-subsetting",
    "title": "2  Geoprocessing",
    "section": "2.2 Spatial Subsetting",
    "text": "2.2 Spatial Subsetting"
  },
  {
    "objectID": "geoprocessing.html#spatial-join",
    "href": "geoprocessing.html#spatial-join",
    "title": "2  Geoprocessing",
    "section": "2.3 Spatial Join",
    "text": "2.3 Spatial Join"
  },
  {
    "objectID": "geoprocessing.html#dissolve",
    "href": "geoprocessing.html#dissolve",
    "title": "2  Geoprocessing",
    "section": "2.4 Dissolve",
    "text": "2.4 Dissolve"
  },
  {
    "objectID": "geoprocessing.html#extract-and-zonal-statistics",
    "href": "geoprocessing.html#extract-and-zonal-statistics",
    "title": "2  Geoprocessing",
    "section": "2.5 Extract and Zonal Statistics",
    "text": "2.5 Extract and Zonal Statistics"
  },
  {
    "objectID": "geoprocessing.html#fixing-topology-errors",
    "href": "geoprocessing.html#fixing-topology-errors",
    "title": "2  Geoprocessing",
    "section": "2.6 Fixing topology errors",
    "text": "2.6 Fixing topology errors\n\nshow typical fixes for topology errors\n\n\n\n\n\n\n\nNote\n\n\n\nYou may encounter errors like this when running geoprocessing operations like st_join in R:\nError in wk_handle.wk_wkb(wkb, s2_geography_writer(oriented\n= oriented,  :  Loop 0 is not valid: Edge 772 crosses edge 774\nRunning st_make_valid might not fix.\nYou may need to turn off spherical geometry - sf_use_s2(TRUE), run st_make_valid, and then turn spherical geometry back on - sf_use_s2(FALSE) See background on S2 here and discussion of S2 related issues here"
  },
  {
    "objectID": "visualization.html#goals-and-outcomes",
    "href": "visualization.html#goals-and-outcomes",
    "title": "3  Visualization",
    "section": "\n3.1 Goals and Outcomes",
    "text": "3.1 Goals and Outcomes\n\nUnderstand how to use several of the most popular libraries for plotting and visualizing spatial data inR\nThis is goal 2.\nThis is goal 3."
  },
  {
    "objectID": "visualization.html#mapview",
    "href": "visualization.html#mapview",
    "title": "3  Visualization",
    "section": "\n3.2 mapview",
    "text": "3.2 mapview"
  },
  {
    "objectID": "visualization.html#leaflet",
    "href": "visualization.html#leaflet",
    "title": "3  Visualization",
    "section": "\n3.3 leaflet",
    "text": "3.3 leaflet"
  },
  {
    "objectID": "visualization.html#ggplot2",
    "href": "visualization.html#ggplot2",
    "title": "3  Visualization",
    "section": "\n3.4 ggplot2",
    "text": "3.4 ggplot2"
  },
  {
    "objectID": "visualization.html#tmap",
    "href": "visualization.html#tmap",
    "title": "3  Visualization",
    "section": "\n3.5 tmap",
    "text": "3.5 tmap\nIt uses the same syntax as ggplot: the grammar of graphics - it supports both static and interactive modes\n\n3.5.1 Plotting rasters and vectors with tmap\nBring in boundary and elevation of Crater Lake NP (datasets in Rspatialworkshop package) and plot with tmap\n\nlibrary(Rspatialworkshop)\nlibrary(tmap)\ndata(CraterLake)\nraster_filepath <- system.file(\"extdata\", \"elevation.tif\", package = \"Rspatialworkshop\")\nelevation <- rast(raster_filepath)\n\nmap_crlk <- tm_shape(CraterLake) + tm_polygons(lwd = 2)\nmap_crlkel = map_crlk +\n  tm_shape(elevation) + tm_raster(alpha = 0.7,palette = terrain.colors(12)) + tm_layout(legend.position = c(\"left\",\"bottom\"),\n          legend.width = 1)\n\nmap_crlkel"
  },
  {
    "objectID": "advanced_applications.html#goals-and-outcomes",
    "href": "advanced_applications.html#goals-and-outcomes",
    "title": "4  Advanced Applications",
    "section": "4.1 Goals and Outcomes",
    "text": "4.1 Goals and Outcomes\n\nThis is goal 1.\nThis is goal 2.\nThis is goal 3.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may click on any of the functions in this book to be directed to their respective documentation. For example, clicking on st_join() takes you to the documentation page for the st_join() function on the sf website."
  },
  {
    "objectID": "advanced_applications.html#advanced-applications",
    "href": "advanced_applications.html#advanced-applications",
    "title": "4  Advanced Applications",
    "section": "4.2 Advanced Applications",
    "text": "4.2 Advanced Applications\n\n4.2.1 Using Web Services"
  },
  {
    "objectID": "resources.html#r-spatial-resources",
    "href": "resources.html#r-spatial-resources",
    "title": "Resources",
    "section": "R Spatial Resources",
    "text": "R Spatial Resources\n\nR Spatial - Spatial Data Science with R\nGeocomputation with R\nR Spatial Task View\nModern Geospatial Data Analysis with R by Zev Ross\nSIGR2021 Summer School\nSpatial Data Science - Pebesma and Bivand\nSpatial Data Science Course- Prof. Adam Wilson\nIntroduction to Mapping and Spatial Analysis with R\nR Spatial Workshop for EPA R User Group\nIntro to GIS and Spatial Analysis by Manuel Gimond\nFOSS4G2019 R for Geospatial Processing\nAn Introduction to Spatial Analysis and Mapping in R\nEarth Analytics Spatial Data in R\nHydroinformatics at VT: Extensive Notes and exercises for a course on data analysis techniques in hydrology using the programming language R"
  },
  {
    "objectID": "resources.html#r-vector-processing-simple-features-resources",
    "href": "resources.html#r-vector-processing-simple-features-resources",
    "title": "Resources",
    "section": "R Vector Processing / Simple Features Resources",
    "text": "R Vector Processing / Simple Features Resources\n\nSimple Features for R\nSpatial Data in R: New Directions\nsp-sf Migration\nAn Exploration of Simple Features for R\nSimple Features: Building Spatial Data Pipelines in R\nTidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf"
  },
  {
    "objectID": "resources.html#r-raster-resources",
    "href": "resources.html#r-raster-resources",
    "title": "Resources",
    "section": "R Raster Resources",
    "text": "R Raster Resources\n\nterra\nSpatial Data Science with R and terra\nstars - spatiotemporal arrays\nWageningen University Intro to Raster\nWageningen University Advanced Raster Analysis\nThe Visual Raster Cheat Sheet GitHub Repo\nRastervis"
  },
  {
    "objectID": "resources.html#r-mapping-resources",
    "href": "resources.html#r-mapping-resources",
    "title": "Resources",
    "section": "R Mapping Resources",
    "text": "R Mapping Resources\n\nmapview\nLeaflet for R\ntmap\nZev Ross Creating beautiful demographic maps in R with the tidycensus and tmap packages\nGeocomputation with R: Making maps with R\nNico Hahn: Making Maps with R R"
  },
  {
    "objectID": "resources.html#web-services-in-r",
    "href": "resources.html#web-services-in-r",
    "title": "Resources",
    "section": "Web Services in R",
    "text": "Web Services in R\n\nAccessing REST API (JSON data) using httr and jsonlite\nWorking with Geospatial Hydrologic Data Using Web Services (R)"
  },
  {
    "objectID": "resources.html#general-r-resources",
    "href": "resources.html#general-r-resources",
    "title": "Resources",
    "section": "General R Resources",
    "text": "General R Resources\n\nGoogle R Style Guide\nAdvanced R by Hadley Wickham"
  },
  {
    "objectID": "foundations.html#why-r-for-spatial-analysis",
    "href": "foundations.html#why-r-for-spatial-analysis",
    "title": "1  Foundations",
    "section": "\n1.3 Why R for Spatial Analysis",
    "text": "1.3 Why R for Spatial Analysis\nAdvantages\n\nR is:\n\nlightweight\nopen-source\ncross-platform\n\n\nWorks with contributed packages - currently 19926\n\nprovides extensibility\n\n\n\nAutomation and recording of workflow\nprovides reproducibility\n\nOptimized work flow - data manipulation, analysis and visualization all in one place\n\nprovides integration\n\n\n\nR does not alter underlying data - manipulation and visualization in memory\n\nR is great for repetitive graphics\nR is great for integrating different aspects of analysis - spatial and statistical analysis in one environment\n\nagain, integration\n\n\n\nLeverage statistical power of R (i.e. modeling spatial data, data visualization, statistical exploration)\nCan handle vector and raster data, as well as work with spatial databases and pretty much any data format spatial data comes in\nR’s GIS capabilities growing rapidly right now - new packages added monthly - currently about 275 spatial packages (depending on how you categorize)\n\nDrawbacks for usig R for GIS work\n\nR is not as good for interactive use as desktop GIS applications like ArcGIS or QGIS (i.e. editing features, panning, zooming, and analysis on selected subsets of features)\nExplicit coordinate system handling by the user\n\nno on-the-fly projection support\n\n\nIn memory analysis does not scale well with large GIS vector and tabular data\nSteep learning curve\nUp to you to find packages to do what you need - help not always great\n\n\n1.3.1 A Couple Motivating Examples\nHere’s a quick example demonstrating R’s flexibility and current strength for plotting and interactive mapping with very simple, expressive code:\n\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(dplyr)\n\nmy_town <-tmaptools::geocode_OSM(\"Corvallis OR USA\", \n        as.sf = TRUE)  |>  \n  glimpse()\n#> Rows: 1\n#> Columns: 9\n#> $ query   <chr> \"Corvallis OR USA\"\n#> $ lat     <dbl> 44.56457\n#> $ lon     <dbl> -123.262\n#> $ lat_min <dbl> 44.51993\n#> $ lat_max <dbl> 44.60724\n#> $ lon_min <dbl> -123.3362\n#> $ lon_max <dbl> -123.231\n#> $ bbox    <POLYGON [°]> POLYGON ((-123.3362 44.5199...\n#> $ point   <POINT [°]> POINT (-123.262 44.56457)\n\n\n\n\n\n\n\nQuick Exercise\n\n\n\nRun and examine code chunk above and try geocoding examples you think of\n\nWhat is the double colon doing?\nWhat is the geocode_OSM function doing?\nExplain how the code runs together using the |> chaining operator\nWhat is glimpse? How is it useful compared to the head function?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nIt specifies using geocode_OSM from the tmaptools package. R gives namespace preference to packages in order loaded; some packages share function names; so it’s good practice to disambiguate your functions with the double-colon\nIt is looking up a named feature in OpenStreetMap and returning the coordinates (bonus - we’ll delve into more in next section - what coordinate reference system are coordinates in and how do you find out?)\nYou would translate code using the |> operator from:\n\n\ndo this |> do that |> do that\n\nTo\n\ndo this then do that then do that\n\n\nTechnically, it’s a transposed version of print - columns run down page, data across like rows - it additionally gives you the number of observations and variables, and the data type of each column. Note that glimpse omits geometry information - you would type the name of your object at the console to get full information, or use print.sf() or str().\n\n\nbonus: how would you quickly learn more about glimpse from the console?\n\n\n\n\n\n1.3.1.1 Choropleth map\nThe tigris package can be used to get census boundaries such as states and counties as vector sf objects in R\n\nlibrary(tigris)\nlibrary(sf, quietly = T)\ncounties <- counties(\"Oregon\", cb = TRUE)\ncounties$area <- as.numeric(st_area(counties))\nglimpse(counties)\n\ntm_shape(counties) +\n  tm_polygons(\"area\", \n              style=\"quantile\", \n              title=\"Area of Oregon Counties\")\n\n\n\n\n\n1.3.1.2 Interactive mapping\n\nlibrary(mapview)\nmapviewOptions(fgb=FALSE)\nmapview(my_town, col=\"red\", col.regions = \"red\") + mapview(counties, alpha.regions = .1)"
  },
  {
    "objectID": "foundations.html#background-on-data-structures",
    "href": "foundations.html#background-on-data-structures",
    "title": "1  Foundations",
    "section": "\n1.2 Background on data structures",
    "text": "1.2 Background on data structures\n\n\n\n\n\n\nNote\n\n\n\nMuch of this background on data structures is borrowed from Mike Johnson’s Introduction to Spatial Data Science and lecture material from our AWRA 2022 Geo Workshop\n\n\nBefore we dive into spatial libraries, it’s worth a quick review of relevant data structures in R - this will be a whirlwind overview, assuming most everyone is familiar with using R already.\n\nYou, computers, and software ‘understand’ values in particular and different ways\n\nComputers convert bytes –> hex –> value\n\nHumans read values\nSoftware reads Hex bytes\nHardware reads Binary bytes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat’s the difference between 10 and ‘10’?\n\n\nTo us: meaning\n\nTo software: how it is handled\n\nTo a computer: nothing\n\nWe need human-defined (computer-guessable) data types\n\n\nFundamental data types\n\n# Numeric\ntypeof(1.9)\n#> [1] \"double\"\n# Integer\ntypeof(1L)\n#> [1] \"integer\"\n# Boolean\ntypeof(TRUE)\n#> [1] \"logical\"\n# Character\ntypeof(\"Welcome\")\n#> [1] \"character\"\n\nStoring more than one value requires a structure.\nValues can be structured in ways such as:\n\nvectors\ndimensions/attributes\ndata.frames\n\nAnd data.frames can be operated on with functions such as:\n\nfilter\nselect\nmutate\nsummarize\ngroup_by\n\n\n1.2.1 Vectors\n\n\n\n\n\n\nNote\n\n\n\n‘vector’ has two meanings when working with GIS data and working in R!\n\ngeographic vector data (which we’ll explore)\n\nvector class (what we’re talking about here)\n\ngeographic vector data is a data model, and the vector class is an R class like data.frame and matrix\n\n\nVectors come in two flavors:\n\natomic\nlist\n\n\natomic vectors elements must have the same type\nlists elements can have different types\n\n1.2.2 Atomic vectors\n\n# Numeric\ndbl_vec = c(1.9, 2, 3.5)\ntypeof(dbl_vec)\n#> [1] \"double\"\nlength(dbl_vec)\n#> [1] 3\n# Logical\nlg_vec = c(TRUE, FALSE, F, T)\ntypeof(lg_vec)\n#> [1] \"logical\"\nlength(lg_vec)\n#> [1] 4\n\nCoercion\n\n\ntype is a property of a vector\nWhen you try to combine different types they’ll be coerced in the following fixed order:\n\ncharacter => double => integer => logical\n\n\nCoercion occurs automatically but generates a warning and a missing value when it fails\n\n\nc(\"a\", 1)\n#> [1] \"a\" \"1\"\nc(\"a\", TRUE)\n#> [1] \"a\"    \"TRUE\"\nc(4.5, 1L)\n#> [1] 4.5 1.0\nc(\"1\", 18, \"GIS\")\n#> [1] \"1\"   \"18\"  \"GIS\"\nas.numeric(c(\"1\", 18, \"GIS\"))\n#> [1]  1 18 NA\nas.logical(c(\"1\", 18, \"GIS\"))\n#> [1] NA NA NA\n\nSubsetting atomic vectors\n\n# Atomic numeric vector\n(x = c(3.4, 7, 18, 9.6))\n#> [1]  3.4  7.0 18.0  9.6\n\n# Third Value\nx[3]\n#> [1] 18\n\n# Third and Fourth value\nx[c(3,4)]\n#> [1] 18.0  9.6\n\n# Drop the third value\nx[-3]\n#> [1] 3.4 7.0 9.6\n\n# Keep the 1 and 2 value, but drop 3 and 4\nx[c(T,T,F,F)]\n#> [1] 3.4 7.0\n\n\n1.2.3 Matrix\n\nA matrix is 2D atomic (row, column)\n\nSame data types\nSame column length\n\n\n\nThis is how spatial raster data is structured\nSubsetting matrices uses row,column (i,j) syntax\n\n(x = matrix(1:9, nrow = 3))\n#>      [,1] [,2] [,3]\n#> [1,]    1    4    7\n#> [2,]    2    5    8\n#> [3,]    3    6    9\nx[3,]\n#> [1] 3 6 9\nx[,3]\n#> [1] 7 8 9\nx[3,3]\n#> [1] 9\n\n\n1.2.4 Array\n\nArray is a 3d Atomic (row, column, slice)\n\nThis is how spatial raster data with a time dimension is structured\n\n(array(c(1:12), dim = c(3,2,2)))\n#> , , 1\n#> \n#>      [,1] [,2]\n#> [1,]    1    4\n#> [2,]    2    5\n#> [3,]    3    6\n#> \n#> , , 2\n#> \n#>      [,1] [,2]\n#> [1,]    7   10\n#> [2,]    8   11\n#> [3,]    9   12\n\nSubsetting arrays uses row, column, slice syntax (i,j,z)\n\n(x = array(1:12, dim = c(2,2,3)))\n#> , , 1\n#> \n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\n#> \n#> , , 2\n#> \n#>      [,1] [,2]\n#> [1,]    5    7\n#> [2,]    6    8\n#> \n#> , , 3\n#> \n#>      [,1] [,2]\n#> [1,]    9   11\n#> [2,]   10   12\nx[1,,]\n#>      [,1] [,2] [,3]\n#> [1,]    1    5    9\n#> [2,]    3    7   11\nx[,1,]\n#>      [,1] [,2] [,3]\n#> [1,]    1    5    9\n#> [2,]    2    6   10\nx[,,1]\n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\n\n\n1.2.5 Lists\nEash list element can be any data type\n\n(my_list <- list(\n  matrix(1:4, nrow = 2), \n  \"GIS is great!\", \n  c(TRUE, FALSE, TRUE), \n  c(2.3, 5.9)\n))\n#> [[1]]\n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\n#> \n#> [[2]]\n#> [1] \"GIS is great!\"\n#> \n#> [[3]]\n#> [1]  TRUE FALSE  TRUE\n#> \n#> [[4]]\n#> [1] 2.3 5.9\n\n\ntypeof(my_list)\n#> [1] \"list\"\n\nSubsetting Lists\n\nEach element of a list can be accessed with the [[ operator\n\n\nmy_list[[1]]\n#>      [,1] [,2]\n#> [1,]    1    3\n#> [2,]    2    4\nmy_list[[1]][1,2]\n#> [1] 3\n\n\n1.2.6 Data Frames\n\n\ndata.frame is built on the list structure in R\nlength of each atomic or list vector has to be the same\nThis gives data.frame objects rectangular structure so they share properties of both matrices and lists\n\n\n(df1 <- data.frame(name = c(\"Me\", \"Tim\", \"Sarah\"),\n                  age  = c(53,15,80),\n                  retired = c(F,F,T)))\n#>    name age retired\n#> 1    Me  53   FALSE\n#> 2   Tim  15   FALSE\n#> 3 Sarah  80    TRUE\ntypeof(df1)\n#> [1] \"list\"\n\nSubsetting a data.frame\n\ndf1[1,2]\n#> [1] 53\n\n# or like a list\ndf1[[2]]\n#> [1] 53 15 80\n\n# or with column name operator\ndf1$name\n#> [1] \"Me\"    \"Tim\"   \"Sarah\"\n\n\n1.2.7 Data manipulation\n\n\ndata.frame manipulation is all based on SQL queries\nR abstracts the SQL logic and provides function-ized methods\n\ndplyr in the tidyverse ecosystem provides the ’grammar of data manipulation` approach we’ll use in this workshop\n\nData manipulation verbs:\n\nPrimary:\n\n\nselect(): keeps or removes variables based on names\n\nfilter(): keeps or removes observations based on values _ Manipulation:\n\nmutate(): adds new variables that are functions of existing variables\n\nsummarise(): reduces multiple values down to a single summary\n\narrange(): changes ordering of the rows\n\n\nGrouping:\n\n\ngroup_by(): combine with any or all of the above to perform manipulation ‘by group’\n\n\n\n1.2.8 Pipe operator\n\nThe pipe operator (native R pipe operator |> or magrittr pipe operator %>%) provides a more concise and expressive coding experience\nThe pipe passes the object on the left hand side of the pipe into the first argument of the right hand function\nTo be |> compatible, the data.frame is ALWAYS the fist argument to dplyr verbs\n\nA demonstration using dataRetrieval package stream gage data from USGS:\n\nflows <- dataRetrieval::readNWISdv(siteNumbers = '14187200',\n                   parameterCd = \"00060\")  |>  \n  dataRetrieval::renameNWISColumns() \ndplyr::glimpse(flows)\n#> Rows: 18,331\n#> Columns: 5\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-…\n#> $ Flow      <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 92…\n#> $ Flow_cd   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the :: after the package name and before the function in the R code above doing? How would you find out more about it?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt disambiguates the function or method by selecting a definition from a particular namespace.\nYou can get help on it the same way you get help on anything in R:\n\n?'::'\n# or\nhelp(\"::\")\n\n\n\n\n\n1.2.9 Filter\n\n\nfilter() takes logical (binary) expressions and returns the rows in which all conditions are TRUE.\n\nFilter on a single condition:\n\nflows |> \ndplyr::filter(Flow > 900) |> \n  dplyr::glimpse()\n#> Rows: 14,440\n#> Columns: 5\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 1973-08-04, 1973-08-05, 1973-08-06, 1973-08-07, 1973-08-…\n#> $ Flow      <dbl> 930, 939, 939, 944, 932, 927, 925, 927, 928, 945, 938, 94…\n#> $ Flow_cd   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n\nOr multiple conditions:\n\nflows  |> \ndplyr::filter(Flow > 900, Date > as.Date(\"2010-01-01\"))  |>  \n  dplyr::glimpse()\n#> Rows: 4,390\n#> Columns: 5\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 2010-01-02, 2010-01-03, 2010-01-04, 2010-01-05, 2010-01-…\n#> $ Flow      <dbl> 7870, 6920, 5860, 7860, 10000, 10100, 9760, 9130, 8600, 7…\n#> $ Flow_cd   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n\n\n1.2.10 Select\n\nSubset variables (columns) you want to keep or exlcude by name\n\nJust keep three columns\n\nflows |> \ndplyr::select(Date, Flow)  |> \n  dplyr::glimpse()\n#> Rows: 18,331\n#> Columns: 2\n#> $ Date <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-05, 1…\n#> $ Flow <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 928, 94…\n\nExclude just one\n\nflows |> \ndplyr::select(-Flow_cd)  |> \n  dplyr::glimpse()\n#> Rows: 18,331\n#> Columns: 4\n#> $ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"…\n#> $ site_no   <chr> \"14187200\", \"14187200\", \"14187200\", \"14187200\", \"14187200…\n#> $ Date      <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-…\n#> $ Flow      <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 92…\n\n..And can rename while selecting\n\nflows |> \ndplyr::select(Date, flow_cfs = Flow)  |> \n  dplyr::glimpse()\n#> Rows: 18,331\n#> Columns: 2\n#> $ Date     <date> 1973-08-01, 1973-08-02, 1973-08-03, 1973-08-04, 1973-08-0…\n#> $ flow_cfs <dbl> 809, 828, 829, 930, 939, 939, 944, 932, 927, 925, 927, 928…\n\n\n1.2.11 Mutate\n\n\nmutate() defines and inserts new variables into a existing data.frame\n\n\nmutate() builds new variables sequentially so you can reference earlier ones when defining later ones\n\nWe can extract Year and Month as new variables from the Date variable using date time\n\nflows  |> \n  dplyr::select(Date, Flow)  |>  \n  dplyr::filter(Date > as.Date('2010-01-01'))  |> \n  dplyr::mutate(Year  = format(Date, \"%Y\"),\n         Month = format(Date, \"%m\"))  |>  \n  dplyr::glimpse()\n#> Rows: 5,028\n#> Columns: 4\n#> $ Date  <date> 2010-01-02, 2010-01-03, 2010-01-04, 2010-01-05, 2010-01-06, …\n#> $ Flow  <dbl> 7870, 6920, 5860, 7860, 10000, 10100, 9760, 9130, 8600, 7040,…\n#> $ Year  <chr> \"2010\", \"2010\", \"2010\", \"2010\", \"2010\", \"2010\", \"2010\", \"2010…\n#> $ Month <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"…\n\n\n1.2.12 Summarize and Group_By\n\n\nsummarize() allowa you to summarize across all observations\n\ngroup_by() allows you to apply any of these manipulation verbs by group in your data\n\n\nflows |> \n dplyr::select(Date, Flow) |> \n dplyr::mutate(Year  = format(Date, \"%Y\"))  |> \n dplyr::group_by(Year) |> \n dplyr::summarize(meanQ = mean(Flow),\n           maxQ = max(Flow))\n#> # A tibble: 51 × 3\n#>    Year  meanQ  maxQ\n#>    <chr> <dbl> <dbl>\n#>  1 1973  4669. 13200\n#>  2 1974  3659. 14400\n#>  3 1975  3611. 14100\n#>  4 1976  2340. 15000\n#>  5 1977  2860. 16200\n#>  6 1978  2206. 11300\n#>  7 1979  2378. 12000\n#>  8 1980  2548. 14700\n#>  9 1981  2976. 17000\n#> 10 1982  3424. 15100\n#> # ℹ 41 more rows\n\n\n\n\n\n\n\nNote\n\n\n\nYou may click on any of the functions in this book to be directed to their respective documentation. For example, clicking on st_join() takes you to the documentation page for the st_join() function on the sf website."
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "3  Visualization",
    "section": "",
    "text": "Understand how to use several of the most popular libraries for plotting and visualizing spatial data inR\nThis is goal 2.\nThis is goal 3."
  },
  {
    "objectID": "foundations.html#coordinate-reference-systems",
    "href": "foundations.html#coordinate-reference-systems",
    "title": "1  Foundations",
    "section": "\n1.5 Coordinate Reference Systems",
    "text": "1.5 Coordinate Reference Systems\nA CRS is made up of several components:\n\n\nCoordinate system: The x,y grid that defines where your data lies in space\n\nHorizontal and vertical units: The units describing grid along the x,y and possibly z axes\n\nDatum: The modeled version of the shape of the earth\n\nProjection details: If projected, the mathematical equation used to flatten objects from round surface (earth) to flat surface (paper or screen)\n\n\n\n\n\nSource: https://mgimond.github.io/Spatial/coordinate-systems.html\n\n\n\n\n\n1.5.1 The ellipsoid and geoid\nThe earth is a sphere, but more precisely, an ellipsoid, which is defined by two radii: - semi-major axis (equatorial radius) - semi-minor axis (polar radius)\nThe terms speroid and ellipsoid are used interchangeably. One particular spheroid is distinguished from another by the lengths of the semimajor and semiminor axes. Both GRS80 and WGS84 spheroids use semimajor and semiminor axes of 6,378,137 meters and 6,356,752 meters respectively (the semiminor axis differs by thousandths of meters between the two). You’ll encounter older spheroid / ellipsoids out there such as Clark 1866.\nMore precisely than an ellipsoid, though, we know that earth is a geoid - it is not perfectly smooth - and modelling the the undulations due to changes in gravitational pull for different locations is crucial to accuracy in a GIS. This is where a datum comes in - a datum is built on top of the selected spheroid and can incorporate local variations in elevation. We have many datums to choose from based on location of interest - in the US we would typically choose NAD83\n\n1.5.2 Why you need to know about CRS working with spatial data in R:\n\nlibrary(Rspatialworkshop)\nlibrary(readr)\nlibrary(sf)\ndata(pnw)\n\ngages = read_csv(system.file(\"extdata/Gages_flowdata.csv\", package = \"Rspatialworkshop\"),show_col_types = FALSE)\n\ngages_sf <- gages  |> \n  st_as_sf(coords = c(\"LON_SITE\", \"LAT_SITE\"), crs = 4269, remove = FALSE)  |> \n  dplyr::select(STATION_NM,LON_SITE, LAT_SITE)\n\n# Awesome, let's plot our gage data and state boundaries!\nplot(pnw$geometry, axes=TRUE)\nplot(gages_sf$geometry, col='red', add=TRUE)\n\n\n\n# um, what?\n\nThere is no ‘on-the-fly’ projection in R - you need to make sure you specify the CRS of your objects, and CRS needs to match for any spatial operations or you’ll get an error\n\nspatialreference.org is your friend in R - chances are you will use it frequently working with spatial data in R.\nprojection Wizard is also really useful, as is epsg.io\n\n1.5.3 Defining your coordinate reference system\nA coordinate reference system for spatial data can be defined in different ways - for instance for the standard WGS84 coordinate system you could use:\n\nProj4\n\n+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\nOGC WKT\n\nGEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.01745329251994328,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]]\n\n\nESRI WKT\n\nGEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]]\n\n\n‘authority:code’ identifier\n\nEPSG:4326\n\n\n\nProj4 used to be the standard crs identifier in R but the preferrable way is ty use the AUTHORITY:CODE method which sf as well as software like QGIS will recognize. The AUTHORITY:CODE method is durable and easily discoverable online.\nThe WKT representation of EPSG:4326 in sf is:\n\nsf::st_crs('EPSG:4326')\n#> Coordinate Reference System:\n#>   User input: EPSG:4326 \n#>   wkt:\n#> GEOGCRS[\"WGS 84\",\n#>     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#>         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#>         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]],\n#>         ENSEMBLEACCURACY[2.0]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Horizontal component of 3D system.\"],\n#>         AREA[\"World.\"],\n#>         BBOX[-90,-180,90,180]],\n#>     ID[\"EPSG\",4326]]\n\n\n1.5.4 Projected coordinate systems\nTypically we want to work with data that is projected. Projected coordinate systems (which are based on Cartesian coordinates) have: an origin, an x axis, a y axis, and a linear unit of measure. Going from geographic coordinates to a projected coordinate reference systems requires mathematical transformations.\nFour spatial properties of projected coordinate systems that are subject to distortion are: shape, area, distance and direction. A map that preserves shape is called conformal; one that preserves area is called equal-area; one that preserves distance is called equidistant; and one that preserves direction is called azimuthal (from https://mgimond.github.io/Spatial/coordinate-systems.html.\nThe takeaway from all this is you need to be aware of the crs for your objects in R, make sure they are projected if appropriate and in a projection that optimizes properties you are interested in, and objects you are analyzing or mapping together need to be in same crs.\nGoing back to our original example, we can transform crs of objects to work with them together:\n\nlibrary(ggplot2)\nlibrary(sf)\n# Check our coordinate reference systems\nst_crs(gages_sf)\n#> Coordinate Reference System:\n#>   User input: EPSG:4269 \n#>   wkt:\n#> GEOGCRS[\"NAD83\",\n#>     DATUM[\"North American Datum 1983\",\n#>         ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Geodesy.\"],\n#>         AREA[\"North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands. British Virgin Islands.\"],\n#>         BBOX[14.92,167.65,86.46,-47.74]],\n#>     ID[\"EPSG\",4269]]\nst_crs(pnw)\n#> Coordinate Reference System:\n#>   User input: +proj=aea +lat_1=41 +lat_2=47 +lat_0=44 +lon_0=-120 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs \n#>   wkt:\n#> PROJCRS[\"unknown\",\n#>     BASEGEOGCRS[\"unknown\",\n#>         DATUM[\"North American Datum 1983\",\n#>             ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#>                 LENGTHUNIT[\"metre\",1]],\n#>             ID[\"EPSG\",6269]],\n#>         PRIMEM[\"Greenwich\",0,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8901]]],\n#>     CONVERSION[\"unknown\",\n#>         METHOD[\"Albers Equal Area\",\n#>             ID[\"EPSG\",9822]],\n#>         PARAMETER[\"Latitude of false origin\",44,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8821]],\n#>         PARAMETER[\"Longitude of false origin\",-120,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8822]],\n#>         PARAMETER[\"Latitude of 1st standard parallel\",41,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8823]],\n#>         PARAMETER[\"Latitude of 2nd standard parallel\",47,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8824]],\n#>         PARAMETER[\"Easting at false origin\",0,\n#>             LENGTHUNIT[\"metre\",1],\n#>             ID[\"EPSG\",8826]],\n#>         PARAMETER[\"Northing at false origin\",0,\n#>             LENGTHUNIT[\"metre\",1],\n#>             ID[\"EPSG\",8827]]],\n#>     CS[Cartesian,2],\n#>         AXIS[\"(E)\",east,\n#>             ORDER[1],\n#>             LENGTHUNIT[\"metre\",1,\n#>                 ID[\"EPSG\",9001]]],\n#>         AXIS[\"(N)\",north,\n#>             ORDER[2],\n#>             LENGTHUNIT[\"metre\",1,\n#>                 ID[\"EPSG\",9001]]]]\n# Are they equal?\nst_crs(gages_sf)==st_crs(pnw)\n#> [1] FALSE\n# transform one to the other\ngages_sf <- st_transform(gages_sf, st_crs(pnw))\nggplot() + \n  geom_sf(data=gages_sf,  color=\"blue\") +\n  geom_sf(data=pnw,  color=\"black\", fill=NA) +\n  labs(title=\"USGS Stream Gages in the Pacific Northwest\") +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRe-project both the PNW state polygons and the stream gages to a different projected CRS - I suggest UTM zone 11, or perhaps Oregon Lambert, but choose your own. Use resources I list above to find a projection and get it’s specification to use in R to re-project both sf objects, then plot together either in base R or ggplot.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMy solution:\n\nlibrary(cowplot)\n# first I save the previous plot so we can view it alongside our update:\np1 <-ggplot() + \n  geom_sf(data=gages_sf,  color=\"blue\") +\n  geom_sf(data=pnw,  color=\"black\", fill=NA) +\n  labs(title=\"Albers Equal Area\") +\n  theme_bw()\n# You can fully specify the WKT:\nutmz11 <- 'PROJCS[\"NAD83(CSRS98) / UTM zone 11N (deprecated)\",GEOGCS[\"NAD83(CSRS98)\",DATUM[\"NAD83_Canadian_Spatial_Reference_System\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[\"EPSG\",\"6140\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9108\"]],AUTHORITY[\"EPSG\",\"4140\"]],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],AUTHORITY[\"EPSG\",\"2153\"],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'\n\n# or you can simply provide the EPSG code:\nutmz11 <- 2153\ngages_sf <- st_transform(gages_sf, utmz11)\npnw <- st_transform(pnw, utmz11)\np2 <- ggplot() + \n  geom_sf(data=gages_sf,  color=\"blue\") +\n  geom_sf(data=pnw,  color=\"black\", fill=NA) +\n  labs(title=\"UTM Zone 11\") +\n  theme_bw() \nplot_grid(p1, p2)\n\n\n\n\n\n\n\n\n1.5.5 Projecting\nFor spatial operations in R, raster or vector, we always need to make sure objects are in an appropriate crs and in the same crs as one another.\nWe’ll use the elevation data and a polygon feature for Crater Lake from the Rspatialworkshop package for this example.\n\nlibrary(Rspatialworkshop)\ndata(CraterLake)\nraster_filepath <- system.file(\"extdata\", \"elevation.tif\", package = \"Rspatialworkshop\")\nelevation <- terra::rast(raster_filepath)\n\n:::{.callout-note} #### Exercise 1. Find the projection of both the Crater Lake spatial polygon feature and the elevation raster 2. Are they the same? Should we project one to the other, or apply a new projection?\n:::{.callout-note collapse=“true”} #### Solution 1. Check the CRS of each feature and test for equality\n\ncrs(elevation)\n#> [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\nst_crs(CraterLake)\n#> Coordinate Reference System:\n#>   User input: WGS 84 \n#>   wkt:\n#> GEOGCRS[\"WGS 84\",\n#>     DATUM[\"World Geodetic System 1984\",\n#>         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     ID[\"EPSG\",4326]]\nst_crs(CraterLake) == crs(elevation)\n#> [1] FALSE\nst_crs(CraterLake)$wkt == crs(elevation)\n#> [1] FALSE\n\n\nThey share the same EPSG code, but are parameterized slightly differently for each - the crs function in terra does not include the input list item that st_crs in sf does - they are the same though as demonstrated when specifying the wkt item only from st_crs(CraterLake)\n\nEach feature is in WGS84, an unprojected CRS - for most operations, we would prefer to have them in a projected CRS\n\n\n\n\n\n\n\nExercise\n\n\n\nFind an appropriate area-preserving projection using Projection Wizard or spatialreference.org or any means you prefer and project both Crater Lake and elevation to this CRS.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere I’ll apply a custom Transverse Mercator I found the WKT representation for using Projection Wizard. Another good projection would be UTM zone 10N or UTM zone 11N\n\ntranvmerc <- 'PROJCS[\"ProjWiz_Custom_Transverse_Mercator\", GEOGCS[\"GCS_WGS_1984\", DATUM[\"D_WGS_1984\", SPHEROID[\"WGS_1984\",6378137.0,298.257223563]], PRIMEM[\"Greenwich\",0.0], UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"False_Easting\",500000],PARAMETER[\"False_Northing\",0.0],PARAMETER[\"Central_Meridian\",-483.2226562],PARAMETER[\"Scale_Factor\",0.9996],PARAMETER[\"Latitude_Of_Origin\",0.0],UNIT[\"Meter\",1.0]]'\nelevation_tm <- project(elevation, tranvmerc, method = \"bilinear\")\ncrs(elevation_tm)\n#> [1] \"PROJCRS[\\\"ProjWiz_Custom_Transverse_Mercator\\\",\\n    BASEGEOGCRS[\\\"WGS 84\\\",\\n        DATUM[\\\"World Geodetic System 1984\\\",\\n            ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                LENGTHUNIT[\\\"metre\\\",1]],\\n            ID[\\\"EPSG\\\",6326]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"Degree\\\",0.0174532925199433]]],\\n    CONVERSION[\\\"unnamed\\\",\\n        METHOD[\\\"Transverse Mercator\\\",\\n            ID[\\\"EPSG\\\",9807]],\\n        PARAMETER[\\\"Latitude of natural origin\\\",0,\\n            ANGLEUNIT[\\\"Degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8801]],\\n        PARAMETER[\\\"Longitude of natural origin\\\",-483.2226562,\\n            ANGLEUNIT[\\\"Degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8802]],\\n        PARAMETER[\\\"Scale factor at natural origin\\\",0.9996,\\n            SCALEUNIT[\\\"unity\\\",1],\\n            ID[\\\"EPSG\\\",8805]],\\n        PARAMETER[\\\"False easting\\\",500000,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8806]],\\n        PARAMETER[\\\"False northing\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8807]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"(E)\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"(N)\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\nCraterLake_tm <- st_transform(CraterLake,tranvmerc)\n\n\n\n\n\n1.5.6 S2 class\nIt’s important to note a recent change in R spatial with spherical geometry. Since sf version 1.0.0, R supports spherical geometry operations by default via its interface to Google’s S2 spherical geometry engine and the s2 interface package. S2 is perhaps best known as an example of a Discrete Global Grid System (DGGS). Another example of this is the H3 global hexagonal hierarchical spatial index.\nsf can run with s2 on or off and by default the S2 geometry engine is turned on:\n\nsf::sf_use_s2()\n#> [1] TRUE\n\nSee the Geocomputation with R s2 section for further details\n\n\n\n\n\n\nNote\n\n\n\nThere are sometimes good reasons for turning S2 off during an R session. See issue 1771 in sf’s GitHub repo - the default behavior can make code that would work with S2 turned off (and with older versions of sf) fail.\nSituations that can cause problems include operations on polygons that are not valid according to S2’s stricter definition. If you see error message such as #> Error in s2_geography_from_wkb … you may want to turn s2 off and try your operation again, perhaps turning of s2 and running a topology clean operation like st_make_valid()."
  },
  {
    "objectID": "foundations.html#geographic-data-io",
    "href": "foundations.html#geographic-data-io",
    "title": "1  Foundations",
    "section": "\n1.6 Geographic Data I/O",
    "text": "1.6 Geographic Data I/O\n\nThere are several ways we typically get spatial data into R:\n\nLoad spatial files we have on our machine or from remote source\nLoad spatial data that is part of an R package\nGrab data using API (often making use of particular R packages)\nConverting flat files with x,y data to spatial data\nGeocoding data (we saw example of this at beginning)\n\n\n\nFor reading and writing vector and raster data in R, the several main packages we’ll look at are:\n\n\nsffor vector formats such as ESRI Shapefiles, GeoJSON, and GPX - sf uses OGR, which is a library under the GDAL source tree,under the hood\n\nterra or stars for raster formats such as GeoTIFF or ESRI or ASCII grid using GDAL under the hood\n\nWe can quickly discover supported I/O vector formats either via sf or rgdal:\n\nlibrary(knitr)\nlibrary(sf)\nprint(paste0('There are ',st_drivers(\"vector\")  |>  nrow(), ' vector drivers available using st_read or read_sf'))\n#> [1] \"There are 67 vector drivers available using st_read or read_sf\"\nkable(head(st_drivers(what='vector'),n=5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nlong_name\nwrite\ncopy\nis_raster\nis_vector\nvsi\n\n\n\nESRIC\nESRIC\nEsri Compact Cache\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\n\n\nnetCDF\nnetCDF\nNetwork Common Data Format\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\nPDS4\nPDS4\nNASA Planetary Data System 4\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nVICAR\nVICAR\nMIPL VICAR file\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nJP2OpenJPEG\nJP2OpenJPEG\nJPEG-2000 driver based on OpenJPEG library\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\n\n\n\n\n\nAs well as I/O raster formats via sf:\n\nprint(paste0('There are ',st_drivers(what='raster')  |>  nrow(), ' raster drivers available'))\n#> [1] \"There are 141 raster drivers available\"\nkable(head(st_drivers(what='raster'),n=5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nlong_name\nwrite\ncopy\nis_raster\nis_vector\nvsi\n\n\n\nVRT\nVRT\nVirtual Raster\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nDERIVED\nDERIVED\nDerived datasets using VRT pixel functions\nFALSE\nFALSE\nTRUE\nFALSE\nFALSE\n\n\nGTiff\nGTiff\nGeoTIFF\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nCOG\nCOG\nCloud optimized GeoTIFF generator\nFALSE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\nNITF\nNITF\nNational Imagery Transmission Format\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\n\n\n\n\n\n\n1.6.1 Reading in vector data\nsf can be used to read numerous file types:\n\nShapefiles\nGeodatabases\nGeopackages\nGeojson\nSpatial database files\n\n\n1.6.1.1 Shapefiles\nTypically working with vector GIS data we work with ESRI shapefiles or geodatabases - here we have an example of how one would read in a shapefile using sf:\n\ncitylims <- read_sf(system.file(\"extdata/city_limits.shp\", package = \"Rspatialworkshop\"))\n\noptions(scipen=3)\nplot(citylims$geometry, axes=T, main='Oregon City Limits') # plot it!\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nst_read versus read_sf - throughout this section I’ve used read_sf and st_read - I typically use read_sf - why would that be a good practice?\n\n\n:::{.callout-note collapse=“true”} #### Solution read_sf is an sf alternative to st_read (see this section 1.2.2). Try reading in citylims data above using read_sf and notice difference, and check out help(read_sf). read_sf and write_sf are simply aliases for st_read and st_write with modified default arguments. Big differences are:\n\nstringsAsFactors=FALSE\nquiet=TRUE\nas_tibble=TRUE\n\nNote that stringsAsFactors = FALSE is the new default in R versions >= 4.0\n\n1.6.1.2 Geodatabases\nWe use st_read or read_sf similarly for reading in an ESRI file geodatabase feature:\n\nlibrary(ggplot2)\n# List all feature classes in a file geodatabase\nst_layers(system.file(\"extdata/StateParkBoundaries.gdb\", package = \"Rspatialworkshop\"))\n#> Driver: OpenFileGDB \n#> Available layers:\n#>            layer_name geometry_type features fields                 crs_name\n#> 1 StateParkBoundaries Multi Polygon      431     15 WGS 84 / Pseudo-Mercator\nfgdb = system.file(\"extdata/StateParkBoundaries.gdb\", package = \"Rspatialworkshop\")\n# Read the feature class\nparks <- st_read(dsn=fgdb,layer=\"StateParkBoundaries\")\n#> Reading layer `StateParkBoundaries' from data source \n#>   `C:\\Program Files\\R\\R-4.2.2\\library\\Rspatialworkshop\\extdata\\StateParkBoundaries.gdb' \n#>   using driver `OpenFileGDB'\n#> Simple feature collection with 431 features and 15 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -13866300 ymin: 5159950 xmax: -13019620 ymax: 5818601\n#> Projected CRS: WGS 84 / Pseudo-Mercator\nggplot(parks) + geom_sf()\n\n\n\n\n\n1.6.2 Geopackages\nAnother spatial file format is the geopackage. Let’s try a quick read and write of geopackage data. First we’ll read in a geopackage using data that comes with sf using dplyr syntax just to show something a bit different and use read_sf as an alternative to st_read. You may want to try writing the data back out as a geopackage as well.\n\nlibrary(dplyr)\nnc <- system.file(\"gpkg/nc.gpkg\", package=\"sf\")  |>  read_sf() # reads in\nglimpse(nc)\n#> Rows: 100\n#> Columns: 15\n#> $ AREA      <dbl> 0.114, 0.061, 0.143, 0.070, 0.153, 0.097, 0.062, 0.091, 0…\n#> $ PERIMETER <dbl> 1.442, 1.231, 1.630, 2.968, 2.206, 1.670, 1.547, 1.284, 1…\n#> $ CNTY_     <dbl> 1825, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 183…\n#> $ CNTY_ID   <dbl> 1825, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 183…\n#> $ NAME      <chr> \"Ashe\", \"Alleghany\", \"Surry\", \"Currituck\", \"Northampton\",…\n#> $ FIPS      <chr> \"37009\", \"37005\", \"37171\", \"37053\", \"37131\", \"37091\", \"37…\n#> $ FIPSNO    <dbl> 37009, 37005, 37171, 37053, 37131, 37091, 37029, 37073, 3…\n#> $ CRESS_ID  <int> 5, 3, 86, 27, 66, 46, 15, 37, 93, 85, 17, 79, 39, 73, 91,…\n#> $ BIR74     <dbl> 1091, 487, 3188, 508, 1421, 1452, 286, 420, 968, 1612, 10…\n#> $ SID74     <dbl> 1, 0, 5, 1, 9, 7, 0, 0, 4, 1, 2, 16, 4, 4, 4, 18, 3, 4, 1…\n#> $ NWBIR74   <dbl> 10, 10, 208, 123, 1066, 954, 115, 254, 748, 160, 550, 124…\n#> $ BIR79     <dbl> 1364, 542, 3616, 830, 1606, 1838, 350, 594, 1190, 2038, 1…\n#> $ SID79     <dbl> 0, 3, 6, 2, 3, 5, 2, 2, 2, 5, 2, 5, 4, 4, 6, 17, 4, 7, 1,…\n#> $ NWBIR79   <dbl> 19, 12, 260, 145, 1197, 1237, 139, 371, 844, 176, 597, 13…\n#> $ geom      <MULTIPOLYGON [°]> MULTIPOLYGON (((-81.47276 3..., MULTIPOLYGON…\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat are a couple advantages of geopackages over shapefiles?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSome thoughts here, main ones probably:\n\ngeopackages avoid mult-file format of shapefiles\ngeopackages avoid the 2gb limit of shapefiles\ngeopackages are open-source and follow OGC standards\nlighter in file size than shapefiles\ngeopackages avoid the 10-character limit to column headers in shapefile attribute tables (stored in archaic .dbf files)\n\n\n\n\n\n1.6.2.1 Open spatial data sources\nThere is a wealth of open spatial data accessible online now via static URLs or APIs - a few examples include Data.gov, NASA SECAC Portal, Natural Earth, UNEP GEOdata, and countless others listed here at Free GIS Data\n\n1.6.2.2 Spatial data from R packages\nThere are also a number of R packages written specifically to provide access to geospatial data - below are a few and we’ll step through some examples of pulling in data using some of these packages.\n\n\n\nExample R packages for spatial data retrieval.\n\n\n\n\n\nPackage name\nDescription\n\n\n\nUSABoundaries\nProvide historic and contemporary boundaries of the US\n\n\ntigris\nDownload and use US Census TIGER/Line Shapefiles in R\n\n\ntidycensus\nUses Census American Community API to return tidyverse and optionally sf ready data frames\n\n\nFedData\nFunctions for downloading geospatial data from several federal sources\n\n\nelevatr\nAccess elevation data from various APIs (by Jeff Hollister)\n\n\ngetlandsat\nProvides access to Landsat 8 data.\n\n\nosmdata\nDownload and import of OpenStreetMap data.\n\n\nraster\nThe getData() function downloads and imports administrative country, SRTM/ASTER elevation, WorldClim data.\n\n\nrnaturalearth\nFunctions to download Natural Earth vector and raster data, including world country borders.\n\n\nrnoaa\nAn R interface to National Oceanic and Atmospheric Administration (NOAA) climate data.\n\n\nrWBclimate\nAn access to the World Bank climate data.\n\n\n\n\n\nBelow is an example of pulling in US states using the rnaturalearth package - note that the default is to pull in data as sp objects and we coerce to sf. Also take a look at the chained operation using dplyr. Try changing the filter or a parameter in ggplot.\n\nlibrary(rnaturalearth)\nlibrary(dplyr)\nstates <- ne_states(country = 'United States of America')\nstates_sf <- st_as_sf(states)\nstates_sf  |>  \n  dplyr::filter(!name %in% c('Hawaii','Alaska') & !is.na(name)) |> \n  ggplot() + geom_sf(fill=NA)\n\n\n\n\n\n1.6.2.3 Read in OpenStreetMap data\nThe osmdata package is a fantastic resource for leveraging the OpenStreetMap (OSM) database.\nFirst we’ll find available tags to get foot paths to plot\n\nlibrary(osmdata)\nlibrary(mapview)\nmapviewOptions(fgb=FALSE)\nhead(available_tags(\"highway\")) # get rid of head when you run - just used to truncate output\n#> # A tibble: 6 × 2\n#>   Key     Value       \n#>   <chr>   <chr>       \n#> 1 highway bridleway   \n#> 2 highway bus_guideway\n#> 3 highway bus_stop    \n#> 4 highway busway      \n#> 5 highway construction\n#> 6 highway corridor\n\n\nfootway <- opq(bbox = \"corvallis oregon\") %>% \n  add_osm_feature(key = \"highway\", value = c(\"footway\",\"cycleway\",\"path\", \"path\",\"pedestrian\",\"track\")) %>% \n  osmdata_sf()\nfootway <- footway$osm_lines\n\nrstrnts <- opq(bbox = \"corvallis oregon\") %>% \n    add_osm_feature(key = \"amenity\", value = \"restaurant\") %>%\n    osmdata_sf()\nrstrnts <- rstrnts$osm_points\n\nmapview(footway$geometry) + mapview(rstrnts)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTake a minute and try pulling in data of your own for your own area and plotting using osmdata\n\n\n\n1.6.3 Reading in Raster data\n\n1.6.4 terra package\nLoad stock elevation .tif file that comes with package\n\nlibrary(terra)\nf <- system.file(\"ex/elev.tif\", package=\"terra\")\nelev <- rast(f)\nbarplot(elev, digits=-1, las=2, ylab=\"Frequency\")\n\n\n\n\n\nplot(elev)\n\n\n\n\n\n1.6.5 stars package\nLoad stock Landsat 7 .tif file that comes with package\n\nlibrary(stars)\ntif = system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nread_stars(tif) %>%\n  dplyr::slice(index = 1, along = \"band\") %>%\n  plot()\n\n\n\n\n\n1.6.6 Convert flat files to spatial\nWe often have flat files, locally on our machine or accessed elsewhere, that have coordinate information which we would like to make spatial.\nIn the steps below, we\n\nread in a .csv file of USGS gages in the PNW that have coordinate columns\nUse st_as_sf function in sf to convert the data frame to an sf spatial simple feature collection by:\n\npassing the coordinate columns to the coords parameter\nspecifying a coordinate reference system (CRS)\nopting to retain the coordinate columns as attribute columns in the resulting sf feature collection.\n\n\nKeep only the coordinates and station ID in resulting sf feature collection, and\nPlotting our gages as spatial features with ggplot2 using geom_sf.\n\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(Rspatialworkshop)\ngages = read_csv(system.file(\"extdata/Gages_flowdata.csv\", package = \"Rspatialworkshop\"))\n\ngages_sf <- gages |> \n  st_as_sf(coords = c(\"LON_SITE\", \"LAT_SITE\"), crs = 4269, remove = FALSE)  |> \n  dplyr::select(STATION_NM,LON_SITE, LAT_SITE)\n\nggplot() + geom_sf(data=gages_sf)"
  }
]